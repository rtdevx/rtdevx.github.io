[{"content":" Jump to Section âœ¨Cheat Sheets A set of useful cheat sheets\u0026hellip; âœ¨Trainings and Certifications Training in progress being documented at this moment and the trainings that I have completed in the past\u0026hellip; ","date":null,"permalink":"https://robk.uk/posts/","section":"ğŸ”°Posts","summary":"Welcome to my blog.","title":"ğŸ”°Posts"},{"content":"GitHub Actions is a feature that allows you to automate software development workflows directly within your GitHub repository. It enables you to build, test, package, release, and deploy your code automatically based on specific events, such as code pushes or pull requests.\n\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Maximilian SchwarzmÃ¼ller\u0026rsquo;s GitHub Actions - The Complete Guide course on Udemy.\nMaximilian SchwarzmÃ¼ller on Udemy.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":null,"permalink":"https://robk.uk/posts/training/github/github-actions---the-complete-guide/","section":"ğŸ”°Posts","summary":"Learnings from \u003cstrong\u003eMaximilian SchwarzmÃ¼ller\u003c/strong\u003eâ€™s \u0026ldquo;\u003cem\u003eGitHub Actions - The Complete Guide\u003c/em\u003e\u0026rdquo; course on \u003cem\u003eUdemy\u003c/em\u003e.","title":"GitHub Actions - The Complete Guide"},{"content":" My thoughts and learnings about:\nDevOps, Agile, AWS, Linux, Ansible, Terraform, Containers, Network\n{ Cheat Sheets }\n{ Trainings and Certifications }\n{ Productivity }\nÂ» About Â« ","date":null,"permalink":"https://robk.uk/","section":"Welcome my DevOps blog.","summary":"","title":"Welcome my DevOps blog."},{"content":"I treat training as an ongoing part of my professional journey - not just to keep up, but to dig deeper. Whether it\u0026rsquo;s automation, infrastructure, or security, each course I take helps me build systems that are resilient, clear, and future-ready.\nThis part of my blog highlights the trainings I\u0026rsquo;ve completed along the way - chosen with intention, driven by curiosity, and always aimed at doing the work better.\n","date":null,"permalink":"https://robk.uk/posts/training/","section":"ğŸ”°Posts","summary":"This section of my blog highlights training in progress being documented at this moment and the trainings that I have completed in the past\u0026hellip;","title":"ğŸ—‚ï¸My Trainings"},{"content":"Git is the most widely used version control system, helping developers track changes, collaborate, and manage code effectively. GitHub builds on Git by providing a cloud platform to host, review, and share projects with ease.\nGit enables version tracking, branching, and seamless teamwork. GitHub adds cloud hosting, pull requests, issues, and collaboration tools. Together, they power open-source development and large-scale project management. External Resources Â» GitHub Actions official Documentation GitHub Actions Marketplace GitHub.com ","date":null,"permalink":"https://robk.uk/posts/training/github/","section":"ğŸ”°Posts","summary":"","title":"Git / GitHub Trainings and Certifications"},{"content":"","date":null,"permalink":"https://robk.uk/tags/assoc003/","section":"Tags","summary":"","title":"Assoc003"},{"content":"","date":null,"permalink":"https://robk.uk/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"https://robk.uk/categories/devops/","section":"Categories","summary":"","title":"DevOps"},{"content":"","date":null,"permalink":"https://robk.uk/categories/iac/","section":"Categories","summary":"","title":"IaC"},{"content":"","date":null,"permalink":"https://robk.uk/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"https://robk.uk/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":" Terraform must store state about your managed infrastructure and configuration. This state is used by Terraform to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures.\nMore info: https://developer.hashicorp.com/terraform/language/state\nterraform state commands #Refer to the following subcommands for additional information:\nterraform state list #Usage:Â terraform state list [options] [address...]\nThe command will list all resources in the state file matching the given addresses (if any). If no addresses are given, all resources are listed.\nThe resources listed are sorted according to module depth order followed alphabetically. This means that resources that are in your immediate configuration are listed first, and resources that are more deeply nested within modules are listed last.\nMore info: terraform state list\nterraform state mv #TheÂ terraform state mvÂ command changes bindings in Terraform state so that existing remote objects bind to new resource instances.\nUsage:Â terraform state mv [options] SOURCE DESTINATION\nWarning:Â If you are using Terraform in a collaborative environment, you must ensure that when you are usingÂ terraform state mvÂ for a code refactoring purpose you communicate carefully with your coworkers to ensure that nobody makes any other changes between your configuration change and yourÂ terraform state mvÂ command, because otherwise they might inadvertently create a plan that will destroy the old object and create a new object at the new address. Example: Rename a Resource\nRenaming a resource means making a configuration change like the following:\nresource \u0026#34;packet_device\u0026#34; \u0026#34;worker\u0026#34; { resource \u0026#34;packet_device\u0026#34; \u0026#34;helper\u0026#34; { # ... } To tell Terraform that it should treat the new \u0026ldquo;helper\u0026rdquo; resource as a rename of the old \u0026ldquo;worker\u0026rdquo; resource, you can pair the above configuration change with the following command:\nterraform state mv packet_device.worker packet_device.helper Example: Move a Resource Into a Module\nIf you originally wrote a resource in your root module but now wish to refactor it into a child module, you can move theÂ resourceÂ block into the child module configuration, removing the original in the root module, and then run the following command to tell Terraform to treat it as a move:\nterraform state mv packet_device.worker module.worker.packet_device.worker More info: terraform state mv\nterraform state pull #TheÂ terraform state pullÂ downloads and outputs state information from aÂ remote stateÂ or local state.\nUsage:Â terraform state pull\nThis command downloads the state from its current location, upgrades the local copy to the latest state file version that is compatible with locally-installed Terraform, and outputs the raw format to stdout.\nMore info: terraform state pull\nterraform state replace-provider #TheÂ terraform state replace-providerÂ command replaces the provider for resources in aÂ Terraform state.\nUsage:Â terraform state replace-provider [options] FROM_PROVIDER_FQN TO_PROVIDER_FQN\nThis command will update all resources using the \u0026ldquo;from\u0026rdquo; provider, setting the provider to the specified \u0026ldquo;to\u0026rdquo; provider. This allows changing the source of a provider which currently has resources in state.\nExample: Replace Terraform provider\nThe example below replaces theÂ hashicorp/awsÂ provider with a fork byÂ acme, hosted at a private registry atÂ registry.acme.corp:\n$ terraform state replace-provider hashicorp/aws registry.acme.corp/acme/aws More info: terraform state replace-provider\nterraform state rm #TheÂ terraform state rmÂ command removes the binding to an existing remote object without first destroying it. The remote object continues to exist but is no longer managed by Terraform.\nUsage:Â terraform state rm [options] ADDRESS...\nTerraform will search the state for any instances matching the givenÂ resource address, and remove the record of each one so that Terraform will no longer be tracking the corresponding remote objects.\nThis means that although the objects will still continue to exist in the remote system, a subsequentÂ terraform planÂ will include an action to create a new object for each of the \u0026ldquo;forgotten\u0026rdquo; instances. Depending on the constraints imposed by the remote system, creating those objects might fail if their names or other identifiers conflict with the old objects still present.\nExample: Remove all Instances of a Resource\nThe following example will cause Terraform to \u0026ldquo;forget\u0026rdquo; all of the instances of theÂ packet_deviceÂ resource named \u0026ldquo;worker\u0026rdquo;.\n$ terraform state rm \u0026#39;packet_device.worker\u0026#39; More info: terraform state rm\nterraform state show #TheÂ terraform state showÂ command shows the attributes of a single resource in theÂ Terraform state.\nUsage:Â terraform state show [options] ADDRESS\nThe command will show the attributes of a single resource in the state file that matches the given address.\nExample: Show a Resource\nThe example below shows aÂ packet_deviceÂ resource namedÂ worker:\n$ terraform state show \u0026#39;packet_device.worker\u0026#39; # packet_device.worker: resource \u0026#34;packet_device\u0026#34; \u0026#34;worker\u0026#34; { billing_cycle = \u0026#34;hourly\u0026#34; created = \u0026#34;2015-12-17T00:06:56Z\u0026#34; facility = \u0026#34;ewr1\u0026#34; hostname = \u0026#34;prod-xyz01\u0026#34; id = \u0026#34;6015bg2b-b8c4-4925-aad2-f0671d5d3b13\u0026#34; locked = false } terraform state show works very well in combination with terraform state list:\nâ¯ terraform state list data.aws_availability_zones.available module.vpc.aws_db_subnet_group.database[0] module.vpc.aws_default_network_acl.this[0] module.vpc.aws_default_route_table.default[0] module.vpc.aws_default_security_group.this[0] module.vpc.aws_eip.nat[0] terraform state show \u0026#39;module.vpc.aws_eip.nat[0]\u0026#39; More info: terraform state show\nRemote State #The Terraform state subcommands all work with remote state just as if it was local state. Reads and writes may take longer than normal as each read and each write do a full network roundtrip. Otherwise, backups are still written to disk and the CLI usage is the same as if it were local state.\nBackups #AllÂ terraform stateÂ subcommands that modify the state write backup files. The path of these backup file can be controlled withÂ -backup.\nNote that backups for state modificationÂ can not be disabled. Due to the sensitivity of the state file, Terraform forces every state modification command to write a backup file. You\u0026rsquo;ll have to remove these files manually if you don\u0026rsquo;t want to keep them around. Terraform backend #Terraform uses persisted state data to keep track of the resources it manages. You can eitherÂ integrate with HCP TerraformÂ to store state data or define aÂ backendÂ block to store state in a remote object. This lets multiple people access the state data and work together on that collection of infrastructure resources.\nTo configure a backend, add a nestedÂ backendÂ block within the top-levelÂ terraformÂ block. The following example configures theÂ remoteÂ backend.\nFile: ğŸ“„providers.tf\nterraform { required_version = \u0026#34;~\u0026gt; 1.14.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; } } # INFO: S3 Backend Block backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;your-backend-s3-bucket\u0026#34; key = \u0026#34;terraform-core/aws-codepipelines/terraform.tfstate\u0026#34; region = \u0026#34;eu-west-2\u0026#34; dynamodb_table = \u0026#34;prod-s3backend-lock\u0026#34; encrypt = true } } More info: Terraform Backend\n\u0026raquo; Sources \u0026laquo; # terraform state Terraform state commands:\nterraform state commands reference\nterraform state list\nterraform state mv\nterraform state pull\nterraform state replace-provider\nterraform state rm\nterraform state show\nTerraform Backend:\nTerraform Backend ","date":"28 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/terraform-state/","section":"ğŸ”°Posts","summary":"Terraform must store state about your managed infrastructure and configuration\u0026hellip;","title":"Terraform State"},{"content":" TheÂ resourceÂ block defines a piece of infrastructure and specifies the settings for Terraform to create it with.\nThe arguments that an individual resource supports are determined by the provider. Refer to the provider documentation for more information about specific resource configuration.\nMore info: https://developer.hashicorp.com/terraform/language/block/resource\nConfiguration model #TheÂ resourceÂ block supports the following arguments:\nresource \u0026quot;\u0026lt;TYPE\u0026gt;\u0026quot; \u0026quot;\u0026lt;LABEL\u0026gt;\u0026quot;Â block PROVIDER ARGUMENTSÂ block | refer to your provider documentation countÂ number | mutually exclusive withÂ for_each depends_onÂ list of references for_eachÂ map or set of strings | mutually exclusive withÂ count providerÂ reference lifecycleÂ block action_triggerÂ block eventsÂ list | required to invoke an action conditionÂ expression actionsÂ list | required to invoke an action create_before_destroyÂ boolean prevent_destroyÂ boolean ignore_changesÂ list of attributes replace_triggered_byÂ list of references preconditionÂ block conditionÂ string error_messageÂ string postconditionÂ block conditionÂ string error_messageÂ string connectionÂ block typeÂ string userÂ string passwordÂ string hostÂ string portÂ string timeoutÂ string script_pathÂ string private_keyÂ string certificateÂ string agentÂ string agent_identityÂ string host_keyÂ string target_platformÂ string script_pathÂ string httpsÂ string insecureÂ string use_ntlmÂ string cacertÂ string bastion_hostÂ string bastion_host_keyÂ string bastion_portÂ string bastion_userÂ string bastion_passwordÂ string bastion_private_keyÂ string bastion_certificateÂ string proxy_schemeÂ string proxy_portÂ string proxy_user_nameÂ string proxy_user_passwordÂ string provisioner \u0026quot;\u0026lt;TYPE\u0026gt;\u0026quot;Â block sourceÂ string destinationÂ string contentÂ string commandÂ string working_dirÂ string interpreterÂ string environmentÂ string whenÂ keyword quietÂ boolean inlineÂ list of strings scriptÂ string scriptsÂ string on_failureÂ keyword connectionÂ block Complete configuration #The followingÂ resourceÂ block defines all of the supported built-in arguments you can set on a resource:\nresource \u0026#34;\u0026lt;TYPE\u0026gt;\u0026#34; \u0026#34;\u0026lt;LABEL\u0026gt;\u0026#34; { \u0026lt;PROVIDER_ARGUMENTS\u0026gt; count = \u0026lt;NUMBER\u0026gt; # `for_each` and `count` are mutually exclusive depends_on = [ \u0026lt;RESOURCE.ADDRESS.EXPRESSION\u0026gt; ] for_each = { # `for_each` and `count` are mutually exclusive \u0026lt;KEY\u0026gt; = \u0026lt;VALUE\u0026gt; } for_each = [ # `for_each` accepts a map or a set of strings \u0026#34;\u0026lt;VALUE\u0026gt;\u0026#34;, \u0026#34;\u0026lt;VALUE\u0026gt;\u0026#34; ] provider = \u0026lt;REFERENCE.TO.ALIAS\u0026gt; lifecycle { action_trigger { events = [ # specify one or more lifecycle events as a list before_create, after_create, before_update, after_update ] condition = \u0026lt;EXPRESSSION\u0026gt; actions = [ action.\u0026lt;TYPE\u0026gt;.\u0026lt;LABEL\u0026gt; ] } create_before_destroy = \u0026lt;true || false\u0026gt; prevent_destroy = \u0026lt;true || false\u0026gt; ignore_changes = [ \u0026lt;ATTRIBUTE\u0026gt; ] replace_triggered_by = [ \u0026lt;RESOURCE.ADDRESS.EXPRESSION\u0026gt; ] precondition { condition = \u0026lt;EXPRESSION\u0026gt; error_message = \u0026#34;\u0026lt;STRING\u0026gt;\u0026#34; } postcondition { condition = \u0026lt;EXPRESSION\u0026gt; error_message = \u0026#34;\u0026lt;STRING\u0026gt;\u0026#34; } } connection { type = \u0026lt;\u0026#34;ssh\u0026#34; or \u0026#34;winrm\u0026#34;\u0026gt; host = \u0026lt;EXPRESSION\u0026gt; \u0026lt;DEFAULT_CONNECTION_SETTINGS\u0026gt; } provisioner \u0026#34;\u0026lt;TYPE\u0026gt;\u0026#34; { source = \u0026#34;\u0026lt;PATH\u0026gt;\u0026#34; destination = \u0026#34;\u0026lt;PATH\u0026gt;\u0026#34; content = \u0026#34;\u0026lt;CONTENT TO COPY TO `destination`\u0026gt;\u0026#34; command = \u0026lt;COMMAND\u0026gt; working_dir = \u0026#34;\u0026lt;PATH TO DIR WHERE TERRAFORM EXECUTES `command`\u0026gt;\u0026#34; interpreter = [ \u0026#34;\u0026lt;PATH TO INTERPRETER EXECUTABLE\u0026gt;\u0026#34;, \u0026#34;\u0026lt;COMMAND\u0026gt; \u0026lt;ARGUMENTS\u0026gt;\u0026#34; ] environment { \u0026#34;\u0026lt;KEY\u0026gt;\u0026#34; = \u0026#34;\u0026lt;VALUE\u0026gt;\u0026#34; } when = \u0026lt;TERRAFORM COMMAND\u0026gt; quiet = \u0026lt;true || false\u0026gt; inline = [ \u0026#34;\u0026lt;COMMAND\u0026gt;\u0026#34; ] script = \u0026#34;\u0026lt;PATH\u0026gt;\u0026#34; scripts = [ \u0026#34;\u0026lt;PATH\u0026gt;\u0026#34; ] on_failure = \u0026lt;continue || fail\u0026gt; connection { type = \u0026lt;\u0026#34;ssh\u0026#34; or \u0026#34;winrm\u0026#34;\u0026gt; host = \u0026lt;EXPRESSION\u0026gt; \u0026lt;SPECIFIC_CONNECTION_SETTINGS\u0026gt; } } } \u0026raquo; Sources \u0026laquo; # https://developer.hashicorp.com/terraform/language/block/resource ","date":"27 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/terraform-resource-block-reference/","section":"ğŸ”°Posts","summary":"The resource block defines a piece of infrastructure and specifies the settings for Terraform to create it with.","title":"Terraform resource block reference ğŸ”¥"},{"content":" Terraform expressions are used to compute values within Terraform configurations, allowing for dynamic and flexible infrastructure management. They can include simple literals, complex references, and various built-in functions to manipulate data types and structures.\nMore info: Terraform Expressions\nTypes and Values #Data types that Terraform expressions can resolve to, and the literal syntaxes for values of those types.\nTypes # Strings, numbers, and bools are sometimes calledÂ primitive types.Â Lists/tuples and maps/objects are sometimes calledÂ complex types,Â structural types,Â orÂ collection types. Primitive types # string - characters representing some text, likeÂ \u0026quot;hello world!\u0026quot;. number - a numeric value. TheÂ numberÂ type can represent both whole numbers likeÂ 15Â and fractional values likeÂ 6.283185. bool - a boolean value, eitherÂ trueÂ orÂ false.Â boolÂ values can be used in conditional logic. Complex types # list (orÂ tuple) - a sequence of values, likeÂ [\u0026quot;us-west-1a\u0026quot;, \u0026quot;us-west-1c\u0026quot;]. Identify elements in a list with consecutive whole numbers, starting with zero. set - a collection of unique values that do not have any secondary identifiers or ordering. Terraform does not support directly accessing elements of a set by index because sets are unordered collections. To access elements in a set by index, first convert the set to a list. Define a set. The following example specifies a set nameÂ example_set:\nvariable \u0026#34;example_set\u0026#34; { type = set(string) default = [\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;] } Use theÂ tolistÂ function to convert the set to a list. The following example stores the converted list as a local variable calledÂ example_list:\nlocals { example_list = tolist(var.example_set) } You can then reference an element in the list:\noutput \u0026#34;first_element\u0026#34; { value = local.example_list[0] } output \u0026#34;second_element\u0026#34; { value = local.example_list[1] } map (orÂ object) - a group of values identified by named labels, likeÂ {name = \u0026quot;Mabel\u0026quot;, age = 52}. Maps/objects are represented by a pair of curly braces containing a series ofÂ \u0026lt;KEY\u0026gt; = \u0026lt;VALUE\u0026gt;Â pairs: { name = \u0026#34;John\u0026#34; age = 52 } Key/value pairs can be separated by either a comma or a line break. The keys in a map must be strings. null - a value that representsÂ absenceÂ orÂ omission. nullÂ is most useful in conditional expressions, so you can dynamically omit an argument if a condition isn\u0026rsquo;t met. More info: Types and Values\nStrings and Templates #Syntaxes for string literals, including interpolation sequences and template directives.\nQuoted Strings #A quoted string is a series of characters delimited by straight double-quote characters (\u0026quot;).\n\u0026#34;hello\u0026#34; Escape Sequences #In quoted strings, the backslash character serves as an escape sequence, with the following characters selecting the escape behaviour:\nSequence Replacement \\n Newline \\r Carriage Return \\t Tab \\\u0026quot; Literal quote (without terminating the string) \\\\ Literal backslash \\uNNNN Unicode character from the basic multilingual plane (NNNN is four hex digits) \\UNNNNNNNN Unicode character from supplementary planes (NNNNNNNN is eight hex digits) There are also two special escape sequences that do not use backslashes: Sequence Replacement $${ LiteralÂ ${, without beginning an interpolation sequence. %%{ LiteralÂ %{, without beginning a template directive sequence. Heredoc Strings #Terraform supports a \u0026ldquo;heredoc\u0026rdquo; style of string literal inspired by Unix shell languages, which allows multi-line strings to be expressed more clearly.\n\u0026lt;\u0026lt;EOT hello world EOT Terraform also accepts anÂ indentedÂ heredoc string variant that is introduced by theÂ \u0026lt;\u0026lt;-Â sequence:\nblock { value = \u0026lt;\u0026lt;-EOT hello world EOT } Don\u0026rsquo;t use \u0026ldquo;heredoc\u0026rdquo; strings to generate JSON or YAML. Instead, useÂ theÂ jsonencodeÂ functionÂ orÂ theÂ yamlencodeÂ functionÂ so that Terraform can be responsible for guaranteeing valid JSON or YAML syntax.\nexample = jsonencode({ a = 1 b = \u0026#34;hello\u0026#34; }) Escape Sequences #Backslash sequences are not interpreted as escapes in a heredoc string expression. Instead, the backslash character is interpreted literally.\nHeredocs support two special escape sequences that do not use backslashes:\nSequence Replacement $${ LiteralÂ ${, without beginning an interpolation sequence. %%{ LiteralÂ %{, without beginning a template directive sequence. Interpolation #AÂ ${ ... }Â sequence is anÂ interpolation,Â which evaluates the expression given between the markers, converts the result to a string if necessary, and then inserts it into the final string:\n\u0026#34;Hello, ${var.name}!\u0026#34; In the above example, the named objectÂ var.nameÂ is accessed and its value inserted into the string, producing a result like \u0026ldquo;Hello, Juan!\u0026rdquo;.\nMore info: Strings and Templates\nReferences to Values #How to refer to named values like variables and resource attributes.\nThe main kinds of named values available in Terraform are:\nResources Input variables Local values Child module outputs Data sources Filesystem and workspace info Block-local values More info: References to Values\nOperators #Arithmetic, comparison, and logical operators.\nWhen multiple operators are used together in an expression, they are evaluated in the following order of operations:\n!,Â -Â (multiplication byÂ -1) *,Â /,Â % +,Â -Â (subtraction) \u0026gt;,Â \u0026gt;=,Â \u0026lt;,Â \u0026lt;= ==,Â != \u0026amp;\u0026amp; || Arithmetic Operators Equality Operators Comparison Operators Logical Operators More info: Operators\nFunction Calls #Syntax for calling Terraform\u0026rsquo;s built-in functions.\nThe Terraform language has a number ofÂ built-in functionsÂ that can be used in expressions to transform and combine values. These are similar to the operators but all follow a common syntax:\n\u0026lt;FUNCTION NAME\u0026gt;(\u0026lt;ARGUMENT 1\u0026gt;, \u0026lt;ARGUMENT 2\u0026gt;) The function name specifies which function to call. Each defined function expects a specific number of arguments with specific value types, and returns a specific value type as a result.\nMore info: Function Calls\nConditional Expressions #\u0026lt;CONDITION\u0026gt; ? \u0026lt;TRUE VAL\u0026gt; : \u0026lt;FALSE VAL\u0026gt;Â expression, which chooses between two values based on a bool condition.\nThe syntax of a conditional expression is as follows:\ncondition ? true_val : false_val IfÂ conditionÂ isÂ trueÂ then the result isÂ true_val. IfÂ conditionÂ isÂ falseÂ then the result isÂ false_val.\nA common use of conditional expressions is to define defaults to replace invalid values:\nvar.a == \u0026#34;\u0026#34; ? \u0026#34;default-a\u0026#34; : var.a IfÂ var.aÂ is an empty string then the result isÂ \u0026quot;default-a\u0026quot;, but otherwise it is the actual value ofÂ var.a.\nUse the logical operatorsÂ \u0026amp;\u0026amp;Â (AND),Â ||Â (OR), andÂ !Â (NOT) to combine multiple conditions together.\ncondition = var.name != \u0026#34;\u0026#34; \u0026amp;\u0026amp; lower(var.name) == var.name containsÂ Function #Use theÂ containsÂ functionÂ to test whether a given value is one of a set of predefined valid values.\ncondition = contains([\u0026#34;STAGE\u0026#34;, \u0026#34;PROD\u0026#34;], var.environment) lengthÂ Function #Use theÂ lengthÂ functionÂ to test a collection\u0026rsquo;s length and require a non-empty list or map.\ncondition = length(var.items) != 0 forÂ Expressions #UseÂ forÂ expressionsÂ in conjunction with the functionsÂ alltrueÂ andÂ anytrueÂ to test whether a condition holds for all or for any elements of a collection.\ncondition = alltrue([ for v in var.instances : contains([\u0026#34;t2.micro\u0026#34;, \u0026#34;m3.medium\u0026#34;], v.type) ]) canÂ Function #Use theÂ canÂ functionÂ to concisely use the validity of an expression as a condition. It returnsÂ trueÂ if its given expression evaluates successfully andÂ falseÂ if it returns any error.\ncondition = can(regex(\u0026#34;^[a-z]+$\u0026#34;, var.name)) selfÂ Object #Use theÂ selfÂ object in postcondition blocks to refer to attributes of the instance under evaluation.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { instance_type = \u0026#34;t2.micro\u0026#34; ami = \u0026#34;ami-abc123\u0026#34; lifecycle { postcondition { condition = self.instance_state == \u0026#34;running\u0026#34; error_message = \u0026#34;EC2 instance must be running.\u0026#34; } } } eachÂ andÂ countÂ Objects #In blocks whereÂ for_eachÂ orÂ countÂ are set, useÂ eachÂ andÂ countÂ objects to refer to other resources that are expanded in a chain.\nvariable \u0026#34;vpc_cidrs\u0026#34; { type = set(string) } data \u0026#34;aws_vpc\u0026#34; \u0026#34;example\u0026#34; { for_each = var.vpc_cidrs filter { name = \u0026#34;cidr\u0026#34; values = [each.key] } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;example\u0026#34; { for_each = data.aws_vpc.example vpc_id = each.value.id lifecycle { precondition { condition = data.aws_vpc.example[each.key].state == \u0026#34;available\u0026#34; error_message = \u0026#34;VPC ${each.key} must be available.\u0026#34; } } } More info: Conditional Expressions\nFor Expressions #Expressions likeÂ [for s in var.list : upper(s)], which can transform a complex type value into another complex type value.\nMore info: For Expressions\nSplat Expressions #Expressions likeÂ var.list[*].id, which can extract simpler collections from more complicated expressions.\nAÂ splat expressionÂ provides a more concise way to express a common operation that could otherwise be performed with aÂ forÂ expression.\nIfÂ var.listÂ is a list of objects that all have an attributeÂ id, then a list of the ids could be produced with the followingÂ forÂ expression:\n[for o in var.list : o.id] This is equivalent to the followingÂ splat expression:\nvar.list[*].id The specialÂ [*]Â symbol iterates over all of the elements of the list given to its left and accesses from each one the attribute name given on its right. A splat expression can also be used to access attributes and indexes from lists of complex types by extending the sequence of operations to the right of the symbol:\nvar.list[*].interfaces[0].name More info: Splat Expressions\nDynamic Blocks #A way to create multiple repeatable nested blocks within a resource or other construct.\nWithin top-level block constructs like resources, expressions can usually be used only when assigning a value to an argument using theÂ name = expressionÂ form. This covers many uses, but some resource types include repeatableÂ nested blocksÂ in their arguments, which typically represent separate objects that are related to (or embedded within) the containing object:\nresource \u0026#34;aws_elastic_beanstalk_environment\u0026#34; \u0026#34;tfenvtest\u0026#34; { name = \u0026#34;tf-test-name\u0026#34; # can use expressions here setting { # but the \u0026#34;setting\u0026#34; block is always a literal block } } You can dynamically construct repeatable nested blocks likeÂ settingÂ using a specialÂ dynamicÂ block type, which is supported insideÂ resource,Â data,Â provider, andÂ provisionerÂ blocks:\nresource \u0026#34;aws_elastic_beanstalk_environment\u0026#34; \u0026#34;tfenvtest\u0026#34; { name = \u0026#34;tf-test-name\u0026#34; application = aws_elastic_beanstalk_application.tftest.name solution_stack_name = \u0026#34;64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\u0026#34; dynamic \u0026#34;setting\u0026#34; { for_each = var.settings content { namespace = setting.value[\u0026#34;namespace\u0026#34;] name = setting.value[\u0026#34;name\u0026#34;] value = setting.value[\u0026#34;value\u0026#34;] } } } AÂ dynamicÂ block acts much like aÂ forÂ expression, but produces nested blocks instead of a complex typed value. It iterates over a given complex value, and generates a nested block for each element of that complex value.\nOveruse ofÂ dynamicÂ blocks can make configuration hard to read and maintain, so we recommend using them only when you need to hide details in order to build a clean user interface for a re-usable module.\nAlways write nested blocks out literally where possible.\nMore info: Dynamic Blocks\nValidate your configuration #To verify variable conditions, check blocks, and resource preconditions and postconditions.\nValidate your configuration to improve your module consumer\u0026rsquo;s troubleshooting, make your runs more predictable, and help your maintainers understand your configuration\u0026rsquo;s intent. Input variable validation #Use input variable validation to perform the following tasks:\nVerify input variables meet specific format requirements. Verify input values fall within acceptable ranges. Prevent Terraform operations if a variable is misconfigured. For example, you can validate whether a variable value has valid AMI ID syntax.\nvariable \u0026#34;image_id\u0026#34; { type = string description = \u0026#34;The id of the machine image (AMI) to use for the server.\u0026#34; validation { condition = length(var.image_id) \u0026gt; 4 \u0026amp;\u0026amp; substr(var.image_id, 0, 4) == \u0026#34;ami-\u0026#34; error_message = \u0026#34;The image_id value must be a valid AMI id, starting with \\\u0026#34;ami-\\\u0026#34;.\u0026#34; } } If you set the value of theÂ image_idÂ variable to a string without AMI ID syntax, the condition evaluates toÂ false. When a variable validation fails, Terraform errors, displays the configuredÂ error_message, and stops the operation from proceeding.\nMore info: Validate your configuration\nType Constraints #Syntax for referring to a type, rather than a value of that type. Input variables expect this syntax in theirÂ typeÂ argument.\nMore info: Type Constraints\nVersion Constraints #Syntax of special strings that define a set of allowed software versions. Terraform uses version constraints in several places.\nUse the following syntax to specify version constraints:\nversion = \u0026#34;\u0026lt;operator\u0026gt; \u0026lt;version\u0026gt;\u0026#34; In the following example, Terraform installs a versionsÂ 1.2.0Â and newer, as well as version older thanÂ 2.0.0:\nversion = \u0026#34;\u0026gt;= 1.2.0, \u0026lt; 2.0.0\u0026#34; The following table describes the operators you can use to configure version constraints:\nOperator Description =, no operator Allows only one exact version number. Cannot be combined with other conditions. != Excludes an exact version number. \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= Compares to a specified version. Terraform allows versions that resolve toÂ true. TheÂ \u0026gt;Â andÂ \u0026gt;=Â operators request newer versions. TheÂ \u0026lt;Â andÂ \u0026lt;=Â operators request older versions. ~\u0026gt; Allows only the right-most version component to increment. Examples:\n- ~\u0026gt; 1.0.4: Allows Terraform to installÂ 1.0.5Â andÂ 1.0.10Â but notÂ 1.1.0.\n- ~\u0026gt; 1.1: Allows Terraform to installÂ 1.2Â andÂ 1.10Â but notÂ 2.0. More info: Version Constraints\n\u0026raquo; Sources \u0026laquo; # Terraform Expressions Types and Values Strings and Templates References to ValuesÂ OperatorsÂ Function CallsÂ Conditional ExpressionsÂ For ExpressionsÂ Splat ExpressionsÂ Dynamic BlocksÂ Validate your configuration Type Constraints. Version Constraints ","date":"25 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/terraform-expressions/","section":"ğŸ”°Posts","summary":"Terraform expressions are used to compute values within Terraform configurations, allowing for dynamic and flexible infrastructure management.","title":"Terraform Expressions ğŸ”¥"},{"content":"About Terraform Docs # terraform-docs is a tool that helps you create and format documentation for your terraform modules.\nWhether you\u0026rsquo;re updating providers, adding variables, or tweaking outputs, terraform-docs keeps your module docs accurate and up to date.\nIt supports multiple formats, such as Markdown, AsciiDoc, JSON, and more, and can be extended with plugins and GitHub Action.\nğŸ“º YouTube Tutorial: Effortless Terraform documentation\nGitHub Actions #Original GitHub actions file can be found in the project\u0026rsquo;s GitHub repository, here: https://github.com/terraform-docs/terraform-docs/?tab=readme-ov-file#using-github-actions\nâ„¹ï¸ Below example has been modified to include multiple directories:\nğŸ“„ File: .github/workflows/documentation.yml\n# INFO: Generate Terraform modules documentation in various formats # ? https://terraform-docs.io/ # ? https://github.com/terraform-docs/terraform-docs/ name: Generate terraform docs on: pull_request: jobs: docs: runs-on: ubuntu-latest strategy: matrix: #dir: [\u0026#34;module-a\u0026#34;, \u0026#34;module-b\u0026#34;, \u0026#34;modules/module-c\u0026#34;] dir: [\u0026#34;.\u0026#34;, \u0026#34;terraform-aws\u0026#34;] steps: - uses: actions/checkout@v3 with: ref: ${{ github.event.pull_request.head.ref }} - name: Render terraform docs for directory uses: terraform-docs/gh-actions@main with: working-dir: ${{ matrix.dir }} output-file: README.md output-method: inject git-push: \u0026#34;true\u0026#34; More information: https://terraform-docs.io ","date":"23 October 2025","permalink":"https://robk.uk/posts/devops/iac/terraform/effortless-terraform-documentation/","section":"ğŸ”°Posts","summary":"\u003ccode\u003eterraform-docs\u003c/code\u003e is a tool that helps you create and format documentation for your terraform modules.","title":"Auto-generated Terraform documentation"},{"content":" Modules can be downloaded locally from the Terraform Registry (and modified to your needs) or built from scratch. Example s3_bucket Module #Folder Structure # terraform-project/ â”œâ”€â”€ main.tf â”œâ”€â”€ variables.tf â”œâ”€â”€ outputs.tf â””â”€â”€ modules/ â””â”€â”€ s3_bucket/ â”œâ”€â”€ main.tf â”œâ”€â”€ variables.tf â””â”€â”€ outputs.tf ğŸ“„ File: modules/s3_bucket/variables.tf\nvariable \u0026#34;bucket_name\u0026#34; { description = \u0026#34;Name of the S3 bucket\u0026#34; type = string } variable \u0026#34;acl\u0026#34; { description = \u0026#34;Access control list for the bucket\u0026#34; type = string default = \u0026#34;private\u0026#34; } variable \u0026#34;tags\u0026#34; { description = \u0026#34;Tags to apply to the bucket\u0026#34; type = map(string) default = {} } ğŸ“„ File: modules/s3_bucket/main.tf\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;this\u0026#34; { bucket = var.bucket_name acl = var.acl tags = var.tags } ğŸ“„ File: modules/s3_bucket/outputs.tf\noutput \u0026#34;bucket_id\u0026#34; { description = \u0026#34;The ID of the bucket\u0026#34; value = aws_s3_bucket.this.id } output \u0026#34;bucket_arn\u0026#34; { description = \u0026#34;The ARN of the bucket\u0026#34; value = aws_s3_bucket.this.arn } ğŸ“„ File: Root Module: main.tf\nmodule \u0026#34;my_bucket\u0026#34; { source = \u0026#34;./modules/s3_bucket\u0026#34; bucket_name = \u0026#34;my-terraform-bucket\u0026#34; acl = \u0026#34;private\u0026#34; tags = { Environment = \u0026#34;Dev\u0026#34; Owner = \u0026#34;Rob\u0026#34; } } output \u0026#34;bucket_arn\u0026#34; { value = module.my_bucket.bucket_arn } When to Provide Default Values? # In Terraform, default values in a module\u0026rsquo;s variables.tf file are optional, but they serve specific purposes depending on how you want your module to behave. To Make Inputs Optional\nIf you want a variable to be optional for the user of the module, you must provide a default value. Without a default, the variable becomes required, and Terraform will throw an error if it\u0026rsquo;s not set. variable \u0026#34;acl\u0026#34; { description = \u0026#34;Access control list for the bucket\u0026#34; type = string default = \u0026#34;private\u0026#34; # Makes this optional } To Set Sensible Defaults\nUse defaults to provide common or recommended values so users donâ€™t have to specify them every time. This improves usability and reduces boilerplate. To Support Multiple Environments\nDefaults can help standardize behavior across environments (e.g., default tags, naming conventions). To Avoid Breaking Changes\nWhen updating a module, adding a new variable without a default will break existing usage unless users update their code. Providing a default ensures backward compatibility. When Not to Provide Defaults # If the variable is critical and must be explicitly set (e.g., bucket_name), omit the default to force the user to provide it. If the value depends on external context (e.g., environment-specific settings), it\u0026rsquo;s better to require it. Best Practice # Use defaults for convenience and safety. Avoid defaults for values that must be unique or environment-specific. Always document your variables with description to clarify intent. ","date":"11 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/20-terraform-modules/","section":"ğŸ”°Posts","summary":"Develop local, re-usable Terraform Modules.","title":"Terraform Modules"},{"content":"","date":null,"permalink":"https://robk.uk/categories/aws/","section":"Categories","summary":"","title":"AWS"},{"content":"ğŸ‘¨ğŸ»â€ğŸ’»Part 1: Building VPC manually #VPC Components # Create VPC 2 AZ Public Subnets Private Subnets Create Internet Gateway and Associate to VPC Create NAT Gateway in Public Subnet Create Public Route Table, Add Public Route via Internet Gateway and Associate Public Subnet Create Private Route Table, Add Private Route via NAT Gateway and Associate Private Subnet 1. Create VPC #IAM console \u0026gt; VPC Name: my-manual-vpc IPv4 CIDR Block: 10.0.0.0/16 2. Create Subnets #IAM console \u0026gt; VPC \u0026gt; Subnets â„¹ï¸ For the purpose of the manual exercise, we are only creating subnets in 1 AZ. With terraform we will spread the infrastructure across 2 AZ.\nPublic Subnets # VPC ID: my-manual-vpc Subnet Name: my-public-subnet-1 Availability zone: eu-west-2a IPv4 CIDR Block: 10.0.1.0/24 Private Subnets # Subnet Name: my-private-subnet-1 Availability zone: eu-west-2a IPv4 CIDR Block: 10.0.101.0/24 DB Subnets # VPC ID: my-manual-vpc Subnet Name: my-db-subnet-1 Availability zone: eu-west-2a IPv4 CIDR Block: 10.0.201.0/24 No association with NAT or Internet Gateway is required for the DB subnet as Databases will not require outbound communication. 3. Create IGW and associate it with VPC #IAM console \u0026gt; VPC \u0026gt; Internet gateways Name Tag: my-igw Click on Create Internet Gateway Click on Actions -\u0026gt; Attach to VPC -\u0026gt; my-manual-vpc 4. Create NAT Gateway #ğŸ’¡NAT Gateway should always be placed in the Public Subnet.\nIAM console \u0026gt; VPC \u0026gt; NAT gateways Name: my-nat-gateway Subnet: my-public-subnet-1 Allocate Elastic Ip: click on that Click on Create NAT Gateway 5. Create Public Route Table, Create Routes, Associate Subnets #Create Public Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables Name tag: my-public-route-table vpc: my-manual-vpc Click on Create Create Public Route in newly created Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables \u0026gt; my-public-route-table \u0026gt; Routes Click on Edit Routes \u0026gt; Add Route Add Destination: 0.0.0.0/0 Target: my-igw Click on Save Route Associate Public Subnet 1 in Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables \u0026gt; my-public-route-table \u0026gt; Subnet associations \u0026gt; Explicit subnet associations Click on Edit Subnet Associations Select my-public-subnet-1 Click on Save 6. Create Private Route Table, Create Routes, Associate Subnets #Create Private Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables Name tag: my-private-route-table vpc: my-manual-vpc Click on Create Create Private Route in newly created Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables \u0026gt; my-private-route-table \u0026gt; Routes Click on Edit Routes \u0026gt; Add Route Destination: 0.0.0.0/0 Target: my-nat-gateway Click on Save Route Associate Private Subnet 1 in Route Table #IAM console \u0026gt; VPC \u0026gt; Route tables \u0026gt; my-public-route-table \u0026gt; Subnet associations \u0026gt; Explicit subnet associations Click on Edit Subnet Associations Select my-private-subnet-1 Click on Save 7. Clean-Up # Delete my-nat-gateway Wait till NAT Gateway is deleted Delete my-manual-vpc ğŸ“„Part 2: Building VPC using Terraform #File: ğŸ“„c2-generic-variables.tf\n# INFO: Input Variables # INFO: https://developer.hashicorp.com/terraform/language/block/variable # INFO: AWS Region variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;Region in which AWS Resources will be created\u0026#34; type = string default = \u0026#34;eu-west-2\u0026#34; } # INFO: Environment Variable variable \u0026#34;environment\u0026#34; { description = \u0026#34;Environment Variable used as a prefix\u0026#34; type = string default = \u0026#34;DEV\u0026#34; } # INFO: Business Division variable \u0026#34;business_division\u0026#34; { description = \u0026#34;Business Division in the large organization this Infrastructure belongs\u0026#34; type = string default = \u0026#34;Operations\u0026#34; } # ! Default values will be overwritten in terraform.tfvars File: ğŸ“„c3-local-values.tf\n# INFO: Local Values # INFO: https://developer.hashicorp.com/terraform/language/block/locals # INFO: slice Function used for AZ\u0026#39;s: https://developer.hashicorp.com/terraform/language/functions/slice data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; {} locals { owners = var.business_division environment = var.environment name = \u0026#34;${var.business_division}-${var.environment}\u0026#34; #name = \u0026#34;${local.owners}-${local.environment}\u0026#34; azs = slice(data.aws_availability_zones.available.names, 0, 2) common_tags = { owners = local.owners environment = local.environment } } File: ğŸ“„c4-01-vpc-variables.tf\n# INFO: VPC Input Variables required by VPC module # INFO: https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest # INFO: VPC Name variable \u0026#34;vpc_name\u0026#34; { description = \u0026#34;VPC Name\u0026#34; type = string default = \u0026#34;myvpc\u0026#34; } # INFO: VPC CIDR Block variable \u0026#34;vpc_cidr\u0026#34; { description = \u0026#34;VPC CIDR Block\u0026#34; type = string default = \u0026#34;10.0.0.0/16\u0026#34; } File: ğŸ“„c4-02-vpc-module.tf\n# INFO: Create VPC using Terraform Module # INFO: https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest module \u0026#34;vpc\u0026#34; { source = \u0026#34;./modules/aws-vpc\u0026#34; #version = \u0026#34;~\u0026gt; 6.4.0\u0026#34; name = \u0026#34;${local.name}-${var.vpc_name}\u0026#34; cidr = var.vpc_cidr azs = local.azs private_subnets = [for k, v in local.azs : cidrsubnet(var.vpc_cidr, 8, k)] public_subnets = [for k, v in local.azs : cidrsubnet(var.vpc_cidr, 8, k + 4)] database_subnets = [for k, v in local.azs : cidrsubnet(var.vpc_cidr, 8, k + 8)] create_database_subnet_group = true create_database_subnet_route_table = true enable_nat_gateway = true single_nat_gateway = true tags = local.common_tags } File: ğŸ“„c4-03-vpc-outputs.tf\n# INFO: Output VPC ID output \u0026#34;vpc_id\u0026#34; { description = \u0026#34;The ID of the VPC\u0026#34; value = module.vpc.vpc_id } # INFO: Output VPC CIDR block output \u0026#34;vpc_cidr_block\u0026#34; { description = \u0026#34;The CIDR block of the VPC\u0026#34; value = module.vpc.vpc_cidr_block } # INFO: Output Private Subnets Information output \u0026#34;private_subnets\u0026#34; { description = \u0026#34;List of IDs of private subnets\u0026#34; value = module.vpc.private_subnets } output \u0026#34;private_subnets_cidr_blocks\u0026#34; { description = \u0026#34;List of cidr_blocks of private subnets\u0026#34; value = module.vpc.private_subnets_cidr_blocks } # INFO: Output Public Subnets Information output \u0026#34;public_subnets\u0026#34; { description = \u0026#34;List of IDs of public subnets\u0026#34; value = module.vpc.public_subnets } output \u0026#34;public_subnets_cidr_blocks\u0026#34; { description = \u0026#34;List of cidr_blocks of public subnets\u0026#34; value = module.vpc.public_subnets_cidr_blocks } # INFO: Output Database Subnets Information output \u0026#34;database_subnets\u0026#34; { description = \u0026#34;List of IDs of database subnets\u0026#34; value = module.vpc.database_subnets } output \u0026#34;database_subnets_cidr_blocks\u0026#34; { description = \u0026#34;List of cidr_blocks of database subnets\u0026#34; value = module.vpc.database_subnets_cidr_blocks } output \u0026#34;database_subnet_group\u0026#34; { description = \u0026#34;ID of database subnet group\u0026#34; value = module.vpc.database_subnet_group } # INFO: Output NAT Gateway route IDs output \u0026#34;private_nat_gateway_route_ids\u0026#34; { description = \u0026#34;List of IDs of the private nat gateway route\u0026#34; value = module.vpc.private_nat_gateway_route_ids } # INFO: Output Availability Zones output \u0026#34;azs\u0026#34; { description = \u0026#34;A list of availability zones spefified as argument to this module\u0026#34; value = local.azs } File: ğŸ“„terraform.tfvars\n# ! This will overwrite default values from c2-generic-variables.tf aws_region = \u0026#34;eu-west-2\u0026#34; environment = \u0026#34;UAT\u0026#34; business_division = \u0026#34;Training\u0026#34; File: ğŸ“„vpc.auto.tfvars\n# ! This will overwrite default values from c2-generic-variables.tf vpc_name = \u0026#34;myvpc\u0026#34; vpc_cidr = \u0026#34;10.0.0.0/16\u0026#34; \u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"6 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/06-vpc/","section":"ğŸ”°Posts","summary":"Building AWS VPC using Terraform.","title":"Building AWS VPC"},{"content":" Meta-arguments are a class of arguments built into the Terraform configuration language that control how Terraform creates and manages your infrastructure. You can use meta-arguments in any type of resource. You can also use most meta-arguments inÂ moduleÂ blocks.\nMore info: https://developer.hashicorp.com/terraform/language/meta-arguments\ndepends_on #TheÂ depends_onÂ meta-argument instructs Terraform to complete all actions on the dependency object, includingÂ readÂ operations, before performing actions on the object declaring the dependency. Use theÂ depends_onÂ argument to explicitly set the order in which Terraform creates resources. Refer to theÂ depends_onÂ referenceÂ for details.\ncount #By default, Terraform configures one infrastructure object for eachÂ resource,Â module, andÂ ephemeralÂ block. Terraform also creates single instances of a module perÂ moduleÂ block. You can add theÂ countÂ argument toÂ resource,Â module, andÂ ephemeralÂ blocks to create and manage multiple instances of each without writing a separate block for each instance. Refer to theÂ countÂ referenceÂ for details.\nfor_each #By default, Terraform configures one infrastructure object for eachÂ resource,Â module, andÂ ephemeralÂ block. You can add theÂ for_eachÂ block to yourÂ resource,Â data,Â module, andÂ ephemeralÂ blocks to create and manage several similar objects, such as a fixed pool of compute instances, without writing a separate block for each instance. Refer to theÂ for_eachÂ referenceÂ for details.\nğŸ“„ File: c5-ec2instance.tf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # INFO: Create EC2 Instance # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#example-usage # INFO: First retrieve all available availability zones in the region #INFO: https://registry.terraform.io/providers/-/aws/latest/docs/data-sources/availability_zones # INFO: Gather all Availability Zones in your respective Region (as defined in c2-variables.tf) data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;my_azones\u0026#34; { filter { name = \u0026#34;opt-in-status\u0026#34; values = [\u0026#34;opt-in-not-required\u0026#34;] } } # INFO: EC2 Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;myec2vm\u0026#34; { ami = data.aws_ami.amzlinux2.id # NOTE: Referencing List and Map variables #instance_type = var.instance_type #instance_type = var.instance_type_list[1] # NOTE: Accessing variable of a type \u0026#34;list\u0026#34; instance_type = var.instance_type_map[\u0026#34;dev\u0026#34;] # NOTE: Accessing variable of a type \u0026#34;map\u0026#34; user_data = file(\u0026#34;${path.module}/app1-install.sh\u0026#34;) # NOTE: Apply User Data key_name = var.instance_keypair # NOTE: Attach Key-Pair ID vpc_security_group_ids = [ # NOTE: Attach INGRESS SG aws_security_group.vpc-ssh.id, aws_security_group.vpc-web-80.id, aws_security_group.vpc-web-443.id, aws_security_group.vpc-egress.id # NOTE: Attach EGRESS SG ] # NOTE: Create EC2 Instance in all Availabilty Zones of a VPC for_each = toset(data.aws_availability_zones.my_azones.names) availability_zone = each.key # NOTE: You can also use each.value because for list items each.key == each.value tags = { \u0026#34;Name\u0026#34; = \u0026#34;for_each-Demo-${each.value}\u0026#34; } } More about for_each:\nhttps://developer.hashicorp.com/terraform/language/meta-arguments#for_each lifecycle #TheÂ lifecycleÂ block accepts a rule that customizes how Terraform performs the lifecycle stages for each resource. Support for eachÂ lifecycleÂ rule varies across Terraform configuration blocks. Refer to theÂ lifecycleÂ referenceÂ for details.\nprovider #By default, Terraform determines the local name of the provider from the first word in the resource type and uses that provider\u0026rsquo;s default configuration to create the resource. You can add multipleÂ providerÂ blocks to your configuration and use theÂ providerÂ argument to a resource definition to specify which provider it should use. Refer to theÂ providerÂ referenceÂ for details.\nproviders #By default, child modules inherit the default provider configurations of their parent module. You can specify an alternate provider configuration in theÂ moduleÂ block using theÂ providersÂ argument. TheÂ providersÂ argument instructs Terraform to use the reference provider configuration to create the module resources. Refer to theÂ providersÂ referenceÂ for details.\nVariable List and Map #ğŸ“„ File: c2-variables.tf\n# INFO: Redefining \u0026#34;instance_type\u0026#34; variable to use List and / or Map # INFO: EC2 Instance Type - List variable \u0026#34;instance_type_list\u0026#34; { description = \u0026#34;EC2 Instance Type(List)\u0026#34; type = list(string) # NOTE: Define list of strings variable type default = [\u0026#34;t3.nano\u0026#34;, \u0026#34;t3.micro\u0026#34;] # NOTE: (Multiple) default values } # INFO: EC2 Instance Type - Map variable \u0026#34;instance_type_map\u0026#34; { description = \u0026#34;EC2 Instance Type(Map)\u0026#34; type = map(string) # NOTE: Define map of strings default = { \u0026#34;dev\u0026#34; = \u0026#34;t3.nano\u0026#34; # NOTE: Define default string for dev \u0026#34;qa\u0026#34; = \u0026#34;t3.micro\u0026#34; # NOTE: Define default string for qa \u0026#34;prod\u0026#34; = \u0026#34;t3.small\u0026#34; # NOTE: Define default string for prod } } ğŸ“„ File: c5-ec2instance.tf\n# INFO: Create EC2 Instance # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#example-usage # EC2 Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;myec2vm\u0026#34; { ami = data.aws_ami.amzlinux2.id # NOTE: Referencing List and Map variables #instance_type = var.instance_type #instance_type = var.instance_type_list[1] # NOTE: Accessing variable of a type \u0026#34;list\u0026#34; instance_type = var.instance_type_map[\u0026#34;dev\u0026#34;] # NOTE: Accessing variable of a type \u0026#34;map\u0026#34; user_data = file(\u0026#34;${path.module}/app1-install.sh\u0026#34;) # NOTE: Apply User Data key_name = var.instance_keypair # NOTE: Attach Key-Pair ID vpc_security_group_ids = [ # NOTE: Attach INGRESS SG aws_security_group.vpc-ssh.id, aws_security_group.vpc-web-80.id, aws_security_group.vpc-web-443.id, aws_security_group.vpc-egress.id # NOTE: Attach EGRESS SG ] count = \u0026#34;2\u0026#34; # NOTE: Add count Meta-Argument to create a number of the same resoure type tags = { \u0026#34;Name\u0026#34; = \u0026#34;Count Demo ${count.index}\u0026#34; # NOTE: Update the name to reflect \u0026#34;count.index\u0026#34; to iterate \u0026#34;Description\u0026#34; = \u0026#34;Variable Lists, Maps and Meta-Arguments\u0026#34; } } Drawbacks of using count in this example\nResource Instances in this case were identified using index numbers instead of string values like actual subnet_id If an element was removed from the middle of the list, every instance after that element would see its subnet_id value change, resulting in more remote object changes than intended Even the subnet_ids should be pre-defined or we need to get them again using for_each or for using various datasources Using for_each gives the same flexibility without the extra churn For Loops and Splat Operators in Outputs #ğŸ“„ File: c6-outputs.tf\n# INFO: Terraform Output Values # INFO: https://developer.hashicorp.com/terraform/language/block/output /* Concepts Covered 1. For Loop with List 2. For Loop with Map 3. For Loop with Map Advanced 4. Legacy Splat Operator (latest) - Returns List 5. Latest Generalized Splat Operator - Returns the List */ # Output - For Loop with List output \u0026#34;for_output_list\u0026#34; { description = \u0026#34;For Loop with List\u0026#34; value = [for instance in aws_instance.myec2vm : instance.public_dns] # NOTE: Accessing list via square brackets } # Output - For Loop with Map output \u0026#34;for_output_map1\u0026#34; { description = \u0026#34;For Loop with Map\u0026#34; value = { for instance in aws_instance.myec2vm : instance.id =\u0026gt; instance.public_dns # NOTE: Accessing map via \u0026#34;flower\u0026#34; brackets. Maps are key-value. } } # Output - For Loop with Map Advanced output \u0026#34;for_output_map2\u0026#34; { description = \u0026#34;FOr Loop with Map - Advanced\u0026#34; value = { for c, instance in aws_instance.myec2vm : c =\u0026gt; instance.public_dns # NOTE: For c means for each count (like count.index) } } # Output Legacy Splat Operator (Legacy) - Returns the List /* output \u0026#34;legacy_splat_instance_publicdns\u0026#34; { description = \u0026#34;Legacy Splat Operator\u0026#34; value = aws_instance.myec2vm.*.public_dns } */ # Output Latest Generalized Splat Operator - Returns the List output \u0026#34;latest_splat_instance_publicdns\u0026#34; { description = \u0026#34;Generalized latest Splat Operator\u0026#34; value = aws_instance.myec2vm[*].public_dns } \u0026raquo; Sources \u0026laquo; #Terraform:\nhttps://developer.hashicorp.com/terraform/language/meta-arguments availability_zones datasource\nhttps://registry.terraform.io/providers/-/aws/latest/docs/data-sources/availability_zones Kalyanâ€™s GitHub Repositories:\nhttps://github.com/stacksimplify/terraform-on-aws-ec2/tree/main/05-Terraform-Loops-MetaArguments-SplatOperator\n\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"5 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/05-loops-metaarguments-splatoperat-functions/","section":"ğŸ”°Posts","summary":"Terraform Loops, Meta-Arguments, Splat Operator and Functions.","title":"Terraform: Loops, Meta-Arguments, Splat Operator and Functions"},{"content":"Terraform Input Variables # Input Variables serve as parameters for a Terraform module allowing aspects of the module to be customized without altering the module\u0026rsquo;s source code and allowing modules to be shared between different configurations. Basic Input Variables Can be provided when prompted during terraform plan or terraform apply Override default variable values using CLI argument -var Override default variable values using Environment Variables (TF_VAR_aa) Provide Input Variables using terraform.tfvars files Provide Input Variables using \u0026lt;any-name\u0026gt;.tfvars file with CLI argument -var-file Provide Input Variables using auto.tfvars files Implement complex type constructors like List \u0026amp; Map in Input Variables Implement Custom Validation Rules in Variables Protect Sensitive Input Variables More detailed section about Terraform Input Variables and practical examples in Kalyanâ€™s hashicorp-certified-terraform-associate GitHub Repository under this address:\nhttps://github.com/stacksimplify/hashicorp-certified-terraform-associate/tree/main/05-Terraform-Variables\nImplement Input Variables #Declare Variables #ğŸ“„ File: c2-variables.tf\n# INFO: Input Variables # INFO: https://developer.hashicorp.com/terraform/language/block/variable # INFO: AWS Region variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;Region in which AWS Resources will be created\u0026#34; type = string default = \u0026#34;eu-west-2\u0026#34; } # INFO: EC2 Instance Type variable \u0026#34;instance_type\u0026#34; { description = \u0026#34;EC2 Instance Type\u0026#34; type = string default = \u0026#34;t3.nano\u0026#34; } # INFO: EC2 Instance Key Pair variable \u0026#34;instance_keypair\u0026#34; { description = \u0026#34;EC2 Instance Key Pair associated with EC2 Instance\u0026#34; type = string default = \u0026#34;terraform-key\u0026#34; } Terraform Block #ğŸ“„ File: c1-versions.tf\n# INFO: Terraform Block # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs#example-usage terraform { required_version = \u0026#34;~\u0026gt; 1.13.0\u0026#34; # NOTE: Greater than 1.13.2. Only the most upright version number (.0) can change. required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; # NOTE: Greater than 6.0. Only the most upright version number (.0) can change. } } } # INFO: Provider Block provider \u0026#34;aws\u0026#34; { region = var.aws_region } â„¹ï¸ Note: region = var.aws_region is now referring to a variable declared in c1-variables.tf file.\nAWS Provider\nMore info:\nTerraform Input Variables Terraform Input Variable Usage - 10 different types Create Security Groups #ğŸ“„ File: c3-ec2securitygroups.tf\n# INFO: Create Ingress Security Group - SSH Traffic # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group#example-usage resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpc-ssh\u0026#34; { name = \u0026#34;vpc-ssh\u0026#34; description = \u0026#34;Dev VPC SSH\u0026#34; tags = { Name = \u0026#34;vpc-ssh\u0026#34; } } resource \u0026#34;aws_vpc_security_group_ingress_rule\u0026#34; \u0026#34;vpc-ssh_ipv4\u0026#34; { description = \u0026#34;Allow Port 22 INBOUND\u0026#34; security_group_id = aws_security_group.vpc-ssh.id cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; from_port = 22 ip_protocol = \u0026#34;tcp\u0026#34; to_port = 22 tags = { \u0026#34;Name\u0026#34; = \u0026#34;vpc-ssh-inbound\u0026#34; } } # INFO: Create Ingress Security Group - WEB Traffic - 80 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpc-web-80\u0026#34; { name = \u0026#34;vpc-web-80\u0026#34; description = \u0026#34;Dev VPC WEB\u0026#34; tags = { Name = \u0026#34;vpc-web-80\u0026#34; } } resource \u0026#34;aws_vpc_security_group_ingress_rule\u0026#34; \u0026#34;vpc-web-80_ipv4\u0026#34; { description = \u0026#34;Allow Port 80 INBOUND\u0026#34; security_group_id = aws_security_group.vpc-web-80.id cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; from_port = 80 ip_protocol = \u0026#34;tcp\u0026#34; to_port = 80 tags = { \u0026#34;Name\u0026#34; = \u0026#34;vpc-web-80-inbound-80\u0026#34; } } # INFO: Create Ingress Security Group - WEB Traffic - 80 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpc-web-443\u0026#34; { name = \u0026#34;vpc-web-443\u0026#34; description = \u0026#34;Dev VPC WEB\u0026#34; tags = { Name = \u0026#34;vpc-web-443\u0026#34; } } resource \u0026#34;aws_vpc_security_group_ingress_rule\u0026#34; \u0026#34;vpc-web-443_ipv4\u0026#34; { description = \u0026#34;Allow Port 443 INBOUND\u0026#34; security_group_id = aws_security_group.vpc-web-443.id cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; from_port = 443 ip_protocol = \u0026#34;tcp\u0026#34; to_port = 443 tags = { \u0026#34;Name\u0026#34; = \u0026#34;vpc-web-443-inbound-443\u0026#34; } } # INFO: Create Egress Security Group - ALL resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpc-egress\u0026#34; { name = \u0026#34;vpc-egress\u0026#34; description = \u0026#34;Dev VPC Egress\u0026#34; tags = { Name = \u0026#34;vpc-egress\u0026#34; } } resource \u0026#34;aws_vpc_security_group_egress_rule\u0026#34; \u0026#34;allow_all_traffic_ipv4\u0026#34; { description = \u0026#34;Allow all IP and pports OUTBOUND\u0026#34; security_group_id = aws_security_group.vpc-egress.id cidr_ipv4 = \u0026#34;0.0.0.0/0\u0026#34; ip_protocol = \u0026#34;-1\u0026#34; # semantically equivalent to all ports tags = { \u0026#34;Name\u0026#34; = \u0026#34;vpc-all-outbound\u0026#34; } } More about aws_security_group resource:\nResource: aws_security_group Terraform Datasources # Data Sources allow data to be fetched or computed for use elsewhere in Terraform configuration.\nUse of data sources allows Terraform configuration to make use of information defined outside of Terraform or defined by another separate Terraform configuration.\nA data source is accessed via a special kind of resource known as a data resource which determines the kind of object (objects) it reads and what query constraint arguments are available.\nData resources have the same dependency resolution behavior as defined or managed resources.\nSetting the depends_on meta-argument within the data blocks defers reading of the data source until after all changes to the dependencies have been applier.\nCreate AMI Datasource Resource #ğŸ“„ File: c4-ami-datasource.tf\n# INFO: Get the latest AWS AMI ID for Amazon2 Linux # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/ami#example-usage data \u0026#34;aws_ami\u0026#34; \u0026#34;amzlinux2\u0026#34; { # executable_users = [\u0026#34;self\u0026#34;] most_recent = true # name_regex = \u0026#34;^myami-[0-9]{3}\u0026#34; owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;al2023-ami-*-x86_64\u0026#34;] } filter { name = \u0026#34;architecture\u0026#34; values = [\u0026#34;x86_64\u0026#34;] } filter { name = \u0026#34;root-device-type\u0026#34; values = [\u0026#34;ebs\u0026#34;] } filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } } More info:\nAMI Datasource ğŸ’¡ Finding AMI for filter value values = [\u0026quot;al2023-ami-*-x86_64\u0026quot;]:\nCreate EC2 Instance #ğŸ“„ File: c5-ec2instance.tf\n# INFO: Create EC2 Instance # INFO: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#example-usage # EC2 Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;myec2vm\u0026#34; { ami = data.aws_ami.amzlinux2.id instance_type = var.instance_type user_data = file(\u0026#34;${path.module}/app1-install.sh\u0026#34;) # NOTE: Apply User Data key_name = var.instance_keypair # NOTE: Attach Key-Pair ID vpc_security_group_ids = [ # NOTE: Attach INGRESS SG aws_security_group.vpc-ssh.id, aws_security_group.vpc-web-80.id, aws_security_group.vpc-web-443.id, aws_security_group.vpc-egress.id # NOTE: Attach EGRESS SG ] tags = { \u0026#34;Name\u0026#34; = \u0026#34;EC2 Demo 2\u0026#34; } } More about EC2 Instance resource:\nResource: aws_instance Terraform Output Values # The output block lets you expose information about your infrastructure. ğŸ“„ File: c6-outputs.tf\n# INFO: Terraform Output Values # INFO: https://developer.hashicorp.com/terraform/language/block/output output \u0026#34;instance_publicip\u0026#34; { description = \u0026#34;EC2 Instance Public IP\u0026#34; value = aws_instance.myec2vm.public_ip } output \u0026#34;instance_publicdns\u0026#34; { description = \u0026#34;EC2 Instance Public DNS\u0026#34; value = aws_instance.myec2vm.public_dns } More about Output Values:\nOutput Block Reference \u0026raquo; Sources \u0026laquo; #Terraform:\nTerraform Input Variables Resource: aws_security_group AMI Datasource Resource: aws_instance Output Block Reference Kalyanâ€™s GitHub Repositories:\nhttps://github.com/stacksimplify/terraform-on-aws-ec2/tree/main/04-Terraform-Variables-and-Datasources\n\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"4 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/04-variables-datasources-output/","section":"ğŸ”°Posts","summary":"Terraform Variables, Datasources and Output Values.","title":"Terraform: Variables, Datasources and Output Values ğŸ”¥"},{"content":"Terraform Blocks #Terraform Block # Special block used to configure some behaviors Required Terraform Version List Required Providers Terraform Backend Within a Terraform Block only constant values can be used. Arguments may not refer to named objects such as resources, input variables, etc, and may not use any of the Terraform language built-in functions. Provider Block # Heart of the Terraform Terraform is relying on providers to interact with remote systems Declare providers for Terraform to install and use them Provider configurations belong to Root Module ğŸ“„File: c1-versions.tf\n# INFO: Terraform Block terraform { required_version = \u0026#34;~\u0026gt; 1.13.0\u0026#34; # NOTE: Greater than 1.13.2. Only the most upright version number (.0) can change. required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; # NOTE: Greater than 6.0. Only the most upright version number (.0) can change. } } } # INFO: Provider Block provider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-west-2\u0026#34; } required_version = \u0026quot;~\u0026gt; 1.13.2\u0026quot; means that valid versions will be: 1.13.2-9\nrequired_version = \u0026quot;~\u0026gt; 1.13\u0026quot; means that valid versions will be: 1.13-99 # INFO: Provider Block provider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-west-2\u0026#34; # profile = \u0026#34;dev-account\u0026#34; profile = \u0026#34;terraform-user\u0026#34; } It is possible to create multiple profiles in the ~/.aws/credentials settings. Those profiles can then be referenced in the Provider block and used to connect to your Provider. Resource Block # Each Resource Block describes one or more Infrastructure Objects Resource Syntax: How to declare Resources? Resource Behavior: How Terraform handles resource declarations? Provisioners: We can configure Resource post-creation actions ğŸ“„File: c2-ec2instance.tf\n# Resource: EC2 Instance resource \u0026#34;aws_instance\u0026#34; \u0026#34;myec2vm\u0026#34; { ami = \u0026#34;ami-0742b4e673072066f\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; user_data = file(\u0026#34;${path.module}/app1-install.sh\u0026#34;) tags = { \u0026#34;Name\u0026#34; = \u0026#34;EC2 Demo\u0026#34; } } Resource Behavior # Terraform Resource Create Resource Create resources that exist in the configuration but are not associated with a real infrastructure object in the state. Destroy Resource Destroy resources that exist in the state but no longer exist in the configuration. Update in-place Resources Update in-place resources whose arguments have changed. Destroy and re-create Destroy and re-create resources whose arguments have changed but cannot be updated in-place due to remote API limitations. Terraform State #The primary purpose of Terraform State is to store bindings between objects on a remote system and resource instances declared in your configuration.\nTerraform State file terraform.tfstate terraform.tfstate file is being created / updated every time terraform plan command is being executed.\nImportant! #The terraform state file is the only way Terraform can track which resources it is managing. It often contains sensitive information so must be stored securely and access must be restricted.\nTerraform Registry #â„¹ï¸ https://registry.terraform.io\nTerraform Registry is a selection of Providers and Modules used by Terraform.\nProvider Badges:\nOfficial - Owned and maintained by HashiCorp Verified - Owned and maintained by third-party technology partners. HashiCorp has verified the authenticity of the Provider\u0026rsquo;s publisher Community - Community providers are published to the Terraform Registry by individual maintainers, groups of maintainers or other members of the Terraform community Archived - Official or Verified Providers that are no longer maintained. \u0026raquo; Sources \u0026laquo; # More about Resources Create EC2 Instance Resource More about File Function More about Resources - Argument Reference More about Resources - Attribute Reference \u0026raquo; References \u0026laquo; # Terraform modules \u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"3 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/03-settings-providers-resources/","section":"ğŸ”°Posts","summary":"A word or two about Terraform blocks, resources, Terraform Registry.","title":"Terraform: Settings, Providers and Resources"},{"content":"\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":null,"permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/","section":"ğŸ”°Posts","summary":"Learnings from \u003cstrong\u003eKalyan Reddy Daida\u003c/strong\u003eâ€™s \u0026ldquo;\u003cem\u003eTerraform on AWS with SRE \u0026amp; IaC DevOps\u003c/em\u003e\u0026rdquo; course on \u003cem\u003eUdemy\u003c/em\u003e.","title":"Terraform on AWS with SRE \u0026 IaC DevOps"},{"content":"Prerequisite # Install Terraform CLI Install AWS CLI Install VS Code Editor Install HashiCorp Terraform plugin for VS Code Terraform basic commands # terraform init used to initialize a working directory that contains terraform config files this is the first command that should be run after writing new terraform configuration it will download the provider plugins .terraform.lock.hcl file is created to record the provider selections. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run terraform init in the future. terraform init -upgrade - command to upgrade the provider in Terraform. This command will initialize the working directory and upgrade the provider to the latest version available, including any additional functionality that may have been added. If Terraform was already initiated and providers were downloaded, Terraform will not upgrade to the new version by default. terraform validate validates syntax and consistency of the terraform files terraform plan creates an execution plan terraform performs a refresh and determines what actions are necessary to achieve the desired state specified in the configuration files terraform apply will apply the changes required to be in line with the desired state by default, apply will scan the current directory and apply it\u0026rsquo;s configuration (provision the infrastructure) terraform apply -refresh-only - detect drift between the Terraform configuration and the actual state of the resources in the cloud provider. It will update the state file with any changes made outside of Terraform, ensuring that the configuration remains in sync with the actual resources. terraform apply -replace=\u0026lt;resource\u0026gt; - allows you to tag the specific resource for replacement without impacting the rest of the managed infrastructure. This ensures that only the tagged resource is recreated, potentially resolving any issues with the local script execution. terraform destroy will destroy the terraform-managed infrastructure terraform destroy -target \u0026lt;virtual machine\u0026gt; - destroy only the target resource. Other useful commands # terraform fmt - HashiCorp recommends using consistent formatting in all config files by using the terraform fmt command. Optional command, helps with formatting terraform state command can be used to check what resources have been deployed against the terraform state file list - list resources in the state show - show a resource in the state terraform state rm - will remove the specified resource from the Terraform state, allowing you to then run terraform destroy to remove all remaining resources except for the resource that was removed from the state. This approach effectively excludes the specified resource from the destruction process. The terraform state command and its subcommands can be used for various tasks related to the Terraform state. Some of the tasks that can be performed using the terraform state command are:\nInspecting the Terraform state: The terraform state show subcommand can be used to display the current state of a Terraform configuration. This can be useful for verifying the current state of resources managed by Terraform.\nUpdating the Terraform state: The terraform state mv and terraform state rm subcommands can be used to move and remove resources from the Terraform state, respectively.\nExample: terraform state mv aws_s3_bucket.data-bucket aws_s3_bucket.data-bucket-prod\nPulling and pushing the Terraform state: The terraform state pull and terraform state push subcommands can be used to retrieve and upload the Terraform state from and to a remote backend, respectively. This is useful when multiple users or systems are working with the same Terraform configuration.\nImporting resources into Terraform: The terraform state import subcommand can be used to import existing resources into the Terraform state. This allows Terraform to manage resources that were created outside of Terraform.\nBy using the terraform state command and its subcommands, users can manage and manipulate the Terraform state in various ways, helping to ensure that their Terraform configurations are in the desired state.\nhttps://developer.hashicorp.com/terraform/cli/commands/state/list\nhttps://developer.hashicorp.com/terraform/cli/state\nMore info: Terraform State\nterraform workspace delete - Delete a workspace list - List Workspaces new - Create a new workspace select - Select a workspace show - Show the name of the current workspace Terraform workspaces are a feature that allow you to manage multiple instances of your infrastructure using the same configuration files, each with its own isolated state file. This helps in organizing different environments, such as development and production, without interfering with each other. terraform show - Display the current state of the resources being managed by Terraform. It provides detailed information about the infrastructure that Terraform is managing, including resource attributes, dependencies, and configuration. terraform output - Reads an output variable from a Terraform state file and prints the value. terraform refresh - Update the state file of your infrastructure with metadata that matches the physical resources they are tracking The difference between terraform refresh and terraform apply -refresh-only:\nterraform refresh updates the state file to match real infrastructure but does not produce or apply a plan, while terraform apply -refresh-only runs through the full plan/apply workflow but only refreshes the state (no resource changes), making it auditable and consistent with Terraformâ€™s apply process.\nterraform refresh is being considered a legacy command. Newer workflows encourage apply -refresh-only.\nterraform providers- Prints out a tree of modules in the referenced configuration annotated with their provider requirements It is said that best terraform practice is to output the terraform plan to a file and then run terraform apply with the file input.\nterraform plan -out tf.plan terraform apply tf.plan *terraform will not ask for confirmation (just like using it with the -auto-approve option) when executing the plan from the file.\nThe terraform state file is the only way Terraform can track which resources it is managing. It often contains sensitive information so must be stored securely and access must be restricted.\nterraform graph - used to generate a visual representation of either a configuration or execution plan. The output is in the DOT format, which can be used by GraphViz to generate charts. terraform graph -type=plan | dot -Tpng \u0026gt;graph.png More info: https://developer.hashicorp.com/terraform/cli/commands/graph\nTerraform Configuration Syntax #Source: Terraform Language Syntax\nTerraform Blocks # Code in Terraform language is stored in plain text files ended with the .tf extension. Those are called Terraform Configuration Files or Terraform Manifests. 2 types of blocks:\nTop Level resource provider Block inside Block provisioners resource-specific block tags Terraform Arguments, Attributes and Meta-Arguments # Terraform Arguments are the Input Values. Terraform Attributes are the Output Values. Arguments can be required or optional. resource \u0026#34;aws_instance\u0026#34; \u0026#34;ec2\u0026#34; { ami = \u0026#34;ami-08f714c552929eda9\u0026#34; instance_type = \u0026#34;t2.nano\u0026#34; } Attributes are values exposed by a particular resource. It\u0026rsquo;s format looks like:\nresource_type.resource_name.attribute_name\nMeta-Arguments change a resource type\u0026rsquo;s behavior\ncount depends_on for_each lifecycle provider aws_instance resource documentation: https://registry.terraform.io/providers/-/aws/latest/docs/resources/instance contains references to all Arguments and Attributes. More info:\nAdditional Reference Resource: AWS Instance Resource: AWS Instance Argument Reference Resource: AWS Instance Attribute Reference Resource: Meta-Arguments Top-Level Blocks # Terraform Settings Block Provider Block Resource Block Input Variables Block Output Values Block Local Values Block Data Sources Block Modules Block Terraform Blocks: https://github.com/stacksimplify/hashicorp-certified-terraform-associate/tree/main/03-Terraform-Fundamental-Blocks\nTerraform Resources: https://github.com/stacksimplify/hashicorp-certified-terraform-associate/tree/main/04-Terraform-Resources\nTerraform Comments #3 ways to make comments in Terraform:\nSingle-line: # // Multi-line: /* Multi-line comments Line 1 Line 2 */ Terraform Modules # Terraform Modules are containers for multiple resources that are used together. A module consists of a collection of .tf files kept together in a directory. Modules are the main way of packaging and reusing resource configurations in Terraform Every Terraform configuration has at least one module, known as it\u0026rsquo;s root module, which consists of the resources defined in the .tf files placed in the main working directory A Terraform module (usually the root module of a configuration) can call other modules to include their resources into the configuration A module that has been called by another module is often referred to as a child module Child modules can be called multiple times within the same configuration and multiple configurations can use the same child module In addition to modules from the local system, Terraform can load modules from a public or private registry It is therefore possible to publish modules for others to use and to use modules published by others Version Constraints #Source:\nVersion Constraints Version Constraints Best Practices Use the following syntax to specify version constraints:\nversion = \u0026#34;\u0026lt;operator\u0026gt; \u0026lt;version\u0026gt;\u0026#34; In the following example, Terraform installs a versions 1.2.0 and newer, as well as version older than 2.0.0:\nversion = \u0026#34;\u0026gt;= 1.2.0, \u0026lt; 2.0.0\u0026#34; Operators #The following table describes the operators you can use to configure version constraints:\nOperator Description =, no operator Allows only one exact version number. Cannot be combined with other conditions. != Excludes an exact version number. \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= Compares to a specified version. Terraform allows versions that resolve to true. The \u0026gt; and \u0026gt;= operators request newer versions. The \u0026lt; and \u0026lt;= operators request older versions. ~\u0026gt; Allows only the right-most version component to increment. Examples:\n- ~\u0026gt; 1.0.4: Allows Terraform to install 1.0.5 and 1.0.10 but not 1.1.0.\n- ~\u0026gt; 1.1: Allows Terraform to install 1.2 and 1.10 but not 2.0. It is a Best Practice to use an exact version when using modules from the Terraform registry since they can change significantly between versions and cause issues. \u0026raquo; Sources \u0026laquo; #Kalyan\u0026rsquo;s GitHub Repositories:\nhttps://github.com/stacksimplify/hashicorp-certified-terraform-associate/tree/main/02-Terraform-Basics https://github.com/stacksimplify/terraform-on-aws-ec2/tree/main/02-Terraform-Basics Additional resources:\nAdditional Reference\nResource: AWS Instance\nResource: AWS Instance Argument Reference\nResource: AWS Instance Attribute Reference\nResource: Meta-Arguments\naws_instance resource documentation: https://registry.terraform.io/providers/-/aws/latest/docs/resources/instance\nVersion Constraints\nVersion Constraints Best Practices\nhttps://developer.hashicorp.com/terraform/cli/commands/state/list\nhttps://developer.hashicorp.com/terraform/cli/state\nhttps://developer.hashicorp.com/terraform/cli/commands/graph\n\u0026raquo; References \u0026laquo; # Terraform registry \u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Kalyan Reddy Daida\u0026rsquo;s Terraform on AWS with SRE \u0026 IaC DevOps course on Udemy.\nHis content was a game-changer in helping me understand Terraform.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğŸ—ƒï¸ GitHub\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"2 October 2025","permalink":"https://robk.uk/posts/training/terraform/terraform-on-aws-with-sre--iac-devops/02-terraform-basics/","section":"ğŸ”°Posts","summary":"Terraform Basics. Commands, Syntax, Arguments, Attributes, Meta-attributes.","title":"Terraform: Basics"},{"content":"Terraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\nExternal Resources Â» Terraform Official Page Terraform Registry Terraform Certifications ","date":null,"permalink":"https://robk.uk/posts/training/terraform/","section":"ğŸ”°Posts","summary":"","title":"Terraform Trainings and Certifications"},{"content":"","date":null,"permalink":"https://robk.uk/categories/linux/","section":"Categories","summary":"","title":"Linux"},{"content":" Linux Perf Analysis # 1. uptime Load averages 2. dmesg -T | tail Kernel errors 3. vmstat 1 Overall stats by time (Virtual Memory) 4. mpstat -P ALL 1 CPU balance 5. pidstat 1 Process usage 6. iostat -xz 1 / iotop Disk I/O 7. free -m Memory usage 8. sar -n DEV 1 Network I/O 9. sar -n TCP,ETCP 1 TCP stats 10. top / htop Check overview 11. lsof List Open Files 12. tcpdump Network Packet Analyzer 13. netstat Network Statistics 14. iptraf Real-time IP LAN Monitoring 15. psacct / acct Monitor User Activity 16. nethogs / iftop Monitor per-process network bandwidth 17. nmon Monitor Linux performance 18. s-tui Terminal-based CPU Stress and Monitoring tool 19. atop Advanced system \u0026amp; process monitoring 20. btop Modern Resource Monitoring \u0026raquo; Sources \u0026laquo; # https://www.tecmint.com/command-line-tools-to-monitor-linux-performance/ https://www.brendangregg.com/linuxperf.html ","date":"29 September 2025","permalink":"https://robk.uk/posts/linux/linux-performance-troubleshooting/","section":"ğŸ”°Posts","summary":"Linux Performance Troubleshooting.","title":"Linux Performance Troubleshooting"},{"content":"","date":null,"permalink":"https://robk.uk/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting"},{"content":"","date":null,"permalink":"https://robk.uk/categories/cheatsheets/","section":"Categories","summary":"","title":"Cheatsheets"},{"content":" Source: https://cheatsheets.zip/cron Crontab Generators # https://crontab.guru/ https://crontab-generator.org/ Format #Min Hour Day Mon Weekday * * * * * command to be executed â”¬ â”¬ â”¬ â”¬ â”¬ â”‚ â”‚ â”‚ â”‚ â””â”€ Day of Week (0=Sun .. 6=Sat) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€ Month (1..12) â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Day of Month (1..31) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hour (0..23) â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minute (0..59) Field Range Special characters Minute 0 - 59 , - * / Hour 0 - 23 , - * / Day of Month 1 - 31 , - * ? / L W Month 1 - 12 , - * / Day of Week 0 - 6 , - * ? / L # Examples # */15 * * * * Every 15 mins 0 * * * * Every hour 0 */2 * * * Every 2 hours 15 2 * * * At 2:15AM (of every day) 15 2 * * ? At 2:15AM (of every day) 10 9 * * 5 At 9:10AM (of every Friday) 0 0 * * 0 At 12:00 AM, only on Sunday 15 2 * * 1L At 2:15am on the last monday of every month 15 0 * * 4#2 At 00:15am on the second thursday of every month 0 0 1 * * At 12:00 AM, on day 1 of the month (1st of month / monthly) 0 0 1 1 * At 12:00 AM, on day 1 of the month, only in January (1st of month) @reboot Run once, at system startup @yearly Run once every year, 0 0 1 1 * @annually (same as @yearly) @monthly Run once every month, 0 0 1 * * @weekly Run once every week, 0 0 * * 0 @daily Run once each day, 0 0 * * * @midnight (same as @daily) @hourly Run once an hour, 0 * * * * Commands # Command crontab -e Edit or create a crontab file if doesnâ€™t already exist. crontab -l Display the crontab file. crontab -r Remove the crontab file. crontab -v Display the last time you edited your crontab file. \u0026raquo; Sources \u0026laquo; # https://cheatsheets.zip/cron\nhttps://crontab.guru/\nhttps://crontab-generator.org/\n","date":"22 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/crontab/","section":"ğŸ”°Posts","summary":"Crontab Cheatsheet.","title":"Crontab Cheatsheet"},{"content":"$ cowthink \u0026ldquo;Hmm, I didn\u0026rsquo;t know that\u0026rdquo;\n_________________________\n( Hmm, I didn\u0026rsquo;t know that )\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\noÂ ^^\noÂ (oo)_____\n(__)\\Â )/\\\n||\u0026mdash;-w |\n||Â ||\n","date":null,"permalink":"https://robk.uk/posts/cheatsheets/linux/","section":"ğŸ”°Posts","summary":"A set of useful Linux cheat sheets\u0026hellip;","title":"Linux Cheatsheets"},{"content":" External Resources Â»\nCheat sheets for Developers\nhttps://cheats.dhr.wtf/\nhttps://cheatsheets.zip/\nOther\nAI Directory | Emoji | Google Search | Markdown | Win Terminal\nBlogging Cheatsheets Blogging-related cheatsheets: Hugo Post Template, Markdown\nDevOps Cheatsheets DevOps cheatsheets: Git, TF Variables, VSCode / Better Comments, and more\u0026hellip; Linux Cheatsheets Linux cheatsheets\u0026hellip; ","date":null,"permalink":"https://robk.uk/posts/cheatsheets/","section":"ğŸ”°Posts","summary":"A set of useful cheat sheets\u0026hellip;","title":"ğŸ—‚ï¸Cheat Sheets"},{"content":"","date":"21 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/linux_commands/","section":"ğŸ”°Posts","summary":"Linux Commands Cheatsheet.","title":"Linux Commands Cheatsheet ğŸ”—"},{"content":"","date":"21 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/systemd/","section":"ğŸ”°Posts","summary":"Systemd Cheatsheet.","title":"Systemd Cheatsheet ğŸ”—"},{"content":"","date":null,"permalink":"https://robk.uk/tags/ansible/","section":"Tags","summary":"","title":"Ansible"},{"content":" Quick reference to Ansible Vault.\nYou can use Ansible vault to securely manage individual variables, entire files, or even structured data like YAML files.\nThis data can then be safely stored in a version control system or shared with team members without exposing sensitive information.\n1. Create a password file #sudo su - ansible vi ~/.vault_key chmod 600 ~/.vault_key 2. Encrypt file #ansible-vault encrypt --vault-password-file ~/.vault_key file.txt 3. Decrypt file #ansible-vault decrypt --vault-password-file ~/.vault_key file.txt 4. Editing / Viewing encrypted file #ansible-vault edit --vault-password-file ~/.vault_key file.txt ansible-vault view --vault-password-file ~/.vault_key file.txt 5. Using variables in playbook ## Create encrypted file with variables vi roles/k3s_initialize_control_plane/vars/secrets.encrypted # Example k3s_cluster_token: e4e9a8c08d29ab81e777cf916070bfd1 # Encrypt secrets.encrypted file using .vault_key ansible-vault encrypt secrets.encrypted --vault-password-file ~/.vault_key # Include vars in your playbook - hosts: k3ssvr[0] become: true roles: - k3s_initialize_control_plane vars_files: - roles/k3s_initialize_control_plane/vars/secrets.encrypted # Variable {{ k3s_cluster_token }} can now be used inside of the role 6. Example encrypted content # Important!\nFiles are protected with symmetric encryption of the Advanced Encryption Standard (AES256), where a single password or passphrase is used both to encrypt and decrypt the data.\nğŸ“„ File: secrets.encrypted\n$ANSIBLE_VAULT;1.1;AES256 39393566323132353465623038646136633335333265353232623262643361666130313164656561 3466313362343062623632323737313966386662343762360a656463363865666561306138623634 66336264363234613237633539623536313139613466303838646334336638313063666630663034 3265303032663131380a306462616163636563616234613532323266323033333034363932356565 32613432306464373664373430326361363833653834306561336238356434303261653136646636 38653563653530306534383837363762336462626631353830303233313836326361323765326230 30326665303332386230666566363535323639393630343239646635313434396235623938313431 38373939616265656161626664303462396130636262346435396561616530663266313938633536 6465 \u0026raquo; Sources \u0026laquo; # https://docs.ansible.com/ansible/latest/cli/ansible-vault.html ","date":"19 September 2025","permalink":"https://robk.uk/posts/devops/iac/ansible/ansible-vault/","section":"ğŸ”°Posts","summary":"Encrypting / Decrypting strings with Ansible Vault.","title":"Ansible Vault"},{"content":"","date":null,"permalink":"https://robk.uk/tags/encryption/","section":"Tags","summary":"","title":"Encryption"},{"content":"","date":"19 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/find/","section":"ğŸ”°Posts","summary":"Find Cheatsheet.","title":"Find Cheatsheet ğŸ”—"},{"content":"","date":"19 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/netstat/","section":"ğŸ”°Posts","summary":"Netstat Cheatsheet.","title":"Netstat Cheatsheet ğŸ”—"},{"content":"","date":"19 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/regex/","section":"ğŸ”°Posts","summary":"Regex Cheatsheet.","title":"Regex Cheatsheet ğŸ”—"},{"content":"","date":"18 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/awk/","section":"ğŸ”°Posts","summary":"AWK Cheatsheet.","title":"AWK Cheatsheet ğŸ”—"},{"content":"","date":null,"permalink":"https://robk.uk/categories/containers/","section":"Categories","summary":"","title":"Containers"},{"content":"DevOps is the integration and automation of the software development and information technology operations. DevOps encompasses necessary tasks of software development and can lead to shortening development time and improving the development life cycle\u0026hellip;.\n","date":null,"permalink":"https://robk.uk/posts/cheatsheets/devops/","section":"ğŸ”°Posts","summary":"A set of useful DevOps cheat sheets\u0026hellip;","title":"DevOps Cheatsheets"},{"content":"","date":null,"permalink":"https://robk.uk/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":"17 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/docker/","section":"ğŸ”°Posts","summary":"Docker Cheatsheet.","title":"Docker Cheatsheet ğŸ”—"},{"content":"","date":null,"permalink":"https://robk.uk/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"","date":"17 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/kubernetes/","section":"ğŸ”°Posts","summary":"Kubernetes Cheatsheet.","title":"Kubernetes Cheatsheet ğŸ”—"},{"content":"","date":"17 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/sed/","section":"ğŸ”°Posts","summary":"SED Cheatsheet.","title":"SED Cheatsheet ğŸ”—"},{"content":"","date":"15 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/curl/","section":"ğŸ”°Posts","summary":"Curl Cheatsheet.","title":"Curl Cheatsheet ğŸ”—"},{"content":"","date":"12 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/ansible/","section":"ğŸ”°Posts","summary":"Ansible Cheatsheet.","title":"Ansible Cheatsheet ğŸ”—"},{"content":"AWS Training and Certification helps you build and validate your cloud skills so you can get more out of the cloud.\nMore: https://aws.amazon.com/training/\n","date":null,"permalink":"https://robk.uk/posts/training/aws/","section":"ğŸ”°Posts","summary":"","title":"AWS Trainings and Certifications"},{"content":"TheÂ AWS Certified Cloud PractitionerÂ validates foundational, high-level understanding of AWS Cloud, services, and terminology. This is a good starting point on the AWS Certification journey\u0026hellip;\n\u003e\u003e Disclaimer \u003c\u003c This series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":null,"permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/","section":"ğŸ”°Posts","summary":"Learnings from \u003cstrong\u003eStephane Maarek\u003c/strong\u003eâ€™s \u0026ldquo;\u003cem\u003eUltimate AWS Certified Cloud Practitioner\u003c/em\u003e\u0026rdquo; course on \u003cem\u003eUdemy\u003c/em\u003e.","title":"ğŸ…AWS Certified Cloud Practitioner"},{"content":"","date":null,"permalink":"https://robk.uk/categories/devops-tools/","section":"Categories","summary":"","title":"DevOps Tools"},{"content":"","date":null,"permalink":"https://robk.uk/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"","date":"11 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/grep/","section":"ğŸ”°Posts","summary":"Grep Cheatsheet.","title":"Grep Cheatsheet ğŸ”—"},{"content":"","date":"11 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/lsof/","section":"ğŸ”°Posts","summary":"lsof Cheatsheet.","title":"lsof Cheatsheet ğŸ”—"},{"content":" The open source AI code editor #Visual Studio Code is a free and versatile code editor that supports almost every major programming language and integrates with GitHub Copilot, an AI model that suggests code edits and completions.\nğŸ“º YouTube Tutorial # ğŸ’¡One of many VSCode Setup Tutorials\nManual Settings # Enable minimap Add theme extension One Dark Pro Dark Horizon Tokyo Night Night Owl Change cursor blinking (set to \u0026ldquo;expand\u0026rdquo;) Cursor Smooth Caret Animation Enable Word Wrap in Settings (set to \u0026ldquo;on\u0026rdquo;) Ensure Bracket Pair Colorization:Â Enabled My VSCode Extensions # VSCode Extension Purpose powershell This extension provides rich PowerShell language support for Visual Studio Code (VS Code). Now you can write and debug PowerShell scripts using the excellent IDE-like interface that VS Code provides. remote-ssh The Remote - SSH extension lets you use any remote machine with a SSH server as your development environment remote-server The Remote - Tunnels extension lets you connect to a remote machine, like a desktop PC or virtual machine (VM), via a secure tunnel. You can then securely connect to that machine from anywhere, without the requirement of SSH. remote-wsl The WSL extension lets you use VS Code on Windows to build Linux applications that run on the Windows Subsystem for Linux (WSL). You get all the productivity of Windows while developing with Linux-based tools, runtimes, and utilities.\nThe WSL extension lets you use VS Code in WSL just as you would from Windows. remote-extensionpack The Remote Development extension pack allows you to open any folder in a container, on a remote machine, or in the Windows Subsystem for Linux (WSL) and take advantage of VS Code\u0026rsquo;s full feature set. yaml Provides comprehensive YAML Language support to Visual Studio Code, via the yaml-language-server, with built-in Kubernetes syntax support. copilot GitHub Copilot is an AI peer programming tool that helps you write code faster and smarter.\nGitHub Copilot adapts to your unique needs allowing you to select the best model for your project, customize chat responses with custom instructions, and utilize agent mode for AI-powered, seamlessly integrated peer programming sessions. copilot-chat GitHub Copilot is an AI peer programming tool that helps you write code faster and smarter.\nGitHub Copilot adapts to your unique needs allowing you to select the best model for your project, customize chat responses with custom instructions, and utilize agent mode for AI-powered, seamlessly integrated peer programming sessions. peacock Subtly change the color of your Visual Studio Code workspace. Ideal when you have multiple VS Code instances, use VS Live Share, or use VS Code\u0026rsquo;s Remote features, and you want to quickly identify your editor. vscode-docker The Docker Extension Pack makes it easy to build, manage, and deploy containerized applications from Visual Studio Code. prettier-vscode Prettier is an opinionated code formatter. It enforces a consistent style by parsing your code and re-printing it with its own rules that take the maximum line length into account, wrapping code when necessary. gitlens GitLens is a powerful open-source extension for Visual Studio Code built and maintained by GitKraken.\nEnhance your workflows with powerful Git functionality like in-editor blame annotations, hovers, CodeLens, and moreâ€”all fully customizable within VS Code. code-runner Run code snippet or code file for multiple languages. vsliveshare Visual Studio Live Share enables you to collaboratively edit and debug with others in real time, regardless what programming languages you\u0026rsquo;re using or app types you\u0026rsquo;re building. It allows you to instantly (and securely) share your current project, and then as needed, share debugging sessions, terminal instances, localhost web apps, and more! material-icon-theme Material Design Icons for Visual Studio Code pdf Display pdf in VSCode. rainbow-csv - Highlights columns in CSV, TSV, semicolon, and pipe-separated files with distinct colors.\n- Query, transform, and filter data using a built-in SQL-like language (RBQL).\n- and more\u0026hellip; better-comments The Better Comments extension will help you create more human-friendly comments in your code. With this extension, you will be able to categorise your annotations\u0026hellip; HashiCorp.terraform The HashiCorp Terraform Extension for Visual Studio Code (VS Code) with the Terraform Language Server adds editing features for Terraform and Terraform Stacks files such as syntax highlighting, IntelliSense, code navigation, code formatting, module explorer and much more! Your browser does not support the video tag. Better Comments options âœ¨ ## ! # ? # // # * # todo: # note: # info: # hack: # fix: # optimize: # @critical # @high # @low â„¹ï¸ Here are the settings:\nğŸ“„settings.json\n{ \u0026#34;better-comments.tags\u0026#34;: [ { \u0026#34;tag\u0026#34;: \u0026#34;!\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#FF2D00\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;?\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#3498DB\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;//\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#F0F2F5\u0026#34;, \u0026#34;strikethrough\u0026#34;: true, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#98C379\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;todo:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#F9E79F\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;note:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#A9DFBF\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;info:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#AED6F1\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;hack:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#FFC300\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;fix:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#F65353\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;optimize:\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#000000\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;#98C379\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;@critical\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#FF2D00\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;@high\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#FF8C00\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;@low\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#98C379\u0026#34;, \u0026#34;strikethrough\u0026#34;: false, \u0026#34;backgroundColor\u0026#34;: \u0026#34;transparent\u0026#34; } ], } https://github.com/rtdevx/dotfiles/tree/main/vscode\nInstalling listed VSCode extensions using PowerShell #ğŸ’¡Pick the ones you want / need or are interested in. You can install them with a single PowerShell command on a Windows machine.\nstart-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vscode.powershell --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vscode-remote.remote-ssh --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vscode.remote-server --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vscode-remote.remote-wsl --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vscode-remote.vscode-remote-extensionpack --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension redhat.vscode-yaml --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension github.copilot --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension github.copilot-chat --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension johnpapa.vscode-peacock --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-azuretools.vscode-docker --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension esbenp.prettier-vscode --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension eamodio.gitlens --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension formulahendry.code-runner --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension ms-vsliveshare.vsliveshare --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension pkief.material-icon-theme --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension tomoki1207.pdf --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension mechatroner.rainbow-csv --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension aaron-bond.better-comments --force\u0026#34; -PassThru -Wait start-process code -windowstyle Hidden -ArgumentList \u0026#34;--install-extension hnw.vscode-auto-open-markdown-preview --force\u0026#34; -PassThru -Wait Screenshots # \u0026raquo; Sources \u0026laquo; # VSCode main site: https://code.visualstudio.com/ VSCode extensions marketplace: https://marketplace.visualstudio.com/VSCode Â» References Â« #Git VSCode Cheatsheet\n","date":"11 September 2025","permalink":"https://robk.uk/posts/devops/devops-tools/my-vscode-setup/","section":"ğŸ”°Posts","summary":"My Visual Studio Code setup.","title":"My VSCode setup"},{"content":"","date":null,"permalink":"https://robk.uk/tags/vscode/","section":"Tags","summary":"","title":"Vscode"},{"content":" Sources:\nhttps://cheats.dhr.wtf/sheet/vscode/ https://cheatsheets.zip/vscode Example use of Better Comments extension for VSCode may help you creating more human-friendly comments in your code. General # Shortcut Description Ctrl + Shift + P Show Command Palette Ctrl + P Quick Open, Go to File\u0026hellip; Ctrl + Shift + N New window/instance Ctrl + Shift + W Close window/instance Ctrl + , User Settings Ctrl + K + Ctrl + S Keyboard Shortcuts Ctrl + ` Open Integrated Terminal Ctrl + Shift + ` Create New Terminal Search and Replace # Shortcut Description Ctrl + F Find Ctrl + H Replace F3 Find next Shift + F3 Find previous Alt + Enter Select all occurrences of Find match ğŸ”¥ Ctrl + Shift + L Change all occurrences of a word ğŸ”¥ Ctrl + D Add selection to next Find match Ctrl + K + Ctrl + D Move last selection to next Find match Alt + C/R/W Toggle case-sensitive/regex/whole word Rich Languages Editing # Shortcut Description Ctrl + Space Trigger suggestion ğŸ”¥ Ctrl + Shift + Space Trigger parameter hints Shift + Alt + F Format document Ctrl + K + Ctrl + F Format selection F12 Go to Definition Alt + F12 Peek Definition Ctrl + K + F12 Open Definition to the side Ctrl + . Quick Fix Shift + F12 Show References F2 Rename Symbol Ctrl + K + Ctrl + X Trim trailing whitespace Ctrl + K + M Change file language Editor Shortcuts # Shortcut Description Ctrl + Enter Insert Line Below Ctrl + Backspace Delete Word Left Ctrl + Delete Delete Word Right Ctrl + Home Go to Beginning of Line Ctrl + End Go to End of Line Ctrl + PageUp Scroll Page Up Ctrl + PageDown Scroll Page Down Ctrl + Left Move to Beginning of Word Ctrl + Right Move to End of Word Ctrl + Up Move to Beginning of File Ctrl + Down Move to End of File File Shortcuts # Shortcut Description Ctrl + N New File Ctrl + O Open File Ctrl + S Save File Ctrl + P Quick Open Ctrl + W Close File Ctrl + F4 Close File Ctrl + K / F Close All Files Ctrl + K / Ctrl + W Close All Files Ctrl + K / Ctrl + F Format Document Ctrl + K / Ctrl + O Open Folder Ctrl + K / Ctrl + T Toggle Terminal Ctrl + K / Ctrl + X Close Terminal Debug Shortcuts # Shortcut Description F5 Start Debugging F9 Toggle Breakpoint F10 Step Over F11 Step Into Shift + F11 Step Out Extension Shortcuts # Shortcut Description Ctrl + Shift + X Open Extensions Ctrl + Shift + P Open Command Palette Git Shortcuts # Shortcut Description Ctrl + Shift + G Open Source Control Ctrl + Shift + P Open Command Palette \u0026raquo; Sources \u0026laquo; # https://cheats.dhr.wtf/sheet/vscode/ https://cheatsheets.zip/vscode Other resources:\nOfficial Documentation Customize keyboard shortcuts Personalize VS Code ","date":"11 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/vscode/","section":"ğŸ”°Posts","summary":"VSCode Cheatsheet.","title":"VSCode Cheatsheet"},{"content":"","date":null,"permalink":"https://robk.uk/categories/blogging/","section":"Categories","summary":"","title":"Blogging"},{"content":"Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nHugo is open source and free to use. It is distributed under the Apache 2.0 License.\n","date":null,"permalink":"https://robk.uk/posts/cheatsheets/blogging/","section":"ğŸ”°Posts","summary":"Hugo is one of the most popular open-source static site generators\u0026hellip;.","title":"Blogging Cheatsheets"},{"content":" Read My VSCode Setup post. Example use of Better Comments extension may help you creating more human-friendly comments in your code. Basic Commands # Command Description git status Check status git add [file] Add file to staging area git add . Add all files to staging area git add -A Add all files to staging area git add -u Add all modified files to staging area git add -p Add all modified files to staging area interactively git commit -m \u0026quot;message\u0026quot; Commit changes git commit -a Commit all changes git commit -am \u0026quot;message\u0026quot; Commit all changes with message git commit --amend Amend last commit git commit --amend -m \u0026quot;message\u0026quot; Amend last commit with message git commit --amend --no-edit Amend last commit without changing the commit message git commit --amend --reset-author Amend last commit with new author git commit --amend --no-edit --reset-author Amend last commit with new author and without changing the commit message git commit --amend --no-edit --date=\u0026quot;date -R\u0026quot; Amend last commit with new date git commit --amend --no-edit --date=\u0026quot;date -R\u0026quot; --reset-author Amend last commit with new date and new author git commit --amend --no-edit --date=\u0026quot;date -R\u0026quot; --reset-author --allow-empty Amend last commit with new date and new author and allow empty commit Branching \u0026amp; Merging # Command Description git branch List all branches git branch -a List all branches (local and remote) git branch -r List all remote branches git branch -v List all branches with last commit on each branch git branch -vv List all branches with last commit and commit author on each branch git branch -vvv List all branches with last commit, commit author and commit message on each branch git checkout -b [branch] Create a new branch and switch to it git merge [branch] Merge a branch into the active branch git merge [source branch] [target branch] Merge a branch into a target branch git branch -d [branch] Delete a branch git branch -D [branch] Force delete a branch git push origin --delete [branch] Delete a remote branch git branch -m [old branch] [new branch] Rename a branch git branch --set-upstream-to=origin/[branch] [branch] Set a local branchâ€™s upstream branch git branch --unset-upstream [branch] Unset a local branchâ€™s upstream branch git push origin [branch] Push a branch to your remote repository Sharing \u0026amp; Updating Projects # Command Description git push origin [branch] Push a branch to your remote repository git push -u origin [branch] Push changes to remote repository (and remember the branch) git push Push changes to remote repository (remembered branch) git push origin --delete [branch] Delete a remote branch git push origin :[branch] Delete a remote branch git push origin [branch] --force Force push changes to remote repository Inspection \u0026amp; Comparison # Command Description git log View changes git log --summary View changes (detailed) git log --oneline View changes (brief) git log --stat View changes (detailed) git log --patch View changes (detailed with actual changes) git log --graph View changes (graphical) git log --graph --oneline View changes (graphical and brief) git log --graph --oneline --all View changes (graphical, brief and all branches) Undoing Things âœ¨ # Command Description git reset [file] Unstage a file while retaining the changes in working directory git reset --hard Discard all local changes in your working directory git reset --hard HEAD Discard all local changes in your working directory git reset --hard origin/[branch] Discard all local changes in your working directory and get the latest version from the remote repository git reset --hard [commit] Discard all local changes in your working directory and get the specific commit from the remote repository git checkout -- [file] Discard local changes in a specific file git checkout [branch] Switch to a branch and discard local changes git revert [commit] Revert a commit git revert [commit] --no-commit Revert a commit without committing Syncing Forks # Command Description git remote -v List all currently configured remote repositories git remote add upstream [https://url] Specify a new remote upstream repository that will be synced with the fork git fetch upstream Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master git merge upstream/master Merge the changes from upstream/master into your local master branch. This brings your forkâ€™s master branch into sync with the upstream repository, without losing your local changes Rewrite History # Command Description git rebase -i HEAD~[number] Interactive rebase git rebase -i [commit] Interactive rebase git rebase -i [branch] Interactive rebase git rebase -i [SHA1] Interactive rebase git rebase -i [tag] Interactive rebase Stashing # Command Description git stash Stash changes in a dirty working directory away git stash save \u0026quot;message\u0026quot; Stash changes in a dirty working directory away with a message git stash list List all stashed changesets git stash show Show the changes in the last stashed changeset git stash show -p Show the changes in the last stashed changeset (detailed) Tagging # Command Description git tag List all tags git tag -l \u0026quot;v1.8.5*\u0026quot; List all tags matching a pattern git tag [tag] Annotate a tag git tag -a [tag] -m \u0026quot;[message]\u0026quot; Annotate a tag with a message Setting up Git #Git Configuration # Command Description git config Check all configuration options git config --list Check all configuration options with name and email git clone [https://url] Clone source code from a remote repository git config --global user.name \u0026quot;Your name\u0026quot; Configure username git config --global user.email \u0026quot;Your email\u0026quot; Configure email git config --global core.editor vim Configure editor Getting \u0026amp; Creating Projects # Command Description git init Initialize a local Git repository git clone [https://url] Clone source code from a remote repository git clone [https://url] [folder] Clone source code from a remote repository into a specific folder git clone --bare [https://url] Clone source code from a remote repository without a working directory git clone --mirror [https://url] Clone source code from a remote repository without a working directory and without the remote repository \u0026raquo; Sources \u0026laquo; # https://cheats.dhr.wtf/sheet/git/ Â» References Â« #Better Comments Options\n","date":"10 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/git/","section":"ğŸ”°Posts","summary":"Git Cheatsheet.","title":"Git Cheatsheet"},{"content":"","date":null,"permalink":"https://robk.uk/tags/markdown/","section":"Tags","summary":"","title":"Markdown"},{"content":"This article offers a sample of basic Markdown formatting that can be used in Congo, also it shows how some basic HTML elements are decorated.\nHeadings #The following HTML \u0026lt;h1\u0026gt;â€”\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 #H2 #H3 #H4 #H5 #H6 #Paragraph #Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes #The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution # Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution # Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\nâ€” Rob Pike1\nTables #Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables # Italics Bold Code italics bold code Code Blocks #Code block with backticks #\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces #\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode # 1 2 3 4 5 6 7 8 9 10 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types #Ordered List # First item Second item Third item Unordered List # List item Another item And another item Nested list # Fruit Apple Orange Banana Dairy Milk Cheese Other Elements â€” abbr, sub, sup, kbd, mark #GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n\u0026raquo; Sources \u0026laquo; # https://jpanther.github.io/congo/samples/markdown/ The above quote is excerpted from Rob Pike\u0026rsquo;s talk about nothing during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"10 September 2025","permalink":"https://robk.uk/posts/cheatsheets/blogging/markdown/","section":"ğŸ”°Posts","summary":"\u003cp\u003eThis article offers a sample of basic Markdown formatting that can be used in Congo, also it shows how some basic HTML elements are decorated.\u003c/p\u003e","title":"Markdown Cheatsheet"},{"content":"","date":"10 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/terraform/","section":"ğŸ”°Posts","summary":"Terraform Cheatsheet.","title":"Terraform Cheatsheet ğŸ”—"},{"content":"","date":"9 September 2025","permalink":"https://robk.uk/posts/cheatsheets/blogging/ascii/","section":"ğŸ”°Posts","summary":"ASCII Cheatsheet.","title":"ACSII Cheatsheet ğŸ”—"},{"content":"# INFO: String Variable variable \u0026#34;vpcname\u0026#34; { description = \u0026#34;The name of the VPC\u0026#34; type = string default = \u0026#34;myvpc\u0026#34; } # INFO: Number Variable variable \u0026#34;sshport\u0026#34; { description = \u0026#34;The port for SSH access\u0026#34; type = number default = 22 } # INFO: Boolean Variable variable \u0026#34;enabled\u0026#34; { description = \u0026#34;Enable or disable the feature\u0026#34; type = bool default = true } # INFO: List Variable variable \u0026#34;mylist\u0026#34; { description = \u0026#34;A list of availability zones\u0026#34; type = list(string) default = [\u0026#34;Value1\u0026#34;, \u0026#34;Value2\u0026#34;] } # INFO: Map Variable (Key-value pairs) variable \u0026#34;mymap\u0026#34; { description = \u0026#34;A map of instance types\u0026#34; type = map(string) default = { Key1 = \u0026#34;Value1\u0026#34; Key2 = \u0026#34;Value2\u0026#34; } } # NOTE: Using variables in a resource # INFO: String Variable resource \u0026#34;aws_vpc\u0026#34; \u0026#34;mytfvpc\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = var.vpcname } } # INFO: Number Variable resource \u0026#34;aws_vpc\u0026#34; \u0026#34;mytfvpc\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = var.mylist[0] } } # INFO: Map Variable resource \u0026#34;aws_vpc\u0026#34; \u0026#34;mytfvpc\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = var.mymap[\u0026#34;Key1\u0026#34;] } } # NOTE: Input variables variable \u0026#34;inputname\u0026#34; { description = \u0026#34;Set the name of the VPC\u0026#34; type = string default = \u0026#34;\u0026#34; } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;mytfvpc\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = var.inputname } } # NOTE: Output output \u0026#34;vpcid\u0026#34; { description = \u0026#34;Output the VPC ID\u0026#34; value = aws_vpc.mytfvpc.id } # NOTE: Tuples and Objects variable \u0026#34;mytuple\u0026#34; { description = \u0026#34;A tuple with mixed types\u0026#34; type = tuple([string, number, bool]) default = [\u0026#34;example\u0026#34;, 42, true] } variable \u0026#34;myobject\u0026#34; { description = \u0026#34;An object with specific attributes\u0026#34; type = object({ name = string age = number active = bool }) default = { name = \u0026#34;example\u0026#34; age = 30 active = true } } ","date":"9 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/terraform-variables/","section":"ğŸ”°Posts","summary":"Terraform Variables Cheatsheet.","title":"Terraform Variables Cheatsheet"},{"content":"","date":"9 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/vim/","section":"ğŸ”°Posts","summary":"VIM Cheatsheet.","title":"VIM Cheatsheet ğŸ”—"},{"content":"","date":"8 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/git-conventional-commits/","section":"ğŸ”°Posts","summary":"Conventional Git Commits Cheatsheet.","title":"Conventional Git Commits Cheatsheet ğŸ”—"},{"content":"","date":"8 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/network-ports/","section":"ğŸ”°Posts","summary":"Network Ports Cheatsheet.","title":"Network Ports Cheatsheet ğŸ”—"},{"content":" Source: https://httpstatuses.io/\nhttpstatuses.io is an easy to reference database of HTTP Status Codes with their definitions and helpful code references all in one place.\nYou can visit an individual status code via httpstatuses.io/code for more details.\n1Ã—Ã— Informational # 100 Continue 101 Switching Protocols 102 Processing 103 Early Hints 2Ã—Ã— Success # 200 OK 201 Created 202Accepted 203 Non-authoritative Information 204 No Content 205 Reset Content 206 Partial Content 207 Multi-Status 208 Already Reported 226 IM Used 3Ã—Ã— Redirection # 300 Multiple Choices 301 Moved Permanently 302 Found 303 See Other 304 Not Modified 305 Use Proxy 307 Temporary Redirect 308 Permanent Redirect 4Ã—Ã— Client Error # 400 Bad Request 401 Unauthorized 402 Payment Required 403 Forbidden 404 Not Found 405 Method Not Allowed 406 Not Acceptable 407 Proxy Authentication Required 408 Request Timeout 409 Conflict 410 Gone 411 Length Required 412 Precondition Failed 413 Payload Too Large 414 Request-URI Too Long 415 Unsupported Media Type 416 Requested Range Not Satisfiable 417 Expectation Failed 418 I\u0026rsquo;m a teapot 421 Misdirected Request 422 Unprocessable Entity 423 Locked 424 Failed Dependency 425 Too Early 426 Upgrade Required 428 Precondition Required 429 Too Many Requests 431 Request Header Fields Too Large 444 Connection Closed Without Response 451 Unavailable For Legal Reasons 499 Client Closed Request 5Ã—Ã— Server Error # 500 Internal Server Error 501 Not Implemented 502 Bad Gateway 503 Service Unavailable 504 Gateway Timeout 505 HTTP Version Not Supported 506 Variant Also Negotiates 507 Insufficient Storage 508 Loop Detected 510 Not Extended 511 Network Authentication Required 599 Network Connect Timeout Error \u0026raquo; Sources \u0026laquo; # httpstatuses.io ","date":"3 September 2025","permalink":"https://robk.uk/posts/cheatsheets/devops/http-status-codes/","section":"ğŸ”°Posts","summary":"HTTP Status Codes.","title":"HTTP Status Codes"},{"content":"","date":"3 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/openssl/","section":"ğŸ”°Posts","summary":"OpenSSL Cheatsheet.","title":"OpenSSL Cheatsheet ğŸ”—"},{"content":"","date":"2 September 2025","permalink":"https://robk.uk/posts/cheatsheets/linux/ssh/","section":"ğŸ”°Posts","summary":"SSH Cheatsheet.","title":"SSH Cheatsheet ğŸ”—"},{"content":"","date":null,"permalink":"https://robk.uk/categories/networking/","section":"Categories","summary":"","title":"Networking"},{"content":"What is OSI Model? #The ISO OSI (Open Systems Interconnection) Model is a set of rules that explains how different computer systems communicate over the TCP / IP network.\nThe OSI Model was developed by International Organization for Standardization (ISO) and it consists of 7 layers.\nWhat is OSI Model | Real World Examples\nLayers of the OSI Model #Each OSI layer has it\u0026rsquo;s own functions and responsibilities. Communication process entails both directions (encapsulation and decapsulation).\nLayers 5,6 and 7 can be grouped together. They are all responsible to produce the data (Protocol Data Unit - PDU) for the Transport layer.\n7. Application Layer #Applications produce the data for the Transport layer and often provide a user interface to interact with.\nExample protocols used in the Application layer: HTTP(S), SMTP, FTP,DNS, SSH\u0026hellip;\nThe application layer is concerned with the specific type of application.\n6. Presentation Layer #The presentation layer is primarily responsible for syntax of the data for the applications to send and consume (e.g. HTML, CSV, JSON).\nEncryption / Decryption Compression 5. Session Layer #Session Layer is responsible for establishing, managing, terminating sessions between two devices.\nNetwork File System (NFS) or Server Message Block (SMB) are commonly used protocols at the session layer.\nOSI and TCP IP Models - Best Explanation\n4. Transport Layer #The primary role of the Transport Layer is to ensure that data packets arrive in the right order.\nTCP - data must be intact (e.g. file sharing) acknowledgment (ACK) of the successful data transmission and re-transmission of the data in error slower than UDP but more reliable UDP - retaining packets is less critical (e.g. video streaming) faster than TCP but less reliable (no ACK) The data in the transport layer is referred to as segments. In this layer source and destination port is added in the header. Segmentation is essential for multiplexing.\n3. Network Layer #The Network Layer is responsible for the transmission of data (in this layer referred to as packets) from one host to another that is located in different networks.\nNetwork Layer also takes care of routing - finding the shortest path to transmit the packet, from a number of routes available.\nNetwork Layer is utilizing IP protocol in order to route. Network devices (routers and switches) are appending the source and destination IP address to the packet\u0026rsquo;s header.\n2. Data Link Layer #The Data Link Layer refers to technologies used to connect two machines across a network.\nData in this layer is referred to as frame.\nEthernet is an example of a standard at this layer. Switches and Bridges are common Data Link Layer devices. The Data Link Layer is divided into two sublayers:\nLogical Link Control (LLC) Media Access Control (MAC) 1. Physical Layer #The Physical Layer refers to the physical communication medium and technologies to transmit data across that medium.\nThe physical layer contains information in the form of bits. Physical Layer is responsible for transmitting individual bits from one node to the next.\nWhen receiving data, this layer will get the signal received and convert it into 0s and 1s and send them to the Data Link layer, which will put the frame back together. Common physical layer devices are Hub, Repeater, Modem, and Cables.\nMore: https://www.geeksforgeeks.org/computer-networks/open-systems-interconnection-model-osi/\n\u0026raquo; Sources \u0026laquo; #What is OSI Model:\nhttps://www.geeksforgeeks.org/computer-networks/open-systems-interconnection-model-osi/ https://aws.amazon.com/what-is/osi-model/ \u0026raquo; Disclaimer \u0026laquo; # Disclaimer: Content for educational purposes only, no rights reserved. ","date":"1 September 2025","permalink":"https://robk.uk/posts/networking/osi-model/","section":"ğŸ”°Posts","summary":"What is OSI Model? - Layers of OSI Model\u0026hellip;","title":"OSI Model"},{"content":"","date":null,"permalink":"https://robk.uk/tags/clf-c02/","section":"Tags","summary":"","title":"CLF-C02"},{"content":"AWS Certified Cloud Practitioner #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\rThe AWS Certified Cloud Practitioner validates foundational, high-level understanding of AWS Cloud, services, and terminology. This is a good starting point on the AWS Certification journey\u0026hellip;\nOfficial website for AWS Certified Cloud Practitioner: AWS Certified Cloud Practitioner\nExam guide: https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf\nContent domains and weightings # Domain 1: Cloud Concepts (24% of scored content) Domain 2: Security and Compliance (30% of scored content) Domain 3: Cloud Technology and Services (34% of scored content) Domain 4: Billing, Pricing, and Support (12% of scored content) Exam Prep Official Questions Set # https://skillbuilder.aws/ https://skillbuilder.aws/category/exam-prep/cloud-practitioner-foundational Official Practice Question Set for CLF-C02\n*Loads more learning and exam preparation resources on AWS Skill Builder website\nRegistering for the exam #https://www.aws.training/certification\nAWS Certification Paths - Operations # \u0026raquo; Sources \u0026laquo; #Amazon Whitepapers:\nOverview of Amazon Web Services AWS Well-Architected Framework AWS Shared Responsibility Model AWS Pricing Compare AWS Support Plans \u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"25 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/25-preparing-for-the-exam/","section":"ğŸ”°Posts","summary":"Preparing for AWS Practitioner (CLF-C02) exam\u0026hellip;","title":"Preparing for AWS Practitioner exam âœ¨"},{"content":"Well-Architected Framework # Stop guessing capacity (use Auto Scaling) Test systems at production scale Automate! Allow for evolutionary architectures Scalability: vertical \u0026amp; horizontal\nDisposable Resources: servers should be disposable \u0026amp; easily configured\nAutomation: serverless, Infrastructure as a Service, Auto Scaling\u0026hellip;\nLoose coupling:\nMonolith applications that do more and more over time become bigger Break it down into smaller, \u0026ldquo;loosely coupled\u0026rdquo; components A change or failure in one component should not cascade to other components Services, not Servers:\nDon\u0026rsquo;t just use EC2 Use managed services, databases, serverless, etc\u0026hellip; AWS Well-Architected: https://aws.amazon.com/architecture/well-architected/\n1. Operational Excellence #Operational excellence includes the ability to run and monitor systems to deliver business value and continually improve supporting processes and procedures. #Design Principles:\nPerform Operations as Code - IaS Frequent, small, reversible changes - reverse in case of failure Refine operations procedures frequently - ensure team members are familiar with it Anticipate failure Learn from the failures Use managed services - to reduce operational burden Implement observability for actionable insights - performance, reliability, cost, \u0026hellip; 2. Security #Security includes an ability to protect information, systems and assets while delivering business value through risk assessments and mitigation strategies. #Design Principles:\nImplement a strong identity foundation - centralize privilege management and reduce (or eliminate) reliance on long-term credentials - Principle of least privilege - IAM Enable traceability - integrate logs and metrics with systems to automatically respond and take action - Cloud Monitoring Apply security at all layers - edge network, VPC, subnet, load balancer, (every) EC2 instance, operating system, application Automate security best practices Protect data in transit and at rest - encryption, tokenization and access control Keep people away from data - reduce or eliminate the need for direct access or manual processing of data Prepare for security events - run incident response simulations and use tools with automation to increase your speed of detection, investigation and recovery 3. Reliability #Reliability is an ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand and mitigate disruptions such as misconfigurations or transient network issues. #Design Principles:\nTest recovery procedures - use automation to simulate different failures or to recreate scenarios that led to failures before Automatically recover from failure - anticipate and remediate failures before they occur Scale horizontally to increase aggregate system availability - distribute requests across multiple, smaller resources to ensure that they don\u0026rsquo;t share a common point of failure Stop guessing capacity - maintain the optimal level to satisfy demand without over or under provisioning Manage change in automation - use automation to make changes to infrastructure 4. Performance Efficiency #Performance Efficiency includes ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. #Design Principles:\nDemocratize advanced technologies - advance technologies become services and hence you can focus more on product development Go Global in minutes - easy deployment in multiple regions Use serverless architecture - avoid the burden of managing servers Experiment more often - easy to carry out comparative testing Mechanical sympathy - be aware of all AWS services 5. Cost Optimization #Cost Optimization includes ability to run systems to deliver business value at the lowest price point. #Design Principles:\nAdopt a consumption mode - pay only for what you use Measure overall efficiency - use CloudWatch Stop spending money on data center operations - AWS does the infrastructure part and enables customer to focus on organization projects Analyze and attribute expenditure - accurate identification of system usage and costs helps to measure Return on Investment (ROI) - make sure to use tags! Use managed and application level services to reduce cost of ownership - as managed services operate at cloud scale then can offer a lower cost per transaction or service 6. Sustainability #Sustainability focuses on minimizing the environmental impact of running cloud workloads. #Design Principles:\nUnderstand your impact - establish performance indicators, evaluate improvements Establish sustainability goals - set long-term goals for each workload, model Return on Investment (ROI) Maximize utilization = right size each workload to maximize the energy efficiency of the underlying hardware and minimize idle resources Anticipate and adopt new, more efficient hardware and software offerings - and design for flexibility to adopt new technologies over time Use managed services - shared services reduce the amount of infrastructure AWS Well-Architected Tool #Free tool to review your architecture against the 6 pillars of Well-Architected Framework and adopt architectural best practices.\nAWS Customer Carbon Footprint Tool #Tool to track, measure, review and forecast the carbon emissions generated from your AWS usage.\nAWS Cloud Adoption Framework (CAF) #Helps you build and then execute a comprehensive plan for your digital transformation through innovative use of AWS.\nCreated by AWS Professionals by taking advantage of AWS Best Practices and lessons learned from 1000s of customers.\nAWS CAF groups it\u0026rsquo;s capabilities in six perspectives: # Business\nBusiness perspective helps to ensure that your cloud investments accelerate your digital transformation ambitions and business outcomes People\nServes a bridge between technology and business, accelerating the cloud journey to help organizations more rapidly evolve to a culture of continuous growth, learning and where change becomes BAU Governance\nHelps orchestrating cloud initiatives while maximizing organizational benefits and minimizing transformation related risks Platform\nHelps building an enterprise-grade, scalable, hybrid cloud platform and modernize existing workloads Security\nHelps achieving the confidentiality, integrity and availability of the data and cloud workflows Operations\nHelps ensuring that your cloud services are delivered at a level that meets the needs of your business AWS Cloud Adoption Framework (CAF)\nAWS CAF - Transformation Domains # Technology - using the cloud to migrate and modernize legacy infrastructure, applications, data and analytics platforms Process - digitizing, automating and optimizing your business operations Organization - reimagining your operating model AWS CAF - Transformation Phases # Envision - demonstrate how the Cloud will accelerate business outcomes Align - identify capability gaps across the 6 AWS CAF Perspectives which results in an Action Plan Launch - build and deliver pilot initiatives in production and demonstrate incremental business value Scale - expand pilot initiatives to the desired scale while realizing the desired business benefits More: AWS Cloud Adoption Framework (AWS CAF)\nAWS Right Sizing #EC2 has many instance types. Right sizing is the process of matching instance types and sizes to your workload.\nScaling up is easy so always start small\u0026hellip;\nAWS Ecosystem - Free resources # AWS Blogs: https://aws.amazon.com/blogs/aws/ AWS Forums (community): https://forums.aws.amazon.com/index.jspa AWS Whitepapers \u0026amp; Guides: https://aws.amazon.com/whitepapers AWS Solutions Library (formerly Quick Starts): https://aws.amazon.com/solutions/ Vetted Technology Solutions for the AWS Cloud Example: live streaming on AWS https://aws.amazon.com/solutions/implementations/live-streaming-on-aw AWS Professional Services and Partner Network #APN: AWS Partner Network. # APN Technology Partners: providing hardware, connectivity, software APN Consulting Partners: professional services firm to help build on AWS APN Training Partners: learning AWS AWS IQ #Engage and pay AWS Certified 3rd party experts for on-demand project work.\nAWS re:Post #AWS Forums.\nAWS re:Post is not intended to be used for questions that are time-sensitive.\nDiscover AWS Official Knowledge Center Articles | AWS re:Post\n\u0026raquo; Sources \u0026laquo; # AWS Well-Architected: https://aws.amazon.com/architecture/well-architected/\nAWS Cloud Adoption Framework (CAF): https://aws.amazon.com/cloud-adoption-framework\nAWS re:Post forums: https://repost.aws/knowledge-center\nAWS Blogs: https://aws.amazon.com/blogs/aws/\nAWS Forums (community): https://forums.aws.amazon.com/index.jspa\nAWS Whitepapers \u0026amp; Guides: https://aws.amazon.com/whitepapers\nAWS Solutions Library (formerly Quick Starts): https://aws.amazon.com/solutions/\nVetted Technology Solutions for the AWS Cloud Example: live streaming on AWS https://aws.amazon.com/solutions/implementations/live-streaming-on-aw \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"24 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/24-aws-architecting-cosystem/","section":"ğŸ”°Posts","summary":"AWS Architecting \u0026amp; Ecosystem, Well-Architected Framework, Cloud Adoption Framework\u0026hellip;","title":"AWS Architecting \u0026 Ecosystem"},{"content":"Other AWS Services that don\u0026rsquo;t fit in other categories. #Workspaces #Managed Desktop as a Service (DaaS) solution to easily provision Windows or Linux desktops.\nGreat to eliminate management of on-premise VDI infrastructure Fast and quickly scalable to thousands of users Secured data - integrates with KMS Pay-as-you-go service with monthly or hourly rates AppStream # Desktop Application Streaming Service Deliver to any computer without provisioning infrastructure The application is delivered within a web browser IoT Core #IoT Core allows easily connecting IoT devices to the AWS Cloud.\nAppSync #Store and sync data across mobile and web apps in real-time using GraphQL (mobile technology from Facebook).\nAmplify #A set of tools and services that helps you to develop and deploy scalable full stack web mobile applications.\nAuthentication, Storage, API (REST, GraphQL), CI/CD, Analytics, Monitoring, \u0026hellip; AWS Infrastructure Composer #Visually design and build serverless applications quickly on AWS.\nDeploy AWS infrastructure code without needing to be an expert in AWS.\nGenerates Infrastructure as Code (IaC) using CloudFormation\nCLI \u0026gt; Infrastructure Composer Device Farm Overview #Fully-managed service that can test your web and mobile app against desktop browsers, real mobile devices and tablets.\nAWS Backup #Fully-managed service to centrally manage and automate backups across AWS services.\nOn-demand and scheduled backups. Cross-region and cross-account (AWS Organizations) backups Disaster Recovery Strategies # Backup and Restore - cheapest method Pilot Light - core functions are there (e.g. database) but it\u0026rsquo;s not scaled up Warm Standby - full version of the app but at minimum size (databases, webs, api, \u0026hellip;) Multi-Site / Hot-Site - full version, full size active-active DR AWS Elastic Disaster Recovery (DRS) #Quickly and easily recover physical, virtual and cloud-based servers into AWS.\nContinuous block-level replication of servers to the cloud AWS DataSync #Move large amount of data from on-premises to AWS.\nCan synchronize to: Amazon S3 (any storage classes - including Glacier) Amazon EFS Amazon FSx for Windows Replication tasks can be scheduled hourly, daily, weekly The replication tasks are incremental after the first full load Cloud Migration Strategies - the 7Rs # More info about Cloud Migration Strategy: https://aws.amazon.com/blogs/enterprise-strategy/new-possibilities-seven-strategies-to-accelerate-your-application-migration-to-aws/\nApplication Discovery Service \u0026amp; Application Migration Service # Plan migration projects by gathering information about on-premises data centers\nServer utilization data and dependency mapping are important for migrations\nAgentless Discovery\nAgent-based Discovery\nAWS Migration Evaluator #Helps to build a data-driven business case for migration to AWS.\nAWS Migration Hub #Central location to collect servers and applications inventory data for the assessment, planning and tracking of migrations to AWS.\nAWS Fault Injection Service (FIS) #A fully-managed service for running fault injection experiments on AWS workloads. Based on Chaos Enginneering.\nSupports the following services: EC2, ECS, EKS, RDS, \u0026hellip;\nStep Functions #Build serverless visual workflow to orchestrate your Lambda functions.\nGround Station #Fully managed service to control satellite communications, process data and scale your satellite operations.\nProvides a global network of satellite ground stations near AWS regions.\nSend satellite data to S3 or EC2 instance.\nUse Cases:\nWeather forecasting Surface imaging Communications Video broadcasts AWS Pinpoint #Scalable 2-way (outbound / inbound) marketing communication service.\nSupports email, SMS, push, voice and in-app messaging.\nUsed for running marketing campaigns.\n\u0026raquo; Sources \u0026laquo; # More info about Cloud Migration Strategy: https://aws.amazon.com/blogs/enterprise-strategy/new-possibilities-seven-strategies-to-accelerate-your-application-migration-to-aws/ \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"23 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/23-other-services/","section":"ğŸ”°Posts","summary":"Other AWS Services\u0026hellip;","title":"Other Services"},{"content":"AWS STS (Security Token Service) # Enables creating temporary, limited-privileged credentials to access AWS resources Short-term credentials (you configure the expiration period) Use cases: Identify federation: manage user identities in external systems and provide them STS tokens to access AWS resources IAM Roles for cross / same account access IAM Roles for Amazon EC2: provide temporary credentials for EC2 instances to access AWS resources Amazon Cognito #Identity for Web and Mobile application users (potentially millions). #Instead of creating users in IAM, web users can be created using Cognito\nAmazon Cognito for web and mobile applications. It can also integrate with Google and Facebook login.\nDirectory Services #Database of objects: User accounts, Computers, Printers, File Shares, Security Groups\u0026hellip; #AWS Directory Services # AWS Managed Microsoft AD Create our own AD in AWS, manage users locally, supports MFA Establish \u0026ldquo;Trust\u0026rdquo; connections with on-premise AD AD Connector Directory Gateway (proxy) to redirect to on-premise AD, supports MFA Users are managed on the on-premise AD Simple AD AD-compatible managed directory on AWS Cannot be joined with on-premise AD AWS IAM Identity Center #One login (single sign-on) for: # AWS Accounts in AWS Organizations Business cloud application (e.g. Salesforce, Box, Microsoft 365, \u0026hellip;) SAML2.0-enabled applications EC2 Windows Instances Identity providers: # Built-in identity store in IAM Identity Center 3rd party: Active Directory, OneLogin, Okta, \u0026hellip; Summary # IAM Identity and Access Management inside your AWS account For users that you trust and belong to your company Organizations Manage multiple accounts Security Token Service (STS) Temporary, limited-privileges credentials to access AWS resources Cognito Create a database of users for your web and mobile applications Directory Services Integrate Microsoft Active Directory in AWS IAM Identity Center One login for multiple AWS accounts and applications \u0026raquo; References \u0026laquo; # Identity and Access management (IAM) \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"22 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/22-advanced-identity/","section":"ğŸ”°Posts","summary":"Advanced Identity\u0026hellip;","title":"Advanced Identity"},{"content":"AWS Organizations # How AWS Organizations work\nDiagram of basic Organization\nMore about Organizations terminology and structure: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html_\nBest Practices for Organizational Units with AWS Organizations: https://aws.amazon.com/blogs/mt/best-practices-for-organizational-units-with-aws-organizations/\nCreating Organizations and Accounts # *Accessing newly created accounts to manage organizations wasn\u0026rsquo;t clear. This video explains it in great details. Please note it doesn\u0026rsquo;t cover permissions / restrictions for that account to have in the organization.\nAWS Organization - Consolidated Billing #Consolidated billing must first be enabled.\nCombined Usage - combine the usage across all AWS accounts in the AWS Organization to share the volume pricing, reserved instances and saving plans discounts One bill - get one bill for all AWS Accounts in the AWS Organization AWS Control Tower #AWS Control Tower = easy way to set up and govern a secure and compliant multi-account AWS environment based on best practices. # Automate the setup of your environment in a few clicks Automate ongoing policy management using guardrails Detect policy violations and remediate them Monitor compliance through an interactive dashboard AWS Control Tower runs on top of AWS Organizations. # It automatically set up AWS Organizations to organize accounts and implement SCPs (Service Control Policies) AWS Resource Access Manager (RAM) # Share AWS resources that you own with other AWS accounts Share with any account or within your organization Avoid resource duplication Supported resources include: Aurora VPC Subnets Transit Gateway Route53 EC2 Dedicated Hosts License Manager Configurations AWS Resource Access Manager (RAM)\nAWS Service Catalog #AWS Service Catalog is a self-service portal that allows users to launch a set of authorized products predefined by admins.\nUsers that are new to the organization could create stacks that are not compliant / not in line with the rest of the organization. #Includes: Virtual Machines, Databases, Storage options, etc.\nMore about Service Catalog: https://docs.aws.amazon.com/servicecatalog/\nService offering with standards how the company is building their services.\nPricing models in AWS #4 AWS pricing models # Pay as you go: pay for what you use, remain agile, responsive, meet scale demands Save when you reserve: minimize risks, predictably manage budgets, comply with long-term requirements Reservations are available for EC2 Reserved Instances, DynamoDB Reserved Capacity, ElastiCache Reserved Nodes, RDS Reserved Instance, Redshift Reserved Nodes Pay less by using more: volume-based discounts Pay less as AWS grows Free Services \u0026amp; Free Plan in AWS # With a new AWS account, you get up to $200 in credits You choose between Free Plan or Paid Plan Free Plan expires in 6 months or when credits are consumed Paid Plan charged after you consume your credits Both plans have access to Always Free services (monthly free usage limits) Lambda - 1,000,000 requests / month and 400,000 GB-seconds compute / month DynamoDB - 25 GB of storage and 200M requests / month More about free services in AWS: https://aws.amazon.com/free/\nCompute pricing #EC2 # On-demand instances Minimum of 60 seconds Pay per second (Linux / Windows) or per hour (Other) Reserved instances Up to 75% discount compared to On-demand on hourly rate 1 or 3 years commitment All upfront, partial upfront, no upfront Spot instances Up to 90% discount compared to On-demand on hourly rate Bid for unused capacity Dedicated host On-demand Reservation for 1 or 3 years commitment Saving plans as an alternative to save on sustained usage Lambda \u0026amp; ECS # Lambda Pay per call Pay per duration ECS EC2 Launch Type Model: No additional fees, you pay for AWS resources stored and created in the application Fargate Pay for vCPU and memory resources allocated to the applications running in your containers Storage Pricing #S3 # Storage class (S3 Standard, S3 Infrequent Access, S3 One-Zone IA, S3 Intelligent Tiering, S3 Glacier and S3 Glacier Deep Archive) Number and size of objects: Price can be tiered (based on volume) Number and type of requests Data transfer OUT of S3 region S3 Transfer Acceleration Lifecycle transitions Similar service: EFS (pay per use, has infrequent access and lifecycle rules).\nEBS # Volume type (based on performance) Storage volume in GB per month (provisioned!) IOPS General Purpose SSD: included Provisioned IOPS SSM: provisioned amount of IOPS Magnetic: number of requests Snapshots Added data cost per GB per month Data transfer Outbound data transfer are tiered for volume discounts Inbound is free Database Pricing #RDS # Per hour billing Database characteristics: Engine Size Memory class Purchase type: On-demand Reserved instances (1 or 3 years) with optional up-front Backup Storage: there is no additional charge for backup storage up to 100% of your total database storage for a region Additional storage (per GB per month) Number of input and output requests per month Deployment type (storage and I/O are variable) Single AZ Multiple AZ Data transfer Outbound data transfer are tiered for volume discounts Inbound is free Content Delivery #CloudFront # Pricing is different across different geographic regions Aggregated for each edge location, then applied to the bill Data Transfer Out (volume discount) Number of HTTP(s) requests Networking costs in AWS per GB - Simplified # Use Private IP instead of Public IP for good savings and better network performance Use same AZ for maximum savings (at the cost of High Availability) Savings Plan # Commit a certain $ amount per hours for 1 or 3 years Easiest way to setup long-term commitment on AWS EC2 Savings plan Up to 72% discount compared to On-demand Commit to usage of individual instance families (e.g. C5 or M5) Regardless of AZ, size, OS or tenancy All upfront, partial upfront, no upfront Compute Savings plan Up to 66% discount compared to On-demand Regardless of Family, Region, size, OS, tenancy, compute options Compute Options: EC2, Fargate, Lambda Machine learning Savings plan: SageMaker\u0026hellip; Savings plans can be set up from AWS Cost Explorer console. #Compute Optimizer #Reduce costs and improve performance by recommending optimal AWS resources for your workloads.\nHelps you choose optimal configurations and right-size your workloads (over / under provisioned).\nUses Machine Learning to analyze your resources configurations and their utilization (CloudWatch metrics).\nSupported resources:\nEC2 Instances EC2 Auto Scaling Groups EBS Volumes Lambda functions Compute Optimized can lower costs by up to 25%. #Recommendations can be exported to S3.\nBilling and Costing Tools #Estimating costs in the cloud # Pricing Calculator Available at: https://calculator.aws Estimate the cost of your solution architecture Tracking costs in the cloud # Billing Dashboard (AWSConsole \u0026gt; Billing) Cost Forecast Month to date Cost Allocation Tags (AWSConsole \u0026gt; Resource Groups \u0026amp; Tag Editor) Allows tracking your AWS costs at a detailed level AWS generated tags (automatically applied to resources that are created, start with prefix aws:) User generated tags (defined by user, start with prefix user:) Tags can be used for organizing resources: EC2: instances, images, load balancers, security groups\u0026hellip; RDS, VPC resources, Route53, IAM users\u0026hellip; Most granular AWS cost report: # Cost and Usage Reports (AWSConsole \u0026gt; Billing)\nThe most comprehensive set on AWS cost and usage data available, including additional metadate about AWS services, pricing and reservations (e.g. EC2 Reserved Instances) Lists AWS usage for each service category used by an account and it\u0026rsquo;s IAM users in hourly or daily line items as well as any tags associated / created for cost allocation purposes Can be integrated with Athena, Redshift or QuickSight Cost Explorer (AWSConsole \u0026gt; Billing \u0026gt; Cost Explorer)\nVisualize, understand and manage your AWS costs and usage over time Create custom reports that analyze cost and usage data Analyze your data at higher level: total costs and usage across all accounts Monthly, hourly, resource-level granularity Choose optimal Savings Plan (to lover the prices) Forecast usage up to 12 months based on previous usage Monthly cost by AWS service Monitoring against cost plans: # Billing Alarms Billing data metric is stored in CloudWatch us-east-1 Billing data are for overall worldwide AWS costs It\u0026rsquo;s for actual cost, not for projected costs Intended a simple alarm (not as powerful as Budgets) Budgets (AWSConsole \u0026gt; Billing \u0026gt; Budgets and Planning) Create budget and send alarms when cost (or forecast) exceeds the budget 4 types of budgets: Usage, Cost, Reservation, Savings Plans Up to 5 SNS notifications per budget AWS Cost Anomaly Detection #AWS Cost Anomaly Detection is continuously monitoring your cost and usage using Machine Learning to detect unusual activities.\nIt is learning your unique, historic spending patterns to detect on-time cost spike OR continuous cost increases.\nIt will send the anomaly detection report with root-cause analysis.\nAWSConsole \u0026gt; Billing \u0026gt; Cost and Usage Analysis \u0026gt; Cost Anomaly Detection\nAWS Service Quotas #Notify when you\u0026rsquo;re close to a service quota (e.g. Lambda concurrent connections).\nCreate CloudWatch Alarms on the Service Quotas console.\nAWSConsole \u0026gt; Service Quotas\nAWS Trusted Advisor #Built in, no need to install anything. High level AWS account assessment.\nAnalyzes AWS accounts and provides recommendation in 6 categories:\nCost optimization Performance Security Fault tolerance Service limits Operational Excellence Business or Enterprise support plan for full set of checks.\nSupport Plans for AWS #Basic # Customer Service \u0026amp; Communities - 24/7 access to customer service, documentation, whitepapers, support forums AWS Trusted Advisor - Access to the 7 core Trusted Advisor checks and guidance to provision your resources following best practices to increase performance and improve security AWS Personal Health Dashboard - A personalized view of the health of AWS services and alerts when your resources are impacted Developer # All Basic Business hours email access to Cloud Support Associates Unlimited cases / unlimited contacts Response times:\nGeneral guidance: \u0026lt; 24 business hours System impaired: \u0026lt;12 business hours Business # All Developer Intended for production workloads Trusted Advisor - full set of checks + API access 24/7 phone, email and chat access to Cloud Support Engineers Response times:\nGeneral guidance: \u0026lt; 24 business hours System impaired: \u0026lt;12 business hours Production system impaired: \u0026lt; 4 hours Production system down: \u0026lt; 1 hour Enterprise (On-Ramp) # All business Access to a pool of Technical Account Managers (TAM) Concierge Support Team (for billing and account best practices) Infrastructure Event Management, Well-Architected \u0026amp; Operations Reviews Response times:\nProduction system impaired: \u0026lt; 4 hours Production system down: \u0026lt; 1 hour Business-critical system down: \u0026lt; 30 minutes Enterprise # All Enterprise (On-Ramp) Access to a designated Technical Account Manager (TAM) Access to AWS Incident Detection and Response (for an additional fee) Response times:\nBusiness-critical system down: \u0026lt; 15 minutes Account Best Practices # Operate multiple accounts using Organizations Use SCP (Service Control Policies) to restrict the account privileges Easily setup multiple accounts with best-practices with AWS Control Tower Use Tags \u0026amp; Cost Allocation Tags for easy management and billing IAM guidelines: MFA, least-privilege, password policy, password rotation Config to record all resources configurations and compliance over time CloudFormation to deploy stacks across accounts and regions Trusted Advisor to get insights, Support Plan adapted to your needs Send Service Logs and Access Logs to S3 or CloudWatch Logs CloudTrail to record API calls made within your account Use AWS Service Catalog to define pre-defined stacks that are used by your organization Cost Optimization best practices # Define and enforce cost allocation tags Define effective account and organization structure Define and use metrics - give access to the team to access billing and costing tools in order to enable accountability and ownership Cloud Center of Excellence (CCoE) - team who stays up to date with AWS best practices, new releases, products and services, etc., in order to ensure that you are using AWS in the most efficient and effective ways Summary # Compute Optimizer: recommends resources configurations to reduce cost Pricing Calculator: cost of services on AWS (estimate the cost of your solution) Billing Dashboard: high-level overview (cost, forecast, month to date) Cost Allocation Tags: tag resources to create detailed reports Cost and Usage Reports: most comprehensive billing dataset Cost Explorer: View current usage (detailed) and forecast usage Billing Alarms: in us-east-1 - track overall and per-service billing Budgets: more advanced - track usage, costs and get alerts Savings Plans: easy way to save based on long-term usage of AWS Cost Anomaly Detection: detect unusual spends using Machine Learning Service Quotas: notify you when you\u0026rsquo;re close to service quota threshold \u0026raquo; Sources \u0026laquo; # AWS Organizations: https://docs.aws.amazon.com/organizations\nBest Practices for Organizational Units with AWS Organizations: https://aws.amazon.com/blogs/mt/best-practices-for-organizational-units-with-aws-organizations/\nFree Services in AWS: https://aws.amazon.com/free/\nAWS Pricing Calculator: https://calculator.aws\nFull YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\nWhat is AWS billing? What is AWS cost management? Selecting the best pricing model Getting started with AWS Support AWS Support plans AWS Whitepapers and Guides \u0026raquo; References \u0026laquo; # Identity and Access management (IAM) AWS Cost optimization services: # AWS Cost Explorer (More: Tracking costs in the cloud) AWS budgets Cost and Usage Reports AWS Trusted Advisor Reserved Instance Reporting Auto Scaling Spot Instance Amazon Simple Storage Service Amazon S3 Glacier Lambda \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"21 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/21-account-management-and-billing/","section":"ğŸ”°Posts","summary":"Account Management and Billing\u0026hellip;","title":"Account Management and Billing"},{"content":"Amazon Rekognition #Finding objects, people, text, scenes in images and videos using Machine Learning. #Facial analysis and facial search to perform user verification, people count, etc.\nUse cases: Labeling Content Moderation Text Detection Face Detection and Analysis (gender, age, range, emotions, \u0026hellip;) Face Search and Verification Celebrity Recognition Pathing (e.g. sports game analysis) More about Rekognition: https://aws.amazon.com/rekognition Transcribe #Automatically converts speech to text. Uses deep learning process called ASR (Automatic Speech Recognition) to convert speech to text. #It has a capability of removing Personally Identifiable information (PII) using Redaction.\nIt also supports Automatic Language Identification for multi-lingual audio.\nUse cases: transcribe customer service calls automate closed captioning and subtitling generate metadata for media assets to create a fully searchable archive Polly #Opposite of Transcribe. Turns text into speech using deep learning. #Translate #Amazon Translate allows localizing the content - websites and applications - for international users. It has a capability of translating large volumes of text efficiently.\nLex \u0026amp; Connect # Amazon Lex: same technology that powers Alexa Automatic Speech Recognition (ASR) to convert speech to text Natural Language Understanding to recognize the intent of the text Helps building chatbots or call center bots Amazon Connect Receive calls, create contact flows, cloud-based virtual contact center Can integrate with other CRM systems or AWS No upfront payment, 80% cheaper than traditional contact center solutions Comprehend #Uses Machine Learning to find insights and relationships in the text for Natural Language Processing - NLP. # Language of the text Extracting key phrases, places, people, brands or events Understand how positive or negative text is Analyzes text using tokenization and parts of speech Automatically organizes a collection of text files by topic Use cases:\nAnalyze customer interactions (emails) to find what leads to a positive or negative experience Create and group articles by topics SageMaker #Fully managed service for developers / data scientists to build Machine Learning (ML) models. Model requires training.\nKendra #Fully managed document search service powered by Machine Learning.\nCan extract answers from within a document (text, pdf, HTML, Power Point, MS Word, FAQs, etc.). Can learn from user interactions or feedback to promote preferred results (Incremental Learning) Has an ability to manually fine-tune search results (importance of data, freshness, custom, etc.) Personalize #Fully managed Machine Learning service to build apps with real-time personalized recommendations.\nSame tech used by amazon.com\nUse Cases: retail stores, media, entertainment\u0026hellip;\nTextract #Automatically extracts text, handwriting and data from scanned documents using AI and ML.\nCan read and process any type of document (PDF, images, etc.)\nSummary # Rekognition: face detection, labeling, celebrity recognition Transcribe: audio to text (e.g. subtitles) Polly: text to audio Translate: translations Lex: build conversational bots / chatbots Connect: cloud contact center Comprehend: natural language processing SageMaker: machine learning for every developer and data scientist Kendra: ML-powered document search engine Personalize: real-time personalized recommendation Textract: detect text and data in documents (handwriting / scanned data) \u0026raquo; Sources \u0026laquo; # Amazon Rekognition: https://aws.amazon.com/rekognition \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"20 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/20-machine-learning/","section":"ğŸ”°Posts","summary":"Machine Learning in Amazon AWS\u0026hellip;","title":"Machine Learning"},{"content":"Shared Responsibility Model #AWS responsibility - Security of the Cloud # Protecting infrastructure (hardware, software, facilities, networking) that runs the AWS services Managed services, like S3, DynamoDB, RDS, etc. Customer responsibility - Security in the cloud # For EC2 instance, customer is responsible to the management of the guest OS (including the security patches and updates), firewall and network configuration, IAM Encrypting application data Shared controls # Patch Management, Configuration Management, Awareness \u0026amp; Training Example - RDS # AWS responsibility Manage underlying EC2 instance, disable SSH access Automated DB patching Automated OS patching Audit the underlying instance and disks and guarantee it functions Customer responsibility Check if the ports / IP / security group inbound rules in DB\u0026rsquo;s SG In-database user creation and permissions Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections Database encryption setting Example - S3 # AWS responsibility Guarantee you get unlimited storage Guarantee you get encryption Ensure data separation between customers Ensure AWS employees can\u0026rsquo;t access your data Customer responsibility Bucket configuration Bucket policy / public setting IAM user and roles Enabling encryption AWS Shared Responsibility Model\nMore about Shared Responsibility Model: https://aws.amazon.com/compliance/shared-responsibility-model/\nDDoS Protection on AWS # AWS Shield Standard - protects against DDoS attack for website and applications - for all customers at no additional cost AWS Shield Advanced - 24/7 premium DDoS protection and support AWS WAF - filter specific requests based on predefined rules CloudFront and Route53 Availability protection using global edge network Combined with AWS Shield provides attach mitigation at the edge Be ready to scale - use AWS Auto Scaling.\nSample Reference Architecture for DDOS Protection in AWS\nDDoS components on above picture: # Route53 - Latency / Geolocation routing policies CloudFront - to ensure data is cached at the edge Shield - see below section WAF - optionally Load Balancer and Auto Scaling More about DDOS protection: https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-best-practices-ddos-resiliency.html\nAWS Shield # AWS Shield Standard Free service that is activated for every AWS customer Provides protection from attacks such as SYN / UDP Floods, Reflection attacks and other layer 3 / layer 4 attacks (read more about ISO OSI Model) AWS Shield Advanced Optional DDoS mitigation service ($3000 per month per organization) Protecting against more sophisticated attacks on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator and Route53 AWS WAF # Protecting web applications from common web exploits (Layer 7) Layer 7 is HTTP (vs Layer 4 is TCP) Can be deployed on Application Load Balancer, API Gateway, CloudFront Web Access Control List # Rules can include filters for IP addresses, HTTP headers, HTTP body, URi strings Protecting from common attacks - SQL injection, Cross-Site Scripting (XSS) Size constraints (to ensure request size) , geo match (block countries) Rate-based rules - to count occurrences of events, limit users to x requests per second, etc. - for DDoS protection AWS Network Firewall # Protects entire Amazon VPC (as oppose to Security Groups \u0026amp; Network ACL) From Layer 3 to Layer 7 protection Any direction, you can inspect VPC to VPC traffic Outbound to the internet Inbound from the internet To / From Direct Connect \u0026amp; Site-to-Site VPN AWS Network Firewall is protecting entire VPC from Layer 3 to Layer 7\nAWS Network Firewall offers much better protection than NACL that only operates at the subnet level. AWS Network Firewall operates at VPC level. #More about Network Firewall: https://docs.aws.amazon.com/network-firewall/\nAWS Firewall Manager #AWS Firewall Manager manages security rules in all accounts of an AWS Organization. # Security policy: common set of security rules VPC Security Groups for EC2, Application Load Balancer, etc\u0026hellip; WAF rules AWS Shield Advanced AWS Network Firewall Rules are applied to new resources as they are created (good for compliance) across ALL EXISTING AND FUTURE accounts in all Organization. #Penetration Testing on AWS Cloud #Allowed activities: #AWS Customers are allowed to carry out security assessment or penetration tests against their AWS infrastructure without prior approval for 8 services: # Amazon EC2 instances, NAT Gateways and Elastic Load Balancers Amazon RDS Amazon CloudFront Amazon Aurora Amazon API Gateways AWS Lambda and Lambda Edge functions Amazon Lightsail resources Amazon Elastic Beanstalk environments Prohibited activities # DNS zone walking via Amazon Route53 and Hosted Zones Denial of Service (DoS), Distributed Denial of Service (DDoS), Simulated DoS, Simulated DDoS Port flooding Protocol flooding Request flooding (login request flooding, API request flooding) For any simulated events, contact aws-security-simulated-event@amazon.com\nRead More about Penetration Testing: https://aws.amazon.com/security/penetration-testing/\nData at rest vs Data in Transit # At rest: data stored or archived on a device On a hard disk, in RDS, in S3 Glacier, etc. In transit (in motion): data being transferred Data in both states (at rest, in transit) should be encrypted as a best practice. #AWS KMS (Key Management Service) #AWS is managing encryption keys for the customers using KMS. # Encryption Opt-in:\nEBS Volumes: encrypt volumes S3 buckets: server-side encryption of objects (SSE-S3 enabled by default, SSE-KMS opt-in) Redshift Database RDS database EFS drives: encryption of data Encryption Automatically enabled:\nCloudTrail Logs S3 Glacier Storage Gateway Cloud HSM # KMS = AWS manages the software for encryption Cloud HSM = AWS is provisioning encryption hardware (HSM = Hardware Security Module) Customer is managing their own encryption keys instead of AWS HSM devices are tamper resistant and FIPS compliant Types of KMS Keys # Customer Managed Key: Created, managed and used by the customer Possibility of rotation policy (new key generated every year, old key preserved) Possibility to bring-your-own-key AWS Managed Key: Created and managed by AWS and used by the customer Used by AWS services (S3, EBS, Redshift, etc.) AWS Owned Key: Collection of CMDs that an AWS service owns and manages to use in multiple accounts CloudHSM Keys: Keys generated from your own (dedicated) CloudHSM hardware device Cryptographic operations are performed within the CloudHSM cluster KMS \u0026gt; AWS managed keys AWS Certificate Manager (ACM) #ACM - allows easy provisioning and deploying SSL / TLS Certificates used to provide encryption for HTTPS enabled websites.\nSupports both, public and private TLS certificates Free of charge for public TLS certificates Automatic TLS certificate renewal Integrations with AWS Services, such as: Elastic Load Balancers (ELB) CloudFront Distributions APIs on API Gateway AWS Secrets Manager # Capability to force rotation of secrets every X days Automate generations of secrets (uses Lambda) Integration with Amazon RDS (MySQL, PostgreSQL, Aurora) Secrets are encrypted using KMS Use case: RDS integrations.\nSecrets Manager \u0026gt; Store a new secret Artifact Overview #Artifact is a portal that provides customers with on-demand access to AWS compliance documentation and AWS agreements.\nArtifact Reports - allows downloading AWS security and compliance documents from third-party auditors, like AWS ISO certifications, Payment Card Industry (PCI) and System and Organization Control (SOC) reports Artifact Agreements - allows reviewing, accepting and tracking the status of AWS agreements, such as: Business Associate Addendum (BAA) Health Insurance Portability and Accountability Act (HIPAA) for an individual account in the organization Can be used to support internal audit or compliance. #Artifact \u0026gt; View reports GuardDuty #GuardDuty is an Intelligent Threat Discovery to protect AWS account.\nUses Machine Learning algorithms, anomaly detection.\nEnabled with 1-click, no need to install any software.\nInput data includes: # CloudTrail Event Logs - unusual API calls, unauthorized deployments CloudTrail Management Events - create VPC subnet, create trail, \u0026hellip; CloudTrail S3 Data Events - get object, list object, delete object, \u0026hellip; VPC Flow Logs - unusual internal traffic, unusual IP addresses DNS Logs - compromised EC2 instances sending encoded data within DNS queries Optional Features EventBridge Cloud Monitoring can be set up to be notified in case of any findings. Rules can target Lambda or SNS.\nGuardDuty has a dedicated finding for Crypto Currency (mining?) attacks.\nAmazon GuardDuty input data example\nAmazon Inspector #Automated Security Assessment. # For EC2 instances Using AWS System Manager (SSM) agent Analyze against unintended network accessibility Analyze the running OS against known vulnerabilities For Container Images pushed to ECR Assessment of Container Images as they are being pushed For Lambda Functions Identifies software vulnerabilities in function code and package dependencies Assessment of functions as they are deployed Reporting and integration with AWS Security Hub. #Amazon Inspector evaluates vulnerabilities (against CVE database) only running EC2 instances, Container Images and Lambda Functions.\nA risk score is associated with all vulnerabilities for prioritization.\nAWS Config #AWS Config helps with auditing and recording compliance of your AWS resources. It is recording configurations and changes over time.\nIt can store the configuration data into S3 (then analyzed by Athena).\nWhat AWS Config can record (examples): # Unrestricted SSH access to the Security Groups (i.e. port open for everyone) Public access to S3 buckets ELB configuration changes over time It is using SNS (check: Cloud Integrations) for sending notifications.\nAWS Config is a per-region service but can be aggregated across regions and accounts.\nAWS Config \u0026gt; 1-click setup AWS Macie #Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover your sensitive data in AWS.\nMacie helps identifying and alerting sensitive data, such as e.g. PII (Personally Identifiable Information).\nAWS Security Hub #AWS Security Hub is a central security tool to manage security across several AWS accounts and automate security checks.\nIntegrated dashboards showing current security and compliance status to quickly take actions.\nSecurity Hub automatically aggregates alerts in predefined formats from AWS services (or AWS partner tools):\nConfig GuardDuty Inspector Macie IAM Access Analyzer AWS Systems Manager AWS Firewall Manager AWS Health AWS Partner Network Solutions AWS Config service is a dependency and must first be enabled to use Security Hub.\nAWS Security Hub\nAmazon Detective #GuardDuty, Macie and Security Hub are used to identify potential security issues of findings.\nOften security findings require deeper analysis to isolate the root cause and take action - it can be a complex process.\nAmazon Detective analyzes, investigates and identifies the root cause of security issues or suspicious activities (using Machine Learning).\nIt is automatically collecting and processing events from VPC Flow Logs, CloudTrail, and GuardDuty and creating an unified view. It can produce visualizations with details and context to help getting to the root cause.\nAWS Abuse #Report suspected AWS resources used to abusive or illegal purposes.\nAbusive and prohibited behaviors: SPAM Port scannint DoS or DDoS Intrusion attempts Hosting illegal or copyrighted content Distributing malware Contact the AWS Abuse team at abuse@amazonaws.com\nRoot user privileges #Root user = AWS Account Owner\nActions that can be performed only by the root user: # Change account settings (account name, email address, root user password, root user access keys) View certain tax invoices Close AWS account Restore IAM user permissions Change or cancel your AWS Support plan Register as a seller in the Reserved Instance Marketplace Configure an Amazon S3 bucket to enable MFA Edit or delete an Amazon S3 bucket policy that includes an invalid VPC ID or VPC endpoint ID Sign up for GovCloud IAM Access Analyzer #Find out which resources are shared externally with IAM Access Analyzer. # S3 Buckets IAM Roles KMS Keys Lambda Functions and Layers SQS queues Secrets Manager Secrets Define Zone of Trust (AWS Account or AWS Organization).\nAccess outside zone of trusts = findings.\nSummary # Shared Responsibility Model on AWS Shield: Automatic DDoS Protection + 24/7 support for advanced WAF: Web Application Firewall to filter incoming web requests based on rules KMS: Encryption keys managed by AWS CloudHSM: Hardware encryption (AWS Customer managing own keys) AWS Certificate manager: Provision, manage and deploy TLS Certificates Artifact: Get access to compliance reports such as PCI, ISO, etc. GuardDuty: Find malicious behavior within VPC, DNS and CloudTrail Logs Inspector: Find software vulnerabilities in EC2, ECR images and Lambda functions Network Firewall: Protect VPC against network attacks Config: Track config changes and compliance against rules Macie: Find sensitive data (e.g. PII Personally Identifiable Information data) in Amazon S3 buckets CloudTrail: Track API calls made by users within account AWS Security Hub: gather security findings from multiple AWS accounts Amazon Detective: Find the root cause of security issues or suspicious activities AWS Abuse: Report AWS resources used for abusive or illegal purposes Root user privileges: Change account settings Close AWS account Change or cancel AWS Support plan Register as a seller in the Reserved Instance Marketplace IAM Access Analyzer: Identify which resources are shared externally Firewall Manager: Manage security rules across an Organization (WAF, Shield\u0026hellip;) \u0026raquo; Sources \u0026laquo; # Shared Responsibility Model: https://aws.amazon.com/compliance/shared-responsibility-model/\nDDOS: https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-best-practices-ddos-resiliency.html\nNetwork Firewall: https://docs.aws.amazon.com/network-firewall/\nPenetration Testing: https://aws.amazon.com/security/penetration-testing/\nOSI Model: ISO OSI Model\n\u0026raquo; References \u0026laquo; # EC2 S3 Storage Elastic Load Balancing Cloud Monitoring Databases Other Compute Services AWS Global Infrastructure \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"19 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/19-security-and-compliance/","section":"ğŸ”°Posts","summary":"Security and Compliance, Shared Responsibility Model\u0026hellip;","title":"Security and Compliance"},{"content":"VPC - Virtual Private Cloud #Amazon Virtual Private Cloud (VPC) enables you to provision logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you defined.\nVPC, Subnets, Internet Gateways, NAT Gateways Security Groups, Network ACL (NACL), VPC Flow Logs VPC Peering, VPC Endpoints Site to Site VPN and Direct Connect Transit Gateway IP Addresses in AWS # IPv4 (4.3 Billion Addresses)\nPublic IPv4 - can be used on the Internet EC2 instance gets a new public IP address every time it\u0026rsquo;s started Private IPv4 - can be used on private networks (LAN) such as internal AWS networking Private IPv4 is fixed for EC2 instances even after stopping / starting Elastic IP - allows to attach a fixed public IPv4 address to an EC2 instance\nNote: Every public IPv4 address on AWS will be charged $0.005 per hour (including Elastic IP)\nIPv6 - Number of addresses: 3.4 x 10^38\nEvery IP address is public in AWS (no private range) (??) Free VPC and Subnets Primer #VPC - Virtual Private Cloud - private network to deploy resources (regional resource)\nSubnets allow to partition the network inside of VPC (Availability Zone Resource) A public subnet is a subnet that is accessible from the internet A private subnet is a subnet that is not accessible from the internet To define access to the internet and between subnets, we use Route Tables Internet Gateway \u0026amp; NAT Gateways # Internet Gateways help our VPC instances connecting with the internet\nPublic Subnets will have a route to the internet gateway NAT Gateways (AWS Managed) \u0026amp; NAT Instances (self managed) allow instances in Private Subnets to access the internet while remaining private\nInternet Gateway \u0026amp; NAT Gateways.\nVPC \u0026gt; Subnets EC2 Instance created in a Public Subnet will have Public IPv4 address associated. All Public Subnets have Internet Gateway (IGW) associated with them.\nVPC \u0026gt; Virtual Private Cloud \u0026gt; Subnets \u0026gt; subnet-ID \u0026gt; Route table All traffic coming to 172.31.0.0/16 will be considered local. Traffic to / from anywhere will go through an associated Internet Gateway.\nCreating Private Subnet ## For Private Subnet we don\u0026#39;t associate it with any Internet Gateway. Traffic OUT goes via the NAT Gateway. VPC \u0026gt; Virtual Private Cloud \u0026gt; Subnets \u0026gt; Create subnet Security Groups \u0026amp; Network ACL #Security Groups # A firewall that controls traffic to and from an EC2 Instance Can only have allow rules Rules include IP addresses and other security groups NACL (Network ACL) # A firewall that controls traffic to and from a SUBNET Can have allow and deny rules Are attached at the Subnet level Rules only include IP addresses More: AWS Network Firewall that protects entire VPC.\nVPC \u0026gt; Security \u0026gt; Security Groups VPC \u0026gt; Security \u0026gt; Network ACLs Security Group Network ACL Operates at the instance level Operates at the subnet level Supports ALLOW rules ONLY Supports ALLOW and DENY rules Is stateful (return traffic is automatically allowed, regardless of any rules) Is stateless (return traffic must be explicitly allowed by rules) All rules are evaluated before deciding whether to allow the traffic Rules are processed in a number order when deciding to allow the traffic Applies to an instance only if someone specified the Security Group when launching the instance (or associates it with the instance later on) Automatically applies to all instances in the subnets to which Network ACL is associated with More info: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html Default ACLs associated with the Default Subnets allow all traffic IN and OUT.\nVPC Flow Logs #Helps to monitor and troubleshoot connectivity issues # Capture information about IP traffic going into your interfaces\nVPC Flow Logs Subnet Flow Logs Elastic Network Interface Flow Logs Helps to monitor and troubleshoot connectivity issues\nSubnets to internet Subnets to subnets Internet to subnets Captures network information from AWS managed interfaces too:\nElastic Load Balancers ElastiCache RDS Aurora etc\u0026hellip; VPC Flow Logs must first be created for each VPC #VPC \u0026gt; Virtual Private Cloud \u0026gt; Your VPCs \u0026gt; Flow Logs \u0026gt; Create flow log VPC Peering #Connect to VPC privately using AWS Network and make them behave as if they were in the same network.\nMust NOT have overlapping CIDR (IP address range) VPC Peering connection is NOT transitive (must be established for each VPC that needs to communicate with one another) VPC \u0026gt; Virtual Private Cloud \u0026gt; Your VPCs \u0026gt; Peering Connections \u0026gt; Create peering connection VPC Endpoints #Endpoints allow connecting to AWS Services using a private network instead of the public www network.\nThis gives enhanced security and lower latency to access AWS services.\nVPC Endpoint Gateway - for Amazon S3 and DynamoDB only VPC Endpoint Interface - most services (including S3 and DynamoDB) Private Link #Most secure and scalable way to expose a service to 1000s of VPCs. Using VPC Peering (see above) is not practical because of the management overhead.\nDoes not require VPC peering, internet gateway, NAT, route tables\u0026hellip; Requires a Network Load Balancer (NLB) - Service VPC and Elastic Network Interface (ENI) - Customer VPC AWS Private Link\nSite to Site VPN \u0026amp; Direct Connect #Site to Site VPN # Connect to an on-premises VPN to AWS The connection is automatically encrypted Goes over the public internet (cheaper and slower than Direct Connect) Site-to-Site VPN: - On-Premises - must use Customer Gateway (CGW) - AWS: must use a Virtual Private Gateway (VGW)\nDirect Connect (DX) # Establish a physical connection between on-premises and AWS The connection is private, secure and fast Goes over a private network (more expensive but faster than Site to Site VPN) Takes at least a month to establish AWS Client VPN #Connect from your computer using OpenVPN to your private network in AWS and on-premises.\nAllows connecting to your EC2 instances over a private IP (just as you were in the private VPC network).\nGoes over the public Internet.\nTransit Gateway # Transit Gateway is used for having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection. # Works with Direct Connect Gateway, VPN connections.\nSummary # VPC: Virtual Private Cloud Subnets: Tied to and AZ, network partition of the VPC Internet Gateway: at the VPC level, provide Internet Access NAT Gateway / Instances: give internet access to private subnets Security Groups: Stateful, operate at the EC2 instance level for ENI NACL: Stateless, subnet rules for inbound and outbound VPC Peering: Connect two VPC with non overlapping IP ranges, non-transitive (must be established for each VPC that needs to communicate with one another) Elastic IP: Fixed public IPv4 VPC Endpoints: Provide private access to AWS Services within VPC Private Link: Privately connect to a service in a 3rd party VPC VPC Flow Logs: Network traffic logs Site to Site VPN: VPN over public internet between on-premises DC and AWS Client VPN: OpenVPN connection from your computer into your VPC Direct Connect: Direct private connection to AWS Transit Gateway: Connect thousands of VPC and on-premises networks together \u0026raquo; Sources \u0026laquo; # Amazon VPC Documentation: https://docs.aws.amazon.com/vpc/ Internetwork traffic privacy in VPC: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html \u0026raquo; References \u0026laquo; # Elastic Load Balancing S3 \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"18 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/18-vpc/","section":"ğŸ”°Posts","summary":"VPC - Virtual Private Cloud\u0026hellip;","title":"VPC"},{"content":"CloudWatch Metrics #CloudWatch provides metrics for every service in AWS.\nIt is possible to create CloudWatch Dashboards that consist of different metrics.\nCloudWatch console\nCloudWatch Console: https://console.aws.amazon.com/cloudwatch\nImportant Metrics # EC2 instances: CPU Utilization, Status Checks, Network (not RAM!) Default metrics every 5 minutes Option for Detailed Monitoring ($!): metrics every 1 minute EBS Volumes: Disk Read / Writes S3 Buckets: Bucket size, Number of Objects, All requests Billing: Total Estimated Charge (only in us-east-1) Service Limits: how much of service API\u0026rsquo;s was used (Lambda?) Custom metrics: push your own metrics # See All metrics CloudWatch \u0026gt; Metrics \u0026gt; All metrics # Create a custom alarm CloudWatch \u0026gt; Alarms \u0026gt; All alarms CloudWatch Alarms # Alarms are used to trigger notifications for any metric Alarms actions Auto Scaling - increase or decrease EC2 instances \u0026ldquo;desired\u0026rdquo; count EC2 Actions - stop, terminate, reboot or recover and EC2 instance SNS notifications - send a notification into an SNS topic Various options (sampling, %, max, min, etc\u0026hellip;) Can choose the period on which to evaluate an alarm Example: create a billing alarm on the CloudWatch Billing metric Alarm States: OK, INSUFFICIENT_DATA, ALARM Billing alarms are only available in us-east-1\nCloudWatch Logs #CloudWatch Logs can collect logs from:\nElastic Beanstalk - collection of logs from application ECS - collection of logs from containers AWS Lambda - collection of logs from functions CloudTrail based on filter CloudWatch log agents - on EC2 machines or on-premises servers Route53 - DNS logs CloudWatch Logs for EC2 / on-premise #CloudWatch agent is required to be installed on EC2 instance to push the logs that are needed.\nCloudWatch agent can also be installed on on-premise servers too.\nIAM permissions must be set for CloudWatch Logs to function correctly.\nAmazon EventBridge # Schedule - cron jobs (scheduled scripts) Event Pattern - Event rules to react to a service doing something Trigger Lambda functions, send SQS / SNS messages Default Event Bus - for AWS Services Partner Event Bus - for external entities (i.e. Zendesk) sending events to the cloud Custom Event Bus - custom EventBridge \u0026gt; Create rule AWS CloudTrail #AWS CloudTrail provides governance, compliance and audit your AWS Account and it is ENABLED BY DEFAULT. #It can get a history of events / API calls made within your AWS account by:\nConsole SDK CLI AWS Services Logs from CloudTrail can be stored in CloudWatch Logs or S3. Trail can be applied to All Regions (default) or a single region.\nIf a resource is deleted in AWS, investigate CloudTrail first! # # Check Events History CloudTrail \u0026gt; Event history AWS X-Ray #Visual analysis of our applications. Common view of entire architecture. # Troubleshooting performance Understanding dependencies in a microservices architecture Pinpoint service issues Review request behavior Find Errors and Exceptions SLA\u0026rsquo;s Identify impacted users Amazon CodeGuru #ML-powered service for automated code reviews and application performance recommendations. #Provides 2 functionalities:\nCodeGuru Reviewer - automated code reviews for static code analysis (development) Identify critical issues, security vulnerabilities and hard to find bugs Common coding best practices, resource leaks, security detection, input validation Uses Machine Learning and automated reasoning Hard-learned lessons across millions of code reviews on 1000s of open-source repositories Supports Java and Python Integrates with GitHub, Bitbucket and AWS CodeCommit CodeGuru Profiler - visibility / recommendations about application performance during runtime (production) Helps understand the runtime behavior of an application Identify if application is consuming excessive CPU, etc\u0026hellip; Features: Identify and remove code inefficiencies Improve application performance (e.g. reduce CPU utilization) Decrease compute costs Provide heap summary (identify which objects using up the memory) Anomaly detection Supports applications running on AWS as well as on-premise Minimal overhead on application AWS Health Dashboard #Shows all regions, all services health. General information, not specific to you. #Shows historical information for each day.\nHas an RSS feed that can be subscribed to.\nAWS Health Dashboard - Your Account #AWS Account Health Dashboard provides alerts and remediation guidance when AWS is experiencing events that may impact you.\nWhile the Service Health Dashboard displays the general status of AWS services, Account Health Dashboard gives you a personalized view into the performance and availability of the AWS services underlying your AWS resources.\nIt can aggregate data from an entire AWS Organization.\nSummary # CloudWatch Metrics - monitor the performance of AWS services and billing metrics Alarms - automate notification, perform EC2 action, notify to SNS based on metric Logs - collects log files from EC2 instances, servers, Lambda functions Events (EventBridge) - react to events in AWS or trigger a rule on a schedule CloudTrail - audit API calls made within your AWS account CloudTrail Insights - automated analysis of your CloudTrail Events X-Ray - trace requests made through your distributed applications (analyze flow) AWS Health Dashboard - general status of ALL AWS services across all regions AWS Account Health Dashboard - AWS events that impact your infrastructure Amazon CodeGuru - automated code reviews and application performance recommendations \u0026raquo; Sources \u0026laquo; #CloudWatch / CloudWatch Logs / CloudWatch Events (EventBridge):\nhttps://docs.aws.amazon.com/cloudwatch/ https://console.aws.amazon.com/cloudwatch \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"17 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/17-cloud-monitoring/","section":"ğŸ”°Posts","summary":"CloudWatch, EventBridge, CloudTrail, X-Ray, CodeGuru, Health Dashboard\u0026hellip;","title":"Cloud Monitoring"},{"content":"This section is about multiple applications communicating with each other.\nSynchronous communication (application to application) Can be problematic if there are sudden spikes of traffic Asynchronous / Event-based communication (application to queue to application) It is called decoupling of applications SQS: queue model SNS: pub / sub model Kinesis: real-time data streaming model Those services can scale independently from our application SQS #SQS = Simple Queue Service. # What is Amazon SQS\nSQS - Standard Queue # Oldest AWS offering (over 10 years old) Fully managed, serverless service used to decouple applications Sales from 1 message per second to 10,000s per second Default messages retention: 4 days, maximum 14 days No limit to how many messages can be in the queue Messages are deleted after they\u0026rsquo;re read by consumers (applications) Low latency Consumers share the work to read messages and scale horizontally SQS - FIFO Queue #FIFO = First in First Out (ordering of messages in the queue) # Messages are processed in order by the consumer.\nAmazon Kinesis #Kinesis = real-time big data streaming. #Managed service to collect, process and analyze real-time streaming data at any scale.\nAmazon SNS #SNS = Simple Notification Service. #SNS is sending one message to multiple receivers.\nThe \u0026ldquo;event publishers\u0026rdquo; only sending message to one SNS topic As many \u0026ldquo;event publishers\u0026rdquo; as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages Up to 12,500,000 subscriptions per topic, 100,000 topics limit Amazon MQ #SQS and SNS are \u0026ldquo;cloud-native\u0026rdquo; services. Traditional applications running from on-premises may use open protocols, such as:\nMQTT AMQP STOMP Openwire WSS When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, Amazon MQ can be used instead.\nAmazon MQ is a managed message broker service for:\nRabbitMQ\nActive MQ\nAmazon MQ doesn\u0026rsquo;t scale as much as SQS / SNS\nAmazon MQ runs on servers, can run in Multi-AZ with failover\nAmazon MQ has both - queue feature (~SQS) and topic features (~SNS)\nSummary # SQS Queue service in AWS Multiple Producers, messages kept up to 14 days Multiple Consumers share the read and delete messages when done Used to decouple applications in AWS SNS Notification service in AWS Subscribers: Email Lambda SQS HTTP Mobile Others Multiple Subscribers, sending all messages to all of them No message retention Kinesis Real-time data streaming Amazon MQ Managed message broker for Active MQ and Rabbit MQ in the cloud (MQTT, AMQP protocols) \u0026raquo; Sources \u0026laquo; # Amazon SQS: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"16 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/16-cloud-integrations/","section":"ğŸ”°Posts","summary":"Cloud Integrations / decoupling. This section is about multiple applications communicating with each other. SNS, SQS, MQ\u0026hellip;.","title":"Cloud Integrations"},{"content":"A Global Application is an application deployed in multiple geographies. On AWS this could be Regions and / or Edge Locations.\nDecreased Latency Disaster Recovery (DOS / DDoS) Attack protection (distributed global infrastructure is harder to attack) More: https://aws.amazon.com/about-aws/global-infrastructure/\nRoute53 CloudFront (Global CDN) S3 Transfer Acceleration AWS Global Accelerator AWS Global Infrastructure Overview - Regions, Availability Zones, Edge Locations and more\nRoute53 #Route53 is managed DNS.\nHow Route 53 routes traffic for your domain\nRoute53 Routing Policies # Simple Routing Policy - No health checks, just DNS check Weighted Routing Policy - Specify what amount of traffic goes where (i.e. 70% = Server1, 20% = Server2, 10% = Server3. Simple form of Load Balancing) Latency Routing Policy - Based on latency - minimizing the latency between user and the server sending the traffic that is geographically (latency-based) closer to the user Failover Routing Policy - Disaster Recovery (DR) - based on Health Checks Geolocation Routing Policy - Routing based specifically on Geolocation IP-based Routing Policy - Route the traffic based on the IP address originates from More on Routing Policies: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\nAWS Route 53 Course\nRegistering a domain ## Register a Domain Route 53 \u0026gt; Registered Domains \u0026gt; Register Domain \u0026gt; CHOOSEADOMAIN.COM # Hosted zones Route 53 \u0026gt; Hosted zones \u0026gt; select \u0026#34;CHOOSEADOMAIN.COM\u0026#34; \u0026gt; Update the DNS records with the right EC2 instances, select an adequate Routing Policy More about Registering and managing domains: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar.html\nMore about Route 53: https://docs.aws.amazon.com/route53/\nAmazon CloudFront # Content Delivery Network (CDN) Improves read performance, content cached at the edge Improves users experience Many Points of Presence globally (Edge Locations, Edge Caches) DDoS protection (because it\u0026rsquo;s distributed globally) Integrated with Shield and AWS WAF (Web Application Firewall) CloudFront - Origins # S3 Bucket For distributing files and caching them at the edge For uploading files to S3 through CloudFront Secured using Origin Access Control (OAC) VPC Origin For applications hosted in VPC private subnets Application Load Balancer / Network Load Balancer / EC2 Instances Custom Origin (HTTP) S3 website (must first enable the bucket as a static S3 website) Any public HTTP backend How CloudFront delivers content\nCloudFront vs S3 Cross Region Replication #CloudFront # Global Edge Network Files are cached for a TTL (day?) Use case: static content that must be available everywhere S3 Cross Region Replication # Must be setup for each region you want your replication to happen Files are updated in near real-time Read-only Use case: dynamic content that needs to be available at low-latency in few regions only S3 Transfer Acceleration #Increase transfer speed by transferring files to an AWS edge location which will forward the data to the S3 bucket in the target region.\nAWS Global Accelerator #AWS Global Accelerator is used to improve global application availability and performance using the AWS global network.\nLeverage the AWS internal network to optimize the route to your application (60% improvement).\nMore about AWS Global Accelerator:\nhttps://aws.amazon.com/global-accelerator/ https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html https://speedtest.globalaccelerator.aws AWS Global Accelerator vs CloudFront # They both use AWS global network and it\u0026rsquo;s edge locations Both services integrate with AWS Shield for DDoS protection CloudFront - Content Delivery Network Improves performance for cacheable content (images, videos, etc.) Content is served at the edge Global Accelerator No caching, proxying packets at the edge to applications running in one or more AWS regions Improves performance for a wide range of applications running in one or more AWS regions Improves performance for a wide range of applications over TCP or UDP Good for HTTP use cases that require static IP addresses Good for HTTP use cases that require deterministic, fast, regional failover AWS Outposts #AWS Outposts = Hybrid Cloud appliances. #Outposts are \u0026ldquo;server racks\u0026rdquo; that offer the same AWS infrastructure, services, API\u0026rsquo;s \u0026amp; tools to build your own applications on-premises just as in the cloud.\nAWS will setup and manage Outposts racks within your on-premises infrastructure. #Benefits\nLow latency access to on-premises system Local data processing Data residency Easier migration from on-premises to the cloud Fully managed service Some example services that work on Outposts: EC2 EBS S3 EKS ECS RDS EMR Wavelength #Wavelength Zones are infrastructure deployments embedded within the telecommunication providers datacenters at the edge of the 5G networks.\nUltra low latency applications through 5G networks Traffic doesn\u0026rsquo;t leave the Communication Service Provider\u0026rsquo;s (CSP) network High bandwidth and secure connection to the parent AWS Region No additional charges or service agreements Use cases: Smart Cities ML-assisted (Machine Learning) diagnostics Connected Vehicles Interactive Live Video Streams AR / VR Real-time gaming AWS Local Zones #AWS Local Zones allow placing compute, storage, database and other selected AWS services closer to the users to run latency-sensitive applications.\nIt is an \u0026ldquo;Extension of AWS Region\u0026rdquo;.\nExample: # AWS Region: N. Virginia (us-east-1) AWS Local Zones: Boston, Chicago, Dallas, Houston, Miami, \u0026hellip; How AWS Local Zones work\nCompatible with: # EC2 RDS ECS EBS ElastiCache Direct Connect More\u0026hellip; More about AWS Local Zones: https://docs.aws.amazon.com/local-zones/latest/ug/what-is-aws-local-zones.html\nSummary #Route 53 - Global DNS\nGreat to route users to the closest deployment with least latency Great for Disaster Recovery - DR - Strategies CloudFront - Global CDN - Content Delivery Network\nReplicate part of your application to AWS Edge Locations - decreased latency Cache common requests - improved user experience and decreased latency S3 Transfer Acceleration\nAccelerate global uploads \u0026amp; downloads into Amazon S3 AWS Global Accelerator\nImprove global application availability and performance using the AWS global network AWS Outposts\nDeploy Outposts racks in an on-premises datacenter to extend some AWS services and for easier migration AWS Wavelength\nBrings AWS services to the edge of the 5G networks Ultra-low latency applications AWS Local Zones\nBring AWS resources (compute, database, storage, \u0026hellip;) closer to your users Good for latency-sensitive applications \u0026raquo; Sources \u0026laquo; #Global Infrastructure: https://aws.amazon.com/about-aws/global-infrastructure/\nRoute 53 #Route 53: https://docs.aws.amazon.com/route53/ Route 53 Routing Policies: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html Registering and managing domains: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar.html\nCloudFront #CloudFront: https://docs.aws.amazon.com/cloudfront/\nAWS Global Accelerator #https://aws.amazon.com/global-accelerator/ https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html https://speedtest.globalaccelerator.aws\nAWS Local Zones #https://docs.aws.amazon.com/local-zones/latest/ug/what-is-aws-local-zones.html\n\u0026raquo; References \u0026laquo; # S3 Security and Compliance \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"15 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/15-aws-global-infrastructure/","section":"ğŸ”°Posts","summary":"A Global Application is an application deployed in multiple geographies. On AWS this could be Regions and / or Edge Locations\u0026hellip;.","title":"AWS Global Infrastructure"},{"content":"Deploying and Managing Infrastructure at scale #CloudFormation #CloudFormation is a declarative way of outlining an AWS infrastructure.\nExample:\nSecurity Group Two EC2 instances using this Security Group S3 Bucket Load Balancer (ELB) in front Then CloudFormation creates those resources in the right order and with the exact configuration that was specified (declared).\nIntroduction to AWS CloudFormation\nBenefits of CloudFormation # Infrastructure as Code No resources are manually created Changes to the infrastructure are reviewed through code Cost Each resource within the stack is tagged with an identifier so you can easily see how much a stack costs Cost can be estimated by using CloudFormation template Cost savings strategy: in Dev, automation can delete resources at 5pm and recreate at 8am automatically Productivity Ability to destroy and re-create and infrastructure in the cloud on the fly Declarative programming (no need to figure out ordering and orchestration) Don\u0026rsquo;t re-invent the wheel Adopt existing templates on the web Use extensive documentation Supports (almost) all AWS resources \u0026ldquo;Custom resources\u0026rdquo; can be used for resources that are not supported CloudFormation + Infrastructure Composer #Example: Wordpress CloudFormation Stack\nWe can see all the resources We can see the relations between components More: Infrastructure Composer\nYour browser does not support the video tag. More: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/infrastructure-composer-for-cloudformation.html\nAWS Cloud Development Kit (CDK) # Define your cloud infrastructure using a familiar language: JavaScript/TypeScript, Python, Java, .NET For that reason infrastructure and application can be deployed \u0026ldquo;together\u0026rdquo; - they share the runtime The infrastructure code is converted into a CloudFormation template (JSON / YAML) Beanstalk #Elastic Beanstalk is a developer-centric view of deploying an application on AWS.\nIt uses all the components mentioned earlier (EC2, ASG, ELB, RDS, etc\u0026hellip;)\nBeanstalk = Platform as a Service (PaaS) # Managed service Instance configuration / OS is handled by Beanstalk Deployment strategy is configurable but performed by Elastic Beanstalk Capacity provisioning Load Balancing and Auto Scaling Application health-monitoring and responsiveness Just the application code is the responsibility of the developer Three architecture models: Single instance deployment: for DEV environments LB + ASG: for prod or pre-prod web apps ASG only: for non-web apps in production (workers, etc\u0026hellip;) Beanstalk supports many platforms: # Go Java SE Java with Tomcat .NET on Windows Server with IIS Node.js PHP Python Ruby Packer Builder Single-Container Docker Multi-Container Docker Preconfigured Docker Beanstalk Health Agent pushes metrics to CloudWatch, checks for app health and publishes health events.\nAWS CodeDeploy #AWS CodeDeploy is a deployment service that automates application deployments to:\nEC2 instances as well as on-premise instances - it is a Hybrid service serverless Lambda functions Amazon ECS (Elastic Container Services) Servers / Instances must be provisioned and configured ahead of time with the CodeDeploy Agent.\nAWS CodeBuild #Code building service in the cloud.\nCodeBuild compiles source code, run tests, produces packages that are ready to be deployed.\nAWS CodePipeline #CodePipeline orchestrates the different steps to have the code automatically pushed to production.\nCode \u0026gt; Build \u0026gt; Test \u0026gt; Provision \u0026gt; Deploy AWS CodeArtifact #Software packages depends on each other to be built (also called code dependencies). Storing and retrieving those dependencies is called artifact management. Traditionally you need to setup your own artifact management system.\nCodeArtifact works with common dependency management tools such as:\nMaven, Gradle, npm, yarn, twine, pip, NuGet.\nDevelopers and CodeBuild can retrieve dependencies straight from CodeArtifact.\nSystems Manager (SSM) #SSM helps managing EC2 and On-Premises systems at scale.\nAnother Hybrid AWS service Get operational insights about the state of the infrastructure Suite of 10+ products Features: Patching automation for enhanced compliance Run commands across entire fleet of servers Store parameter configuration with the SSM Parameter Store Works with Linux, Windows, MacOS and Raspberry Pi OS (Raspbian) Allows starting SSH session on EC2 and On-Premise servers No SSH access, bastion hosts or SSH keys needed No port 22 needed Send session log data to S3 or CloudWatch Systems Manager Parameter Store # Secure storage for configuration and secrets API Keys, passwords, configurations Serverless, scalable, durable, easy SDK Control access permissions with IAM policies Version tracking and encryption (optional) More: https://docs.aws.amazon.com/systems-manager/\n\u0026raquo; Sources \u0026laquo; # Infrastructure Composer: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/infrastructure-composer-for-cloudformation.html AWS Systems Manager (SSM): https://docs.aws.amazon.com/systems-manager/ \u0026raquo; Highlights \u0026laquo; # Infrastructure Composer Systems Manager (SSM) \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"14 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/14-deployments/","section":"ğŸ”°Posts","summary":"Deploying and Managing Infrastructure at scale in AWS\u0026hellip;","title":"Deployments"},{"content":"Docker #Docker is a software development platform to deploy apps.\nApps are packaged in containers that can be run on any OS.\nApplications function in the same way, no matter where they\u0026rsquo;re run Any machine No compatibility issues Predictable behavior Less work Easier to maintain and deploy Scale containers up and down quickly (seconds) Docker images are stored in Docker Repositories.\nPublic: Docker Hub (https://hubl.docker.com) Private: Amazon ECR (Elastic Container Registry) ECS (Elastic Container Service) #Amazon ECS = Elastic Container Service.\nECS allows launching Docker containers on AWS. It is an orchestrator.\nYou must provision and maintain the infrastructure (EC2 instances) AWS takes care of starting / stopping containers Has integrations with the Application Load Balancer Fargate # Launch Docker containers on AWS Serverless. No need to provision the infrastructure (no EC2 instances to manage) AWS runs the containers based on CPU / RAM needed ECR # Elastic Container Registry Private Docker Registry on AWS Amazon EKS #Amazon EKS = Elastic Kubernetes Service.\nAllows launching and managing Kubernetes clusters on AWS.\nKubernetes is an open-source system for management, deployment and scaling of containerized apps (Docker, Containerd).\nContainers can be hosted on:\nEC2 instances Fargate Lambda # Virtual functions - no servers to manage Limited by time - short executions Run on-demand Scaling is automated Benefits of Lambda # Easy pricing: Pay per request and compute time Integrated with the whole AWS suite of services Event-Driven: functions get invoked by AWS when needed Integrated with many programming languages Node.js (JavaScript) Python Java C# (.NET Core) / PowerShell Ruby Custom Runtime API (community supported, example Rust or Golang) Easy monitoring through AWS CloudWatch Easy to get more resources per functions (up to 10 GB of RAM) Increasing RAM will also improve CPU and Network Lambda Container Image - although ECS / Fargate is preferred for running Docker images.\nLambda pricing is based on calls and duration.\nAmazon API Gateway # Fully managed service for developers to easily create, publish, maintain, monitor and secure APIs Serverless and scalable Supports RESTful APIs and WebSocket APIs Support for security, user authentication, API throttling, API keys, monitoring Creating Serverless API = API Gateway + Lambda. Expose Lambda functions as HTTP API. #AWS Batch # Fully managed batch processing at any scale Efficiently run 100,000s of computing batch jobs on AWS A \u0026ldquo;batch\u0026rdquo; job is a job with start and and end (opposed to continuous) Batch will dynamically launch EC2 instances or Spot instances AWS batch provisions the right amount of compute / memory You submit or schedule batch jobs and AWS Batch does the rest Batch jobs are defined as Docker images and run on ECS Helpful for cost optimizations and focusing less on the infrastructure Batch vs Lambda # Lambda Time limit Limited runtimes Limited temporary disk space Serverless Batch No time limit Any runtime as long as it\u0026rsquo;s packaged as a Docker image (no programming language dependency) Rely on EBS / Instance Store for disk space Relies on EC2 (can be managed by AWS) Lightsail # Virtual servers, storage, databases and networking Low \u0026amp; predictable pricing Simpler alternative to using EC2, RDS, ELB, EBS, Route53 Great for people with little cloud experience Can setup notifications and monitoring of your Lightsail resources Use cases: Simple web applications Websites Dev / Test environment Has High Availability but no auto scaling, limited AWS integrations Summary # Docker: container technology to run applications ECS: run Docker container on EC2 instances Fargate: Run Docker containers without provisioning the infrastructure Serverless offering (no EC2 instances) ECR: Private Docker Images Repository EKS: Allows launching and managing Kubernetes clusters on AWS (hosted on: EC2 instances OR Fargate) Batch: run batch jobs on AWS across managed EC2 instances Lightsail: predictable \u0026amp; low pricing for simple application \u0026amp; DB stacks Lambda: Serverless, Function as a Service, seamless scaling, reactive Lambda Billing: By the time run x by the RAM provisioned By the number of invocations Language support: many programming languages except (arbitrary) Docker Invocation time: up to 15 minutes API Gateway: expose Lambda functions as HTTP API \u0026raquo; Sources \u0026laquo; # ECS Documentation: https://docs.aws.amazon.com/ecs/ \u0026raquo; References \u0026laquo; # EC2 \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"13 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/13-other-compute-services/","section":"ğŸ”°Posts","summary":"Other Compute Services in AWS cloud\u0026hellip;","title":"Other Compute Services"},{"content":"Relational Databases #A relational database is a type of database that organizes data into rows and columns, which collectively form a table where the data points are related to each other.\nData is typically structured across multiple tables, which can be joined together via a primary key or a foreign key. These unique identifiers demonstrate the different relationships which exist between tables, and these relationships are usually illustrated through different types of data models.\nNo-SQL Databases # No-SQL = non-relational databases No-SQL databases are purpose built for specific data models and have flexible schemas for building modern applications Benefits:\nFlexibility (easy to evolve data model) Scalability (designed to scale out by using distributed clusters) High-Performance (optimized for a specific data model) Highly functional (types optimized for the data model) Use cases: Key-value, document, graph, in-memory, search databases\nRDS and Aurora #Amazon RDS #RDS stands for Relational Database Service. It is a managed DB service.\nIt allows creating databases in the cloud that are managed by AWS:\nPostgres MySQL MariaDB Oracle Microsoft SQL Server IBM DB2 Aurora (AWS Proprietary) Advantage of using RDS vs deploying DB on EC2:\nRDS is a managed service Automated Provisioning and OS patching Continuous backups and restore to specific timestamp (Point in Time Restore) Monitoring dashboards Read replicas for improved read performance Multi-AZ setup for DR Maintenance windows for upgrades Scaling capability (both, vertical and horizontal) Storage backed by EBS Not possible to SSH into DB instances (managed service) Example RDS application architecture # Source: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html\nAmazon Aurora # Aurora is proprietary technology from AWS (not open sourced) PostgreSQL and MySQL are both supported Aurora is \u0026ldquo;AWS Optimized\u0026rdquo; and claims to be 5x performance improved over MySQL on RDS and 3x the performance of Postgres on RDS Aurora storage automatically grows and increments of 10GB (up to 128 TB) Aurora costs about 20% more than RDS Amazon Aurora Serverless # Automated (on-demand) database with autoscaling based on actual usage PostgreSQL and MySQL are both supported as Aurora Serverless DB No capacity planning required Least management overhead Pay-per-second, COULD BE more effective Use cases: infrequent, intermittent or unpredictable workloads.\nAurora with no management overhead = Aurora Serverless. #Create RDS database #Aurora and RDS \u0026gt; Create a database RDS Deployment options # Read Replicas Scale the read workload of your DB Can create up to 15 replicas Data is only written to the main DB Multi-AZ Failover in case of AZ outage (High-Availability) Data only read/written to the main DB Can only have 1 AZ as a failover Multi-Region Multi-Region (Read Replicas) Writes only to the main database Local performance for global reads Additional replication cost Use case: DR in another region More: Configuring and managing a Multi-AZ deployment for Amazon RDS\nOther Database Types #Amazon ElastiCache # The same way RDS is to get managed Relational Databases, ElastiCache is to get managed Redis or Memcached.\nCaches are in-memory databases with high performance and low latency\nHelps reducing load from databases with read-intensive workloads\nAWS taking care of OS maintenance, patching, optimizations, setup, configuration, monitoring, failure recovery and backups\nMore: https://docs.aws.amazon.com/elasticache/\nDynamoDB # Fully managed, Highly Available with replication across 3AZ No-SQL database - not a relational DB Scales to massive workloads, distributed, \u0026ldquo;serverless\u0026rdquo; Millions of requests per second, trillions of row, 100s TB of storage Fast and consistent performance Single-digit millisecond latency Integrated with IAM for security, authorization and administration Low cost and auto scaling capabilities Standard \u0026amp; Infrequent Access (IA) Table Class DynamoDB Accelerator (DAX) # Fully Managed in-memory cache for DynamoDB 10x performance improvement when accessing DynamoDB tables DAX is only used for DynamoDB where ElastiCache can be used for other databases.\nDynamoDB Global Tables # Makes DynamoDB table accessible with low latency in multiple-regions Active-Active replication (read/write to any AWS Region) Redshift # Redshift is based on PostgreSQL It\u0026rsquo;s OLAP - Online Analytical Processing (analytics and data warehousing) Load data once every hour, not every second 10x better performance than other data warehouses Scales to PBs of data Columnar storage of data (instead of rows) Massively Parallel Query Execution (MPP) Pay-as-you-go based on the instances provisioned Has a SQL interface for performing queries Redshift Serverless # Auto Scaling Run analytics workload without managing data warehouse infrastructure Pay only for what you use Use cases: Reporting, real-time analytics Amazon EMR # EMR stands for \u0026ldquo;Elastic MapReduce\u0026rdquo; EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amounts of data The clusters can be made of hundreds of EC2 instances EMR takes care of all the provisioning and configuration Auto-scaling and integrated with Spot instances Use cases: data processing, machine learning, web indexing, big data Athena # Serverless query service to perform analytics against S3 objects Uses standard SQL language to query the files Supports CSV, JSON, ORD, Avro, Parquet Pricing: $5 per TB of data scanned Use cases: Business intelligence, analytics, reporting, analyze \u0026amp; query VPC Flow Logs, ELB Logs, CloudTrail logs, etc. Exam tip: analyze data in S3 using serverless SQL = Athena #QuickSight #Allows creating dashboards for services used in AWS. Per-session pricing.\nServerless machine-learning powered business intelligence service to create interactive dashboards Use cases: Business analytics Building visualisations Ad-hoc analysis Get business insights using data Integrated with RDS, Aurora, Athena, Redshift, S3 More: https://docs.aws.amazon.com/quicksight/\nDocumentDB #Aurora version for MongoDB (NoSQL database).\nMongoDB is used to store, query and index JSON data Fully Managed, Highly Available with replication across 3AZ DocumentDB storage automatically grows in increments of 10 GB Neptune # Fully managed graph database A popular graph dataset would be a social network Users have friends Posts have comments Comments have likes from users Users share and like posts Highly Available across 3AZ with up to 15 replicas Build and run applications working with highly connected datasets = optimized for those complex queries Can store up to billions of relations and query the graph with milliseconds latency Use cases: knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking Amazon Timestream # Serverless time series database Automatically scales up and down to adjust capacity Store and analyze trillions of events per day Amazon managed Blockchain # Blockchain makes it possible to build applications where multiple parties can execute transactions without the need for a trusted, central authority Amazon managed Blockchain is a managed service that allows: Join public Blockchain networks Create your own scalable, private network Compatible with: Hyperledger Fabric Ethereum AWS Glue #Managed Extract, Transform and Load (ETL) service.\nUseful to prepare and transform data for analytics Fully serverless service DMS #DMS - Database Migration Service\nQuick and secure migrate databases to AWS\nThe source database remains available during the migration\nHomogeneous migrations: i.e. Oracle to Oracle\nHeterogeneous migrations: i.e. MSSQL to Aurora\nDatabase Summary # Relational Databases - OLTP: RDS \u0026amp; Aurora (SQL) Differences between Multi-AZ, Read Replicas, Multi-Region In-memory Database: ElastiCache Key/Value Database: DynamoDB (serverless) \u0026amp; DAX (cache for DynamoDB) Warehouse - OLAP: Redshift (SQL) Hadoop Cluster: EMR Athena: query data on Amazon S3 (serverless \u0026amp; SQL) QuickSight: dashboards on your data (serverless) DocumentDB: â€œAurora for MongoDBâ€ (JSON â€“ NoSQL database) Amazon QLDB: Financial Transactions Ledger (immutable journal, cryptographically verifiable) Amazon Managed Blockchain: managed Hyperledger Fabric \u0026amp; Ethereum blockchains Glue: Managed ETL (Extract Transform Load) and Data Catalog service Database Migration: DMS Neptune: graph database Timestream: time-series database \u0026raquo; Sources \u0026laquo; # Amazon RDS and Aurora Documentation: https://docs.aws.amazon.com/rds/ ElastiCache: https://docs.aws.amazon.com/elasticache/ QuickSight: https://docs.aws.amazon.com/quicksight/ Configuring and managing a Multi-AZ deployment for Amazon RDS \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"12 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/12-databases/","section":"ğŸ”°Posts","summary":"AWS oï¬€ers a growing number of database options (15+) with diverse data models to support a variety of workloads. These include relational, key-value, document, in-memory, graph, time series, vector, and wide-column\u0026hellip;","title":"Databases"},{"content":"What is Amazon S3 #Amazon S3 is one of the main building blocks of AWS.\nIt is advertised as \u0026ldquo;infinitely scaling\u0026rdquo; storage.\nMany websites use Amazon S3 as a backbone Many AWS services use Amazon S3 as an integration AWS S3 Full Course | From Beginner to Expert | Deploy Real-Time Projects on AWS\nAmazon S3 Use Cases # Backup and Storage Disaster Recovery Archive Hybrid Cloud storage Application hosting Media hosting Data lakes \u0026amp; big data analytics Software delivery Static Website Amazon S3 - Buckets # Amazon S3 allows people to store objects (files) in \u0026ldquo;buckets\u0026rdquo; (folders) Buckets must have a globally unique name (across all regions and all accounts) Buckets are defined at the region level S3 looks like a global service but buckets are created in a region S3 Bucket naming convention # No uppercase No underscore 3-63 characters long Not an IP Must start with lowercase letter or number Must NOT start with the prefix \u0026ldquo;xn\u0026ndash;\u0026rdquo; Must NOT start with the suffix \u0026ldquo;s3alias\u0026rdquo; Amazon S3 - Objects # Objects (files) have a Key\nThe key is the FULL path:\ns3://my-bucket/my_file.txt s3://my-bucket/my_folder/another_folder/my_file.txt The key is composed of prefix + object name\ns3://my-bucket/my_folder/another_folder/my_file.txt There is no concept of \u0026ldquo;directories\u0026rdquo; within S3 buckets (although UI will suggest there is)\nJust keys with very long names that contain slashes (\u0026quot;/\u0026quot;) Object values are the content of the body\nMax Object size is 5TB If uploading more than 5GB, must be \u0026ldquo;multi-part upload\u0026rdquo; Metadata (list of text key / value pairs - system or user metadata) Tags (Unicode key / value pair - up to 10) - used for security / lifecycle Amazon S3 - Security # User-based IAM Policies - which API calls should be allowed for a specific user from IAM Resource-based Bucket policies - bucket-wide rules from the S3 console - allows cross account Object Access Control List (ACL) - finer grain (can be disabled) Bucket Access Control List (ACL) - less common (can be disabled) IAM Principal can access an S3 object if The user IAM permissions ALLOW it OR the resource policy ALLOWS it AND there is no explicit DENY Encryption - encrypt objects in Amazon S3 using encryption keys S3 Bucket Policies # JSON based policies\nResources: buckets and objects Effect: Allow / Deny Actions: Set of API to Allow or Deny Principal: The account or user to apply the policy to Use S3 bucket policy to:\nGrant public access to the bucket Force objects to be encrypted at upload Grant access to another account (Cross Account) Examples # Public access - Bucket Policy User access to S3 - IAM permissions EC2 instance access - IAM Roles Cross-Account access - Bucket Policy Bucket settings for Block Public Access\nThose settings were created to prevent company data leaks If you know your bucket should never be public, leave them Applying Bucket Policy #S3 \u0026gt; General purpose buckets (or other) \u0026gt; your-bucket \u0026gt; Permissions \u0026gt; Ensure Allow public access is enabled Edit Bucket policy (follow Policy examples OR Policy Generator)\nPolicy Generator # \u0026ldquo;arn\u0026rdquo; can be found in the bucket settings itself \u0026ldquo;/*\u0026rdquo; can / should be added to propagate to all objects in the bucket Add Statement and copy the generated policy to Bucket policy\nObject(s) in that bucket should now be available from the internet via URL\nS3 - Static Website Hosting #Bucket must be made public (S3 Bucket policy, see above) in order for the static website to work. If it isn\u0026rsquo;t then 403 Forbidden error appears.\nEnable static website hosting #S3 \u0026gt; General purpose buckets (or other) \u0026gt; your-bucket \u0026gt; Properties \u0026gt; Static website hosting Voila! # Although no HTTPS enabled! - check \u0026ldquo;AWS Certificate Manager (ACM)\u0026rdquo; section in Security and Compliance on how to use ACM for SSL / TLS certificate management.\nAmazon S3 - Versioning # Versioning can be enabled at the bucket level Versioning buckets: Protect against unintended deletes (ability to restore the version) Easy roll back to previous version Notes: Any file that is not versioned prior to enabling versioning will have version \u0026ldquo;null\u0026rdquo; Suspending versioning does not delete the previous versions Enable S3 Versioning #S3 \u0026gt; General purpose buckets (or other) \u0026gt; your-bucket \u0026gt; Properties \u0026gt; Bucket Versioning \u0026gt; Edit \u0026gt; Enable In order to restore the file, toggle \u0026ldquo;Show Versions\u0026rdquo; switch and DELETE unwanted object (destructive, will permanently delete).\nIf \u0026ldquo;show versions\u0026rdquo; toggle is off, object can be safely deleted. It will NOT be permanently deleted, only \u0026ldquo;Delete marker\u0026rdquo; will be applied. Object can be easily restored when \u0026ldquo;Show versions\u0026rdquo; is ON. #S3 - Replication # CRR - Cross Region Replication SRR - Same Region Replication For S3 replication to work, versioning must be enabled on both - source and destination buckets. Buckets can be different AWS Accounts.\nCopying is asynchronous, proper IAM permissions must be applied to S3.\nUse Cases: # CRR - compliance, lower latency access, replication across accounts SRR - log aggregation, live replication between production and test accounts Enabling S3 Replication # Create 2 new buckets and enable versioning in both\nrk-test-replica-london-origin rk-test-replica-irl-dest On the origin bucket create the Replication rules\nEnable replication Select source and destination buckets Create IAM role or select an existing one (Create) Select any other options (encryption, destination storage class, delete marker replication and so on) S3 Storage Classes #Standard # Amazon S3 Standard - General Purpose 99.99% Availability Used for frequently accessed data Low latency and high throughput Sustain 2 concurrent facility failures Use cases: Big Data analytics, mobile \u0026amp; gaming applications, content distribution Infrequent access #For data that is less frequently accessed but requires rapid access when needed. Lower cost than S3 Standard.\nAmazon S3 Standard-Infrequent Access (IA) 99.9% Availability Use cases: Disaster Recovery, backups Amazon S3 One Zone-Infrequent Access For data that is less frequently accessed but requires rapid access when needed High Durability (99.999999999% Availability in a single AZ) 99.5% Availability Use cases: Secondary backup copies of on-prem data, data can be recreated Glacier #Low-cost object storage for archiving / backup.\nPricing includes storage price + retrieval cost.\nAmazon S3 Glacier Instant Retrieval Milliseconds retrieval, great for data accessed once a quarter Minimum storage duration of 90 days Amazon S3 Glacier Flexible Retrieval Expedited (1 to 5 mins), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free Amazon S3 Glacier Deep Archive For long-term storage Standard (12 hours), Bulk (48 hours) Minimum storage duration of 180 days Intelligent tiering #Moves objects automatically between Storage Tiers based on usage for a small monthly monitoring and auto-tiering fee.\nThere is no retrieval charges in S3 Intelligent Tiering.\nFrequent Access tier (automatic): default tier Infrequent Access tier (automatic): objects not accessed for 30 days Archive instant Access tier (automatic): objects not accessed for 90 days Archive Access tier (optional): configurable from 90 to 700+ days Deep Archive Access tier (optional): configurable from 180 to 700+ days Objects can be moved between classes manually or using S3 Lifecycle policies.\nMore:\nS3 Storage classes: https://aws.amazon.com/s3/storage-classes/ S3 Pricing: https://aws.amazon.com/s3/pricing/ S3 Durability and Availability # Durability High durability (99.999999999, 11 9\u0026rsquo;s) of objects across multiple AZ If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years Same for all storage classes Availability Measures how readily available a service is Varies depending on storage class Example: S3 standard has 99.99% availability = not available for 53 minutes a year More:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html https://aws.amazon.com/s3/storage-classes/ S3 Encryption # Server-Side Encryption (Default) Client-Side Encryption (Encrypted by user at the client side) IAM Access Analyzer for S3 # Ensures that only intended people have access to your S3 buckets Example: publicly accessible bucket, bucket shared with other AWS account Evaluates S3 Bucket Policies, S3 ACLs, S3 Access Point Policies Powered by IAM Access Analyzer, (Security and Compliance section) AWS Snowball # Highly-secure, portable devices to collect and process data at the edge and / or migrate data in and out of AWS Helps to migrate up to Petabytes of data It is recommended to use AWS Snowball devices if it would take more than a week to transfer over the network. # More:\nhttps://aws.amazon.com/snowball/ https://aws.amazon.com/snowball/pricing/ AWS Storage Gateway #Amazon S3 File Gateway connects on-premises applications to the cloud to store and access archive repositories, application data, database backups and so on.\nS3 File Gateway is used for on-premises data intensive applications that need file protocol access to objects in S3.\nMore: https://aws.amazon.com/storagegateway/file/s3/\nTypes of Storage Gateway # File Gateway S3 File Gateway presents Server Message Block (SMB) or Network File System (NFS) based access to data in Amazon S3 Volume Gateway iSCSI block storage volumes to your on-premises applications that you can store in Amazon S3 or migrate to EBS Tape Gateway virtual tapes to leading to backup application that can be stored in S3 or S3 Glacier More about Storage Gateway types: https://aws.amazon.com/storagegateway/\n\u0026raquo; Sources \u0026laquo; # S3 Documentation: https://docs.aws.amazon.com/s3/ S3 Availability and Durability: https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html S3 Storage Classes: https://aws.amazon.com/s3/storage-classes/ S3 Pricing: https://aws.amazon.com/s3/pricing/ S3 AWS Snowball: https://aws.amazon.com/snowball/ S3 File Gateway: https://aws.amazon.com/storagegateway/file/s3/ AWS Storage Gateway types: https://aws.amazon.com/storagegateway/ Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\n\u0026raquo; References \u0026laquo; # Storage Security and Compliance \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"11 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/11-s3/","section":"ğŸ”°Posts","summary":"Amazon S3 is one of the main building blocks of AWS. It is advertised as \u0026ldquo;infinitely scaling\u0026rdquo; storage\u0026hellip;","title":"Amazon S3"},{"content":"Auto Scaling Group contains a collection of EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.\nAuto Scaling Group uses Auto Scaling features such as health check replacements and scaling policies.\nAWS EC2 Auto Scaling : Step By Step Tutorial\nThe purpose of Auto Scaling Group # Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and maximum number of instances running Automatically register new instances to the Load Balancer Replace unhealthy instances Cost Saving (only run at optimal capacity) Creating an Auto Scaling Group # Create Launch Template Create Auto Scaling Group Select Availability Zones Select Availability Zone distribution Attach to an existing Load Balancer Turn on Elastic Load Balancing health checks Define the desired capacity Set up Automatic Scaling (optional) Select Instance maintenance policy Additional capacity settings Additional settings Auto Scaling Groups - Strategies # Manual Scaling - update the size of an ASG manually Dynamic Scaling - respond to changing demand Simple / Step Scaling When a CloudWatch alarm is triggered (i.e. CPU \u0026gt; 70%) then add 2 instances When a CloudWatch alarm is triggered (i.e. CPU \u0026lt; 30%) then remove 1 instance Target Tracking Scaling Example: Average ASG CPU to stay around 40% Scheduled Scaling Anticipate a scaling based on known usage patterns Example: increase the min. capacity to 10 at 5pm on Fridays Predictive Scaling Uses Machine Learning to predict the load \u0026raquo; Sources \u0026laquo; # https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\n\u0026raquo; References \u0026laquo; # Scalability \u0026amp; High Availability Elastic Load Balancing \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"10 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/10-auto-scaling-groups/","section":"ğŸ”°Posts","summary":"Auto Scaling Group contains a collection of EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management\u0026hellip;","title":"Auto Scaling Group"},{"content":"Elastic Load Balancing automatically distributes your incoming traffic across multiple targets (EC2 instances, containers, IP addresses) in one or more Availability Zones.\nIt monitors the health of its registered targets and routes the traffic only to healthy targets.\nAWS Elastic Load Balancing Introduction\nBenefits of using Load Balancer # Spread load across multiple downstream instances\nExpose a single point of access (DNS) to the application\nSeamlessly handle failures of downstream instances\nDo regular health checks to the instances\nSSL termination\nHigh Availability across Availability Zones\nELB is a managed Load Balancer\nAWS is responsible for upgrades, maintenance and High Availability AWS provides only a few configuration items 3 kinds of Load Balancers offered by AWS # Application Load Balancer Network Load Balancer Gateway Load Balancer HTTP / HTTPS / gRPC protocols (layer 7) TCP / UDP protocols (layer 4) Geneve Protocol on IP Packets (layer 3) HTTP Routing features High Performance (millions or requests per second) Route Traffic to Firewalls that you manage on EC2 instances Static DNS (URL) Static IP through Elastic IP Intrusion Detection 1. Application Load Balancer # HTTP / HTTPS only (layer 7) AWS ALB (Application Load Balancer) - Step By Step Tutorial\n2. Network Load Balancer # ultra high performance, allows for TCP (layer 4) Mastering AWS Network Load Balancer | ALB vs NLB | Step by Step Tutorial\n3. Gateway Load Balancer # Supports GENEVE protocol. Built for extra security (layer 3) \u0026raquo; Sources \u0026laquo; # https://aws.amazon.com/elasticloadbalancing/ https://aws.amazon.com/compare/the-difference-between-the-difference-between-application-network-and-gateway-load-balancing/ https://medium.com/@xiaotiancheng.orange/comparison-between-alb-nlb-and-glb-4444f3291173 https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-gateway-load-balancer/ Stephane Maarek\u0026rsquo;s AWS playlists on YouTube: https://www.youtube.com/@StephaneMaarek/playlists Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\n\u0026raquo; References \u0026laquo; # Scalability \u0026amp; High Availability Auto Scaling Groups \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"9 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/9-elastic-load-balancing/","section":"ğŸ”°Posts","summary":"Elastic Load Balancing automatically distributes your incoming traffic across multiple targets\u0026hellip;","title":"Elastic Load Balancing"},{"content":"Scalability #Scalability means that an application / infrastructure can handle greater loads by adapting.\nTwo kinds of scalability # Vertical Scalability Increasing the size of an instance Very common for non-distributed systems, i.e. Databases Hardware limits apply Horizontal Scalability (Elasticity) Increasing the number of instances Implies Distributed Systems Very common for web applications or modern applications Auto Scaling Groups For Horizontal Scaling (increasing the number of instances) we use Auto Scaling Groups and a Load Balancer High Availability # High Availability usually goes hand in hand with horizontal scaling High Availability means running application / infrastructure in at least 2 Availability Zones Goal of High Availability is to survive a data center loss / disaster High Availability is achieved by running Auto Scaling Groups (ASG) as well as Load Balancer in multi-AZ mode. # \u0026raquo; References \u0026laquo; # Elastic Load Balancing Auto Scaling Groups \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"8 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/8-scalability-high-availability/","section":"ğŸ”°Posts","summary":"Scalability means that an application / infrastructure can handle greater loads by adapting\u0026hellip;","title":"Scalability \u0026 High Availability"},{"content":"What is Amazon EC2 AMI #AMI are a customization of an EC2 instance.\nCustom software, configuration, etc. can be added Faster boot / configuration time because all your software is pre-packaged AWS EC2 AMI Tutorial\nAMI are build for a specific AWS region but can be copied across regions.\nAMI can be launched from # Public AMI - AWS provided Own AMI - managed by an organization AWS Marketplace - created by a vendor or another 3rd party (can be sold / bought) Steps to build an AMI # Start EC2 Instance and customize it Stop the instance (for data integrity) Build AMI - this will also create EBS Sapshots Launch instances from other AMI\u0026rsquo;s EC2 \u0026gt; Instances \u0026gt; Select an EC2 instance \u0026gt; Actions \u0026gt; Image and Templates \u0026gt; Create image EC2 Image Builder #EC2 Image Builder is a fully managed AWS service that helps you to automate the creation, management and deployment of customized, secure and up-to-date server images.\nCustom images can be created via AWS Console, AWS CLI or API.\nAutomate the creation, maintain, validate the build of EC2 AMI\u0026rsquo;s. Can run on a schedule and can be distributed to multiple AWS Regions.\nSummary #AMI # Create ready-to-use EC2 instances with own customizations EC2 Image Builder # automatically build, test and distribute AMI\u0026rsquo;s \u0026raquo; Sources \u0026laquo; # What is Image Builder Stephane Maarek\u0026rsquo;s AWS playlists on YouTube: https://www.youtube.com/@StephaneMaarek/playlists\n\u0026raquo; References \u0026laquo; # EC2 Storage \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"7 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/7-ami/","section":"ğŸ”°Posts","summary":"AMI are a customization of an EC2 instance\u0026hellip;","title":"Amazon Machine Image (AMI)"},{"content":"EBS Volume # EBS (Elastic Block Store) - Network drive that can be attached to instances while they run It allows instances to persist the data, even after their termination Can be mounted to 1 instance at a time Bound to an Availability Zone It\u0026rsquo;s a \u0026ldquo;Network Drive\u0026rdquo; it uses the network to communicate to the instance (there can be latency) it can be detached from an EC2 instance and attached to another one quickly It\u0026rsquo;s locked to an Availability Zone (AZ) an EBS Volume that is us-east-1a can\u0026rsquo;t be attached to us-east-1b to move a volume across AZ, snapshot has to be created first It has a provisioned capacity (size in GB and IOPS) you will get billed for a provisioned capacity You can increase the capacity of the drive EBS Volumes can be attached to only 1 EC2 instance at a time but EC2 instances can have multiple EBS Volumes attached to them\nEBS Delete on Termination # Controls the EBS behavior when EC2 instance terminates by default the root EBS volume is deleted (attribute enabled) by default any other attached EBS volume is not deleted (attribute disabled) This can be controlled by both, AWS Console and AWS CLI Use case: preserve root volume when instance is terminated EBS Snapshots # Make a backup (snapshot) of EBS volume at a point in time Not necessary to detach the volume to do the snapshot but recommended Snapshots can be copied across AZ or Regions EBS Snapshots features # EBS Snapshot Archive Move a Snapshot to an \u0026ldquo;archive tier\u0026rdquo; that is 75% cheaper Takes between 24 to 72 hours of restoring the archive EC2 \u0026gt; Snapshots \u0026gt; Recycle Bin \u0026gt; Create retention rule When Snapshot Archive is enabled, it is possible to Archive it from a drop-down box in AWS Console. Archived Snapshots appear in the Recycle Bin.\nRecycle Bin for EBS Snapshots Setup rules to retain deleted snapshots so you can recover them after an accidental deletion Specify retention (from 1 day to 1 year) EC2 Instance Store # EBS volumes are network drives with good but \u0026ldquo;limited\u0026rdquo; performance\nIf high-performance hardware disk is required, EC2 Instance Store can be used\nBetter I/O performance\nEC2 Instance Store is ephemeral (data is lost after stopping EC2 instance)\nUse case: buffer, cache, scratch data, temporary content\nRisk of data loss if hardware fails.\nEFS - Elastic File System # Managed NFS (Network File System) that can be mounted on 100s of EC2 instances EFS works with Linux EC2 instances only and is multi-AZ. Highly-available, scalable, expensive (3x gp2 EBS), pay per use, no capacity planning EFS Infrequent Access (EFS-IA) # Storage class that is cost-optimized for files not accessed every day up to 92% lower cost compared to EFS Standard When enabled, EFS will automatically move your files to EFS-IA based on last time they were accessed Enable EFS-IA with a Lifecycle Policy Example: move files that are not accessed for 60 days to EFS-IA Transparent to the applications accessing EFS (apps don\u0026rsquo;t see whether file is in EFS or EFS-IA) EBS vs EFS # Feature EBS EFS Access Model Single-instance Multi-instance Use Cases Databases, Development Web serving, big data Performance Low-latency, high IOPS High throughput Scalability Limited to provisioned volume Auto-scales to petabytes File Size Limit No limit 47.9 TiB Accessibility Not accessible over the internet Shared across instances Pricing Cheaper for single-instance Cost-effective for shared use More: https://aws.amazon.com/efs/when-to-choose-efs/ https://lucidity.cloud/blog/ebs-vs-efs https://www.cloudzero.com/blog/ebs-vs-efs/ Amazon FSx #3rd party with high-performance file system on AWS.\nFSx for Lustre - fully managed, high-performance, scalable file storage for High Performance Computing (HPC). Use cases: Machine Learning, Analytics, Video processing, Financial Modelling Scales up to 100s GB/s, millions of IOPS, sub-ms latencies FSx for Windows File Server - fully managed, highly reliable and scalable Windows native shared file system built on Windows File Server. Supports SMB and NTFS file systems. Integrated with AD for security. Can be accessed from AWS or from On-Premise. FSx for NetApp ONTAP - Summary #EBS Volumes\nnetwork drives attached to one EC2 instance at a time Mapped to an Availability Zones Can use EBS snapshots for backups and then transferring across AZ\u0026rsquo;s EC2 Instance Store\nhigh performance hardware disk attached to our EC2 instance ephemeral (data lost if instance stopped or terminated) EFS\nNetwork File System Can be attached to 100s of EC2 instances spans through a region expensive compared to EBS EFS-IA\nCost-optimized storage class for infrequently accessed files Lifecycle Policy for automatically moving files between tiers FSx for Windows\nNetwork File System for Windows servers SMB and NTFS Can be accessed from on-prem and the cloud FSx for Lustre\nHigh Performance (HPC) Linux file system \u0026raquo; Sources \u0026laquo; # https://aws.amazon.com/efs/when-to-choose-efs/ https://lucidity.cloud/blog/ebs-vs-efs https://www.cloudzero.com/blog/ebs-vs-efs/ \u0026raquo; References \u0026laquo; # S3 EC2 AMI \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"6 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/6-storage/","section":"ğŸ”°Posts","summary":"Storage in AWS\u0026hellip;","title":"Storage"},{"content":"Amazon Security Groups # Security Groups are the fundamental of network security in AWS They control how traffic is allowed in or out of our EC2 instances Security Groups only contain allow rules (as oppose to NACL or AWS Network Firewall) Security Groups rules can reference an IP or another Security Group Security Groups are acting as a \u0026ldquo;firewall\u0026rdquo; for EC2 instances Security Groups scope # Access to Ports Authorized IP ranges - IPv4 and IPv6 Control inbound network Control outbound network Security Groups principals # Can be attached to multiple instances Locked down to a region / VPC combination Lives \u0026ldquo;outside\u0026rdquo; of an EC2 instance - if traffic is blocked, EC2 won\u0026rsquo;t see it It\u0026rsquo;s a good practice to maintain one separate SG for SSH access If application is not accessible (time out) then it\u0026rsquo;s a Security Group issue If application gives a \u0026ldquo;connection refused\u0026rdquo; error then it\u0026rsquo;s an application error or it\u0026rsquo;s not launched All inbound traffic is blocked by default All outbound traffic is allowed by default \u0026raquo; Sources \u0026laquo; # Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\n\u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"5 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/5-security-groups/","section":"ğŸ”°Posts","summary":"AWS Security Groups are the fundamental of network security in AWS. They control how traffic is allowed in or out of our EC2 instances\u0026hellip;","title":"Security Groups"},{"content":"What is Amazon EC2 #An Amazon EC2 instance is a virtual server in the AWS Cloud. When you launch an EC2 instance, the instance type that you specify determines the hardware available to your instance. Each instance type offers a different balance of compute, memory, network, and storage resources.\nEC2 sizing and configuration options # Operating System (OS) - Linux, Windows, Mac OS How much compute power \u0026amp; CPU cores How much RAM How much storage space Network-attached (EBS \u0026amp; EFS) Hardware (EC2 Instance Store) Network card: speed of the card, Public IP address Firewall rules: security group Bootstrap script (configure at first launch): EC2 User Data Amazon EC2 Instance Types #More info: EC2 Instance Types, EC2Instances.info\nGeneral Purpose (M, T) - General Compute Optimized (C) - Compute bound applications that benefit from thigh performance processors Memory Optimized (R, X) - Optimized to deliver fast performance for workloads that process large data sets in memory (i.e. ElastiCache) Accelerated Computing (P, G, Trn, Inf, DL, F, VT1) - Calculations, graphics processing or data pattern matching Storage Optimized (I, Dx, H1) - High, sequential read and write access to very large data sets on local storage. Low-latency, random I/O operations per second (IOPS) HPC Optimized (Hpc) - High Performance Computing - complex simulations and deep learning workloads Instance type naming conventions #More info: Amazon EC2 Instance type naming conventions\nExample #More info: Amazon EC2 Instance type naming conventions\nSeries Options - C â€“ Compute optimized\n- Hpc â€“ High performance computing\n- I â€“ Storage optimized\n- M â€“ General purpose\n- P â€“ GPU accelerated\n- R â€“ Memory optimized - a â€“ AMD processors\n- i â€“ Intel processors\n- b â€“ Block storage optimization\n- d â€“ Instance store volumes\n- e â€“ Extra storage (for storage optimized instance types), extra memory (for memory optimized instance types), or extra GPU memory (for accelerated computing instance types).\n- n â€“ Network and EBS optimized\n- q â€“ Qualcomm inference accelerators\n- z â€“ High CPU frequency Launching EC2 instance #EC2 \u0026gt; Launch Instance Name and Tags AMI Instance Type (t3.micro,t3.large,etc.) Key Pair Network Settings (Security Group) Storage Advanced Settings Domain Join IAM Instance Profile Hostname type Instance auto-recovery Shutdown behavior Termination protection Placement group Purchasing option None Capacity Blocks Spot Instances Capacity reservation User data Example user data #User data is only bootstrap script and only starts once during the machine creation.\n#!/bin/bash # Use this for your user data (script from top to bottom) # install httpd (Linux 2 version) yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello World from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Connecting to EC2 from Windows Terminal #ssh -i .\\.ssh\\id_rsa_aws25 ec2-user@ec2-3-95-191-175.compute-1.amazonaws.com EC2 Instances Purchasing options # On-Demand instances - short workload, predictable pricing, pay by second Pay for what used Linux or Windows - billing per second after the first minute All other operating systems - billing per hour Highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interrupted workloads Reserved (1 \u0026amp; 3 years) Up to 72% discount compared to On-demand You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS) Reserved Instances - long workloads Payment options - No upfront, Partial Upfront, All Upfront Scope: Regional or Zonal Recommended for steady-state usage applications (think database) Can be bought and sold in the Reserved Instance Market place Convertible Reserved Instances - long workload with flexible instances Can change the EC2 instance type, instance family, OS, scope and tenancy Up to 66% discount Saving plans (1 \u0026amp; 3 years) - commitment to an amount of usage, long workloads Get discount based on long-term usage Commit to a certain type of usage ($10/hour for 1 or 3 years) Usage beyond EC2 Savings Plans is billed at the On-Demand price Locked to a specific instance family \u0026amp; AWS region Flexible across Instance Size (e.g. m5.xlarge, m5.2xlarge) OS (e.g. Linux, Windows) Tenancy (Host, Dedicated, Default) Spot Instances - short workloads, cheap, can lose instances Up to 90% discount Instances can be \u0026ldquo;lost\u0026rdquo; at any point if max price is less than current spot price Recommended for workloads that are resilient to failure Batch jobs Data analysis Image processing Dedicated Hosts - book an entire physical server, control instance placement A physical server with EC2 instance capacity fully dedicated to your use Recommended for for companies with strong compliance requirements OR server-bound software licenses (per-socket, per-core) Purchasing Options: On-demand Reserved Most expensive option Dedicated Instances - no other customers will share your hardware Instances run on a dedicated hardware May share hardware with other instances in the same account No control over instance placement Dedicated Host vs Dedicated Instance: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\nCapacity Reservations - reserve capacity in a specific AZ for any duration \u0026raquo; Sources \u0026laquo; # EC2 Instance Types Amazon EC2 Instance type naming conventions EC2Instances.info Dedicated Host vs Dedicated Instance: Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\n\u0026raquo; References \u0026laquo; # Security Groups \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"4 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/4-ec2/","section":"ğŸ”°Posts","summary":"An Amazon EC2 instance is a virtual server in the AWS Cloud\u0026hellip;","title":"EC2"},{"content":"Setting up a budget #Billing and Cost Management is only available for the root user (or user with the right privileges).\n1. Enabling Billing and Cost Management for IAM user # Log in as root to AWS Console Click on your user in the top right corner, select an account Scroll down to \u0026ldquo;IAM user and role access to Billing information\u0026rdquo; Activate IAM access This will allow access to billing information for IAM users that are in Administrators group.\n2. Create a budget #Billing and Cost Management \u0026gt; Budgets \u0026gt; Create a budget \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"3 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/3-budget/","section":"ğŸ”°Posts","summary":"AWS Budget\u0026hellip;","title":"Budget"},{"content":"IAM = Identity and Access management #IAM is AWS Global service.\nGroups only contain users, NOT other groups.\nUsers don\u0026rsquo;t have to belong to a group and user can belong to multiple groups.\nUsers or Groups can have assigned JSON documents called policies. Those policies define permissions for the users / groups.\nIn AWS you apply the least privilege principle: don\u0026rsquo;t give user more permission than they need.\nCreating user in AWS CLI #IAM console \u0026gt; Users Create user # Create group \u0026ldquo;admin\u0026rdquo; # IAM policies structure # IAM policies structure includes:\nVersion: policy version ID (Optional): identifier Statement: one or more statements (required) SID: identifier (optional) Effect: Allow or Deny Principal: account / user / role to which policy is applied to Action: list of allowed or denied actions Resource: list of resources to which the action is applied to Condition: conditions for when the policy is applied (optional) MFA # Accessing AWS # AWS Management Console - protected by password + MFA AWS Command Line (CLI) - protected by access keys AWS Software Development Kit (SDK) - for code - protected by access keys Access Keys can be generated through AWS Console. Users manage their own access keys. #Access Key ID = username\nSecret Access Key = password\nCreating Access Key #IAM \u0026gt; Users \u0026gt; Username \u0026gt; Security Credentials \u0026gt; Access Keys \u0026gt; Create Access Key \u0026gt; Command Line Interface (CLI) Configuring AWS CLI with the new access key ## Configure AWS CLI aws configure # Test aws iam list-users IAM roles for Services # Some AWS services will need to perform actions on your behalf Those AWS services will need permissions to be assigned with IAM Roles Common Roles for Services:\nEC2 Instance Roles Lambda Function Roles Roles for CloudFormation Create AWS Service Role #IAM \u0026gt; Roles \u0026gt; Create role \u0026gt; AWS service Add permissions:\nIAM Security Tools #IAM Credentials Report (account-level) # Report that lists all users and status of their credentials IAM \u0026gt; Credentials Report IAM Access Advisor (user-level) # Access Advisor shows the service permissions granted to a user and when those services were last accessed IAM \u0026gt; Users \u0026gt; Username \u0026gt; Last Accessed IAM Access Advisor (Last Accessed) can be used to determine what user is accessing and to adjust his / her role in line with the \u0026ldquo;Least Privilege Principle\u0026rdquo;.\nIAM Best Practices # Don\u0026rsquo;t use root account One physical user = One AWS user Assign users to groups and assign permissions (policies) to groups Create strong password policy Use and enforce MFA Create and use Roles for giving permissions to AWS services Use Access Keys for Programmatic access (CLI / SDK) Audit permissions using IAM Credentials Report and IAM Access Advisor Never share IAM users \u0026amp; Access Keys Shared Responsibility Model for IAM # AWS Organization Infrastructure (global network security) Users, Groups, Roles, Policies management and monitoring Configuration and vulnerability analysis Enabling MFA on all accounts Compliance validation Rotating keys Using IAM tools to apply appropriate permissions Analyze access patterns and review permissions \u0026raquo; Sources \u0026laquo; # AWS Global Infrastructure: AWS Global Infrastructure Shared Responsibility Model: Shared Responsibility Model - Amazon Web Services (AWS) Full YouTube Rahul\u0026rsquo;s AWS Course: https://www.youtube.com/playlist?list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP\nSecurity and compliance What is IAM? What is AWS IAM Identity Center? IAM identities AWS security documentation \u0026raquo; References \u0026laquo; # Account Management and Billing \u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"2 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/2-iam/","section":"ğŸ”°Posts","summary":"Identity and Access management\u0026hellip;","title":"Identity and Access management"},{"content":" Private Cloud Public Cloud Hybrid Cloud Cloud services used by a single organization, not exposed to the public Cloud resources owned and operated by a third party cloud service provider delivered over the internet Keep some servers on premises and extend some capabilities to the cloud Complete control Security for sensitive applications Meet specific business needs Five characteristics of Cloud Computing # On-demand self service Users can provision resources and use them without human interaction from the service provider Broad network access Resources available over the network can be accessed by diverse client platforms Multi-tenancy and resource pooling Multiple customers can share the same infrastructure and applications with security and privacy Multiple customers are serviced from the same physical resources Rapid elasticity and scalability Automatically and quickly acquire and dispose resources when needed Quickly and easily 10. Auto Scaling Group based on demand Measured service Usage is measured, users pay for what they used Six advantages of Cloud Computing # Trade capital expense (CAPEX) for operational expense (OPEX) Pay on-demand: don\u0026rsquo;t own the hardware Reduced Total Cost of Ownership (TCO) \u0026amp; Operational Expense (OPEX) Benefit from massive economies of scale Prices are reduced as AWS is more efficient due to a large scale Stop guessing the capacity Scale based on actual measured usage Increase speed and agility Stop spending money running and maintaining data centers Go global in minutes: leverage the AWS Global Infrastructure Problems solved by the cloud # Flexibility: change resources when needed Cost-Effectiveness: pay as you go and for what you use Scalability: accommodate larger loads by making hardware stronger or adding additional nodes Elasticity: ability to scale out and scale in when needed High-Availability and Fault-Tolerance: build across data centers Agility: rapidly develop, test and launch software applications Types of Cloud Computing #Infrastructure as a Service (IaaS) # Provides building blocks for cloud IT Provides networking, compute, storage Highest level of flexibility Easy parallel with traditional on-premises IT Platform as a Service (PaaS) # Removes the need for your organization to manage the underlying infrastcucture Focus on the deployment and management of your applications Software as a Service (SaaS) # Completed product that is run and managed by the service provider AWS Global Infrastructure # AWS Regions AWS Availability Zones AWS Data Centers AWS Edge Locations / Points of Presence More about AWS Global Infrastructure: AWS Global Infrastructure\nShared Responsibility Model # Shared Responsibility Model: Shared Responsibility Model - Amazon Web Services (AWS)\nSecurity and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve the customerâ€™s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates.\n\u0026raquo; Sources \u0026laquo; # AWS Global Infrastructure: AWS Global Infrastructure\nShared Responsibility Model: Shared Responsibility Model - Amazon Web Services (AWS)\nCloud computing with AWS\nWhat is cloud computing?\nAWS Well-Architected Framework\nHigh availability and scalability in AWS\nMigration and transfer\nCloud economics in AWS resources\n\u0026raquo; Table of contents (CLF-C02) \u0026laquo; #\r1. What is Cloud Computing\r2. IAM\r3. Budget\r4. EC2\r5. Security Groups\r6. Storage\r7. AMI\r8. Scalability \u0026 High Availability\r9. Elastic Load Balancing\r10. Auto Scaling Group11. S3\r12. Databases\r13. Other Compute Services\r14. Deployments\r15. AWS Global Infrastructure\r16. Cloud Integrations\r17. Cloud Monitoring\r18. VPC\r19. Security and Compliance\r20. Machine Learning\r21. Account Management and Billing\r22. Advanced Identity\r23. Other Services\r24. AWS Architecting \u0026 Ecosystem\r25. Preparing for AWS Practitioner exam\r\u0026raquo; Disclaimer \u0026laquo; #\rThis series draws heavily from Stephane Maarek\u0026rsquo;s Ultimate AWS Certified Cloud Practitioner course on Udemy.\nHis content was instrumental in helping me pass the certification.\nAbout the instructor\rğŸŒ Website\rğŸ“º YouTube\rğŸ’¼ LinkedIn\rğ• x.com\râ„¹ï¸Shared for educational purposes only, no rights reserved.\n","date":"1 August 2025","permalink":"https://robk.uk/posts/training/aws/2025-aws-cloud-practitioner/1-what-is-cloud-computing/","section":"ğŸ”°Posts","summary":"What is Cloud Computing\u0026hellip;","title":"What is Cloud Computing"},{"content":" Talos Linux is a minimal, hardened and immutable Linux distribution designed for Kubernetes. It supports cloud platforms, bare metal and virtualization, and all system management is done via an API. ğŸ“º YouTube Tutorial # ğŸ’¡Talos Linux - The Best OS For Kubernetes.\nThis blog post describes Talos Linux installation on Proxmox Virtual Environment. #â€¼ï¸When downloading ISO image, ensure downloading one with QEMU guest agent support for Proxmox.\nğŸ’¾ Download Talos Linux # If you need the QEMU guest agent so you can do guest VM shutdowns of your Talos VMs on Proxmox, then you will need a custom ISO To get this, navigate to https://factory.talos.dev/ Scroll down and select your Talos version (v1.10.0 for example) Then tick the box for siderolabs/qemu-guest-agent and submit This will provide you with a link to the bare metal ISO Building Talos Kubernetes cluster using patches #Install talosctl, kubectl and k9s 1 #On your Jump Server, install talosctl, kubectl and k9s. I am using Ubuntu Linux in this example.\n1 2 3 4 5 6 7 8 9 10 # talosctl curl -sL https://talos.dev/install | sh # kubectl sudo snap install kubectl --classic # k9s sudo snap install k9s sudo ln -s /snap/k9s/current/bin/k9s /snap/bin/ Cluster Build #1. Generate Secrets #cd ~ talosctl gen secrets 2. Export Variables #export CLUSTER_IP=192.168.XX.AB export CLUSTER_NAME=talos-cluster export CONTROL_PLANE_IP1=192.168.XX.AC export CONTROL_PLANE_IP2=192.168.XX.AD export CONTROL_PLANE_IP3=192.168.XX.AE export WORKER_IP1=192.168.XX.BA export WORKER_IP2=192.168.XX.BB export WORKER_IP3=192.168.XX.BC 3. Generate config files for the cluster using patches #git clone https://github.com/rtdevx/homelab.git cd ~/homelab/kubernetes/talos talosctl gen config $CLUSTER_NAME https://$CLUSTER_IP:6443 \\ --with-secrets ~/secrets.yaml \\ --config-patch @patches/all.yaml \\ --config-patch-control-plane @patches/cp.yaml \\ --config-patch-worker @patches/worker.yaml \\ --output ~/rendered/ 4. Set Up the cluster #cd ~ talosctl apply -f rendered/controlplane.yaml -n $CONTROL_PLANE_IP1 --insecure talosctl apply -f rendered/controlplane.yaml -n $CONTROL_PLANE_IP2 --insecure talosctl apply -f rendered/controlplane.yaml -n $CONTROL_PLANE_IP3 --insecure 5. Add Worker Nodes #talosctl apply -f rendered/worker.yaml -n $WORKER_IP1 --insecure talosctl apply -f rendered/worker.yaml -n $WORKER_IP2 --insecure talosctl apply -f rendered/worker.yaml -n $WORKER_IP3 --insecure â€¼ï¸Note: --insecure is only used for the initial install. After cluster is installed with it\u0026rsquo;s newly generated keys, this option should not be used.\n6. Configure talosctl #mkdir -p ~/.talos cp rendered/talosconfig ~/.talos/config # Test talosctl config contexts # Set endpoints for talosctl talosctl config endpoint $CONTROL_PLANE_IP1 $CONTROL_PLANE_IP2 $CONTROL_PLANE_IP3 # Set config node talosctl config node $CONTROL_PLANE_IP1 7. Install (Bootstrap) Kubernetes #talosctl bootstrap # Fetch kubeconfig talosctl kubeconfig 8. Add kubectl alias (Optional) #vi ~/.bashrc #Custom Aliases alias k=\u0026#39;kubectl\u0026#39; \u0026raquo; Sources \u0026laquo; # Proxmox Official Documentation: https://www.talos.dev/v1.10/talos-guides/install/virtualized-platforms/proxmox/ Building Cluster using patches: https://www.talos.dev/v1.10/talos-guides/configuration/patching/ \u0026ldquo;Install talosctl, kubectl and k9s\u0026quot;âš ï¸\nERROR:\nk9s command not found after snap install issue in Ubuntu 24.04 Solution:\nCommand highlighted in \u0026ldquo;Install talosctl, kubectl and k9s\u0026rdquo; must be executed in order to solve the problem. More about the problem: https://github.com/derailed/k9s/issues/2128\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"23 July 2025","permalink":"https://robk.uk/posts/devops/kubernetes/talos-cluster-build/","section":"ğŸ”°Posts","summary":"Building Talos Linux Kubernetes Cluster\u0026hellip;","title":"Building Talos Linux Kubernetes Cluster"},{"content":"","date":null,"permalink":"https://robk.uk/categories/homelab/","section":"Categories","summary":"","title":"Homelab"},{"content":"","date":null,"permalink":"https://robk.uk/categories/proxmox/","section":"Categories","summary":"","title":"Proxmox"},{"content":"What is Game Theory? #Game Theory, branch of applied mathematics that provides tools for analyzing situations in which parties, called players, make decisions that are interdependent.\nThis interdependence causes each player to consider the other playerâ€™s possible decisions, or strategies, in formulating strategy. A solution to a game describes the optimal decisions of the players, who may have similar, opposed, or mixed interests, and the outcomes that may result from these decisions.\nSource: https://www.britannica.com/science/game-theory\nWhere the Game Theory is applied #Despite of the above definition, Game Theory is present everywhere around us in our daily life. For that reason it is important to be at least briefly familiar with the concept.\nSo much of the theory\u0026hellip; This is literally mind blowing video that really is worth watching\u0026hellip;\nEnjoy ğŸ™\nThis game theory problem will change the way you see the world\n","date":"22 July 2025","permalink":"https://robk.uk/posts/productivity/2025/250822-game-theory/","section":"ğŸ”°Posts","summary":"The best video about Game Theory I\u0026rsquo;ve ever watched\u0026hellip;","title":"Game theory"},{"content":"","date":null,"permalink":"https://robk.uk/tags/other/","section":"Tags","summary":"","title":"Other"},{"content":"","date":null,"permalink":"https://robk.uk/categories/productivity/","section":"Categories","summary":"","title":"Productivity"},{"content":"","date":null,"permalink":"https://robk.uk/tags/zettelkasten/","section":"Tags","summary":"","title":"Zettelkasten"},{"content":"What is the Zettelkasten method? #Zettelkasten is a system of identifiers to link the notes and navigating between them. This methodology leads to higher productivity.\nZettelkasten is not only a note taking system. It is a tool that is used for thinking since it brings a structure to note taking and \u0026ldquo;connecting the dots\u0026rdquo;.\nThis method is often called a \u0026ldquo;second brain\u0026rdquo;. And it is so for a reason\u0026hellip;\nThe history of the Zettelkasten method #The Zettelkasten method was created in the 20th century by social scientist Niklas Luhman. He used this system to organize his thoughts and research, which allowed him to publish more than 50 books and hundreds of articles. Luhman used physical slip boxes for his Zettelkasten, adding notes and sub notes into the box as he created new ideas.\nWriting is understanding! #Each note contains the thought or idea and should be written in own words. This helps mapping the ideas in human brain. Process similar to translation takes place which helps with understanding the topic.\nStructuring, simplifying and linking ideas helping storing them outside of the brain and helping with solving complex problems.\nA good system removes the need for remembering everything.\nI only do what is easy. I only write when I immediately know how to do it. If I falter for a moment, I put the matter aside and do something else.\nNiklas Luhmann\nHighly productive people deflect resistance like judo champions. Having a flexible workflow allows doing this. This is known as \u0026ldquo;effortless productivity\u0026rdquo;.\nContent Consumption # Intentionality for content consumption 80% of content consumed scheduled and aligned with goals articles reading, YouTube videos watching are all scheduled beforehand with a clear purpose each act of content consumption should align with one of the goals set 20% of content consumption used for relaxing Intentionality # Consume the content with an intention of writing increased focus allows adapting more observant and analytical perspective Being intentional about content consumption will limit the content consumption Obsidian - markdown-based, multi-platform note taking app #Obsidian is a note-taking app that lets you link notes together. Learn how to download, create, and organize your notes with Obsidian in this comprehensive guide.\nwww: https://obsidian.md/\nZettelkasten Smart Notes: Step by Step with Obsidian\nNote Taking # Clear thinking becomes clear writing. One cannot exist without the other\nWilliam Zinnser\nFleeting notes #Notes being taken during content consumption are called Fleeting Notes. These are being used only as a reminder to take action later and put notes in the right order.\nEphemeral - temporary, should be processed on the same or next day Brief - few words or ideas Quick capture This ability is a cornerstone of effective learning and productivity.\nPermanent notes # Articulated\nWritten in own words\nSolidified\nOne idea per card\nDo not copy\nBrief and concise\nWrite to publish\nQuotes allowed - they add context and can add value\nConsistency in transferring Fleeting notes to Permanent notes should be frequent / daily.\nLinking notes # Linking permanent notes is a powerful way of building knowledge Allows to see connection between notes Makes easier to recall and apply knowledge Don\u0026rsquo;t collect information. Gather knowledge instead. Linking leads to surprises\u0026hellip;\nTags vs Folders #Folders are for separating, tags are for uniting\nAvoid moving files around Navigation benefits Satisfaction of collecting in folders Note taking system should free up time Sources #Zettelkasten # Introduction to the Zettelkasten Method How to Use the Zettelkasten Method for Notetaking Obsidian # Obsidian - Sharpen your thinking ","date":"19 July 2025","permalink":"https://robk.uk/posts/productivity/2025/250819-zettelkasten/","section":"ğŸ”°Posts","summary":"Increase your productivity with zettelkasten, highly effective note taking method\u0026hellip;","title":"Zettelkasten note taking method"},{"content":" 1. Pomodoro ritual #25 minutes focus, 5 minutes break.\nClose your eyes, take a breath and make a focus statement (what to accomplish in those 25 minutes). Declare out loud OR write down. filters distractions sharpens attention mentally commits completion 2. The Kanji Recall method #Kanji = visual memory / mental map.\nGrid Recall (repetition) Visualizing Drawing along with saying / reading / writing words 3. Rewrite notes #Rewriting notes stimulates active recall.\nThe basic definition Reworded version Diagram Story or metaphor In their own words like teaching a friend 4. Empty cup mindset #This method originates from Zen Buddhism.\nYou can\u0026rsquo;t pour tea into a cup. Idea of dropping ego while learning and discover what student doesn\u0026rsquo;t know yet.\nApproach learning with humility and curiosity, not pressure and ego (i.e. grades, etc\u0026hellip;). Value is not in your score but in your growth.\n5. Chain learning #Study like everything is connected. Because in real world \u0026hellip;it is.\nRead more: Zettelkasten 6. Study like a Sensei that is teaching others #Mastery is shown in clarity, when you can explain the subject. When you teach something, you truly learn it. Explain it like you were explaining it to a 10 year old.\nThis approach was first avowed by Richard Feynman, a Nobel winning physicist.\nIf you can\u0026rsquo;t explain it simply, you don\u0026rsquo;t understand it well enough.\nRichard Feynman\n*although according to many sources this quote is attributed to Albert Einstein\n7. The forbidden night school #From 7pm to 9pm - no phones, no distractions, no background noise. Collaborate on reviewing, revising if and where possible. Mostly for students.\nMore Videos # Sources # 7 FAST Chinese and Japanese SECRETS for students to EASILY become TOPPERS: https://www.youtube.com/watch?v=BszXLBKWscA The Japanese Student Routine That Helps You Remember EVERYTHING: https://www.youtube.com/watch?v=HC2IaW3DAfQ ","date":"18 July 2025","permalink":"https://robk.uk/posts/productivity/2025/250717-7-studying-techniques/","section":"ğŸ”°Posts","summary":"Pomodoro, Kanji, Rewrite notes, Empty cup mindset, Chain learning, Study like a sensei, The forbidden night school\u0026hellip;","title":"7 Chinese and Japanese studying techniques"},{"content":" Experienced IT professional accomplished in supporting and delivering secure, resilient, and PCI-compliant infrastructure solutions across international payment ecosystems. I specialize in building and automating high-stakes platforms for credit and debit card transaction processing and fraud detection systems (3D-Secure). Resilient platforms and infrastructures trusted by global banks and financial institutions.\nA strong advocate of DevOps and Agile methodologies, I bring end-to-end transparency and automation to infrastructure delivery. Skilled across Linux and Windows Server environments, I leverage tools like Ansible and PowerShell to streamline provisioning, hardening, and support of critical systems.\nMy skillset # Tech Stack Details ğŸ§­ Agile Jira\nAgile delivery and Jira project design and automation, SharePoint and Confluence (collaboration and documentation). â˜ï¸ Cloud Platforms AWS Designing and managing scalable, high-performance cloud environments to support dynamic workloads. ğŸ”§ DevOps Tools AWS CodePipeline / AWS CodeBuild, GitHub Actions\nStreamlining CI/CD pipelines by integrating testing, monitoring, and security tools to maintain high code quality and system performance. ğŸ“œ Infrastructure as Code (IaC) Ansible, Terraform Automating the provisioning and management of infrastructure, ensuring consistency, reliability, and scalability across deployments. ğŸ–¥ï¸ Systems Administration Linux, Windows\nMulti-disciplined - Linux, Windows Server. Feel free to take look at my homelab GitHub page for more details. CV Download # ğŸ“œ Download CV Contact # LinkedIn *This website does not use cookies, analytics, trackers, or any form of data collection. External links (including YouTube embeds) may have different policies - I can't vouch for what happens on other sites. Web crawlers are explicitly disallowed from indexing or scanning this site.\n**No rights reserved. For educational and informational purposes only. ","date":"17 July 2025","permalink":"https://robk.uk/about/","section":"Welcome my DevOps blog.","summary":"Learn more about me and why I am starting this blog.","title":"About me"},{"content":"","date":null,"permalink":"https://robk.uk/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":" Example how a new post can be structured. Header information, highlights, etc\u0026hellip;\nThe worldâ€™s fastest framework for building websites #Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nAnything that requires attention.\nCan be enhanced with emojis or colors\u0026hellip;\nLinking internal content # Installing Hugo with Congo Categories Tags [Better Comments Options]({ {\u0026lt; ref \u0026#34;my-vscode-setup/#better-comments-options-\u0026#34; \u0026gt;} }) Folders # Folder location: content/posts/category Replace thumb.jpg Replace cover.jpg (if applicable) *use feature.jpg if thumb and cover are the same (see: #feature-cover-and-thumbnail-images for more information.)\nğŸŸ¢Colors # Color HEX HTML Usage Red #EB4925 \u0026lt;font color=#EB4925\u0026gt; Warning, Important Yellow #EBAC25 \u0026lt;font color=#EBAC25\u0026gt; Highlights Green #C7EB25 \u0026lt;font color=#C7EB25\u0026gt; Highlights, interesting, etc. Blue #27D3F5 \u0026lt;font color=#27D3F5\u0026gt; Color used for content to be corrected later (i.e. new links to not-yet existing content, etc\u0026hellip;) HTML Color Codes: https://htmlcolorcodes.com/\nğŸ·ï¸Icons # list of available icons:\nhttps://jpanther.github.io/congo/samples/icons/ https://jpanther.github.io/congo/docs/shortcodes/#icon Insert an icon: { {\u0026lt; icon \u0026quot;circle-info\u0026quot; \u0026gt;} }\nâ¤ï¸Emojis # Emoji Usage â€¼ï¸ Attention ğŸš©/ âš ï¸ Issue / Problem ğŸš« / â˜¢ï¸ / â˜£ï¸ Prohibited / Hazard ğŸ” Issue Investigation âœ… Solution / Resolved â„¹ï¸ Info ğŸ’¡ Idea ğŸ‘¨ğŸ»â€ğŸ’» To Do ğŸ’¾ Download ğŸ“º YouTube âœ¨/ ğŸ”¥ Post title highlight ğŸ—‚ï¸ Series ğŸ… Certified ğŸ“„ File / Code ğŸ’¾ https://emojipedia.org/\nğŸ”¥References #If anything requires further explanation, it can be referenced1 like that\u0026hellip;\nReferenced text[^Ref1] (word!). [^Ref1]: This is the explanation of the referenced point 1. Organizing content # Content: https://jpanther.github.io/congo/docs/getting-started/#organising-content Shortcodes: https://jpanther.github.io/congo/docs/shortcodes Content examples: https://jpanther.github.io/congo/docs/content-examples/ Markdown # https://jpanther.github.io/congo/samples/markdown/ Shortcodes # https://jpanther.github.io/congo/samples/rich-content/ https://gohugo.io/content-management/shortcodes/#use-hugos-built-in-shortcodes ğŸ–° Buttons and Badges # Button Badge { {\u0026lt; button href=\u0026quot;../../../tags\u0026quot; target=\u0026quot;_self\u0026quot; \u0026gt;}}\nğŸ·ï¸ Tags\n{ {\u0026lt; /button \u0026gt;}} { {\u0026lt; badge \u0026gt;}}\nBadge Shortcode\u0026hellip;\n{ {\u0026lt; /badge \u0026gt;}} â‰¥ Code Blocks #Code block with backticks #\u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces #\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode # 1 2 3 4 5 6 7 8 9 10 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Mermaid - Diagrams and Flowcharts #https://jpanther.github.io/congo/samples/diagrams-flowcharts/ https://jpanther.github.io/congo/docs/shortcodes/#mermaid https://mermaid.js.org https://mermaid.js.org/syntax/architecture.html https://mermaidviewer.com/\nğŸ–« PowerShell string replace #Get-ChildItem -Path \u0026#34;C:\\Users\\robk\\Documents\\Documents\\Notes\\Obsidian\\Zettelkasten\\4 - Content Creation\\Git\\rtdevx.github.io\u0026#34; -Recurse -Filter *.md | ForEach-Object { (Get-Content $_.FullName) -replace \u0026#39;\u0026lt;font color=#f4e40b\u0026gt;\u0026#39;, \u0026#39;\u0026lt;font color=#EBAC25\u0026gt;\u0026#39; | Set-Content $_.FullName } ğŸª Other Elements - abbr, sub, sup, kbd, mark # Example Code GIF is a bitmap image format. \u0026lt;abbr title=\u0026quot;Graphics Interchange Format\u0026quot;\u0026gt;GIF\u0026lt;/abbr\u0026gt; is a bitmap image format. H2O H\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt;O Xn + Yn = Zn X\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; + Y\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; = Z\u0026lt;sup\u0026gt;n\u0026lt;/sup\u0026gt; Press CTRL+ALT+Delete to end the session. Press \u0026lt;kbd\u0026gt;CTRL\u0026lt;/kbd\u0026gt;+\u0026lt;kbd\u0026gt;ALT\u0026lt;/kbd\u0026gt;+\u0026lt;kbd\u0026gt;Delete\u0026lt;/kbd\u0026gt; to end the session. Most salamanders are nocturnal, and hunt for insects, worms, and other small creatures. Most \u0026lt;mark\u0026gt;salamanders\u0026lt;/mark\u0026gt; are nocturnal, and hunt for insects, worms, and other small creatures. \u0026raquo; Sources \u0026laquo; # Congo, a powerful, lightweight theme for Hugo built with Tailwind CSS Congo Docs Congo Samples HTML Color Codes: https://htmlcolorcodes.com/ Emojis: https://emojipedia.org/ \u0026raquo; References \u0026laquo; # Installing Hugo with Congo Categories Tags \u0026raquo; Table of contents \u0026laquo; # 1. What is Cloud Computing 2. IAM 3. Budget 4. EC2 5. Security Groups 6. Storage 25. Preparing for AWS Practitioner exam \u0026raquo; Disclaimer \u0026laquo; # Disclaimer: Content for educational purposes only, no rights reserved. { {\u0026lt; alert \u0026#34;circle-info\u0026#34; \u0026gt;}} Disclaimer: _Content for educational purposes only, no rights reserved._ { {\u0026lt; /alert \u0026gt;}} This is the reference point number 1.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"17 July 2025","permalink":"https://robk.uk/posts/cheatsheets/blogging/post-template/","section":"ğŸ”°Posts","summary":"Hugo Post Template","title":"Hugo Post Template"},{"content":" The worldâ€™s fastest framework for building websites. #Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nğŸ“º YouTube Tutorial # ğŸ’¡Extensive but exceptionally good Hugo Static Site Generation Tutorial\n1. Create site with Hugo #cd \u0026#34;C:\\Git\u0026#34; hugo new site rtdevx.github.io ğŸ‘¨ğŸ»â€ğŸ’»Hugo framework and Git must be installed on the local system. For more details, visit: gohugo.io/installation and official Hugo website for more information.\nğŸ’¡Install Hugo extended with Winget on Windows:\nwinget install -e --id Hugo.Hugo.Extended 2. Installing Congo theme as Git submodule #cd rtdevx.github.io git init git submodule add -b stable https://github.com/jpanther/congo.git themes/congo Source: https://jpanther.github.io/congo/docs/installation/#install-using-git\nâ„¹ï¸ Other options available, check: https://jpanther.github.io/congo/docs/installation/#installation\n3. Set up the theme configuration files #In the root folder of your website, delete the hugo.toml file that was generated by Hugo. Copy the *.toml config files from the theme into your config/_default/ folder. This will ensure you have all the correct theme settings and will enable you to easily customize the theme to your needs.\nRemove-Item hugo.toml New-Item -ItemType Directory -Path ./config/_default -Force Copy-Item themes/congo/config/_default/*.toml ./config/_default # Check if files are copied ls .\\config\\_default\\ Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---l 17/08/2025 21:59 560 hugo.toml -a---l 17/08/2025 21:59 3038 languages.en.toml -a---l 17/08/2025 21:59 217 markup.toml -a---l 17/08/2025 21:59 1126 menus.en.toml -a---l 17/08/2025 21:59 52 module.toml -a---l 17/08/2025 21:59 2185 params.toml Source: Set up the theme configuration files.\n4. Update hugo.toml #â€¼ï¸If you havenâ€™t use Hugo Modules to install Congo, you must add the line theme = \u0026quot;congo\u0026quot; to the top of your hugo.toml file.\ncode .\\config\\_default\\hugo.toml # Add line on top of the file: theme = \u0026#34;congo\u0026#34; â„¹ï¸ You should also make any other modifications as appropriate (baseURL at the minimum)\nğŸ“„ .\\config\\_default\\hugo.toml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # -- Site Configuration -- # Refer to the theme docs for more details about each of these parameters. # https://jpanther.github.io/congo/docs/getting-started/ theme = \u0026#34;congo\u0026#34; baseURL = \u0026#34;https://rtdevx.github.io/\u0026#34; defaultContentLanguage = \u0026#34;en\u0026#34; enableRobotsTXT = false summaryLength = 0 [pagination] pagerSize = 10 [outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] [privacy] [privacy.vimeo] enableDNT = true [privacy.x] enableDNT = true [privacy.youTube] privacyEnhanced = true [services] [services.x] disableInlineCSS = true 5. Test new website #hugo server -D -disableFastRender â„¹ï¸ After Hugo server is started, website should be available under:\nhttp://localhost:1313/\n6. The basic Congo installation is now complete. #Continue to the Getting Started section to learn more about configuring the theme.\nModify languages.en.toml for English language support (other languages can be configured separately if required) code .\\config\\_default\\languages.en.toml Change the color scheme and adjust other parameters as adequate code .\\config\\_default\\params.toml colorScheme = \u0026#34;slate\u0026#34; â„¹ï¸ Valid color options: congo, avocado, cherry, fire, ocean, sapphire, slate.\nMore: https://jpanther.github.io/congo/docs/getting-started/#colour-schemes\n7. Organizing content #https://jpanther.github.io/congo/docs/getting-started/#organising-content\n8. Configuration #https://jpanther.github.io/congo/docs/configuration/\n9. Pushing to GitHub pages #â€¼ï¸only for the newly initialized repository:\nnew-item -ItemType File .gitignore # git init # was already done git add . git commit -m \u0026#34;Initial commit\u0026#34; git remote add origin git@github.com:rtdevx/rtdevx.github.io.git git push origin main Source: https://jpanther.github.io/congo/docs/hosting-deployment/#github-pages\n10. GitHub Actions 1 #ğŸ“„ .github/workflows/hugo.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # .github/workflows/hugo.yml name: GitHub Pages on: push: branches: - main permissions: contents: write jobs: build-deploy: runs-on: ubuntu-latest concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;latest\u0026#34; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages publish_dir: ./public Source: https://jpanther.github.io/congo/docs/hosting-deployment/#github-pages\n11. Update Congo using Git #Git submodules can be updated using the git command. Simply execute the following command and the latest version of the theme will be downloaded into your local repository:\ngit submodule update --remote --merge Once the submodule has been updated, rebuild your site and check everything works as expected.\nSource: https://jpanther.github.io/congo/docs/installation/#update-using-git\n\u0026raquo; Sources \u0026laquo; # Hugo: https://gohugo.io/\nCongo Theme: https://jpanther.github.io/congo/\nCongo installation: https://jpanther.github.io/congo/docs/installation/\nCongo Configuration: https://jpanther.github.io/congo/docs/configuration/\n\u0026raquo; References \u0026laquo; # Hugo Post Template 10. GitHub Actions âš ï¸\nIssue:\nremote: Permission denied issue emerged when implemented by following original instructions.\nError:\n\u0026ldquo;remote: Permission to rtdevx/rtdevx.github.io.git denied to github-actions[bot].\nfatal: unable to access \u0026lsquo;https://github.com/rtdevx/rtdevx.github.io.git/\u0026rsquo;: The requested URL returned error: 403.\nError: Action failed with \u0026ldquo;The process \u0026lsquo;/usr/bin/git\u0026rsquo; failed with exit code 128\u0026rdquo; Solution:\nSections highlighted in 10. GitHub Actions must be added in order to solve the permissions problem.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"17 July 2025","permalink":"https://robk.uk/posts/blogging/installing-congo-hugo/","section":"ğŸ”°Posts","summary":"Installing Hugo - The worlds fastest framework for building websites - with Congo theme","title":"Installing Hugo with Congo"}]